{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Bản sao của Bản sao của experiment.ipynb","provenance":[],"collapsed_sections":["c0w2mvo-fF_X","38uoKuPKL0_4","5Vo_jQIvM8sX","8ObdUxfbMLgI","hTw8AYfHGszL","87NrC-3SG1pb","FLfiCckAHPWB","Y5i4fuQrHUne","7k1WqTO59A5Z","CAnYaObi9HXf"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"sMicb1TP98ji","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640249221145,"user_tz":-420,"elapsed":17981,"user":{"displayName":"Trinh Truong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibwjDAT5JHasAFlEGN-kjgr2JnHi7XIqtdSxt4=s64","userId":"05537025703764185240"}},"outputId":"a4d8becf-f8b0-4065-8db0-0d59e90d9059"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive._mount('/content/drive')"]},{"cell_type":"code","source":["!pip install transformers seqeval[gpu] -q\n","!pip install fairseq -q\n","!pip install fastBPE -q\n","!pip install pytorch-crf -q"],"metadata":{"id":"if8rbToF-IbX","executionInfo":{"status":"ok","timestamp":1640249262535,"user_tz":-420,"elapsed":41395,"user":{"displayName":"Trinh Truong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibwjDAT5JHasAFlEGN-kjgr2JnHi7XIqtdSxt4=s64","userId":"05537025703764185240"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"5e4b3467-40b1-4551-e6ad-b9cba886707c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 3.4 MB 5.5 MB/s \n","\u001b[K     |████████████████████████████████| 43 kB 1.9 MB/s \n","\u001b[K     |████████████████████████████████| 596 kB 35.5 MB/s \n","\u001b[K     |████████████████████████████████| 61 kB 364 kB/s \n","\u001b[K     |████████████████████████████████| 895 kB 35.7 MB/s \n","\u001b[K     |████████████████████████████████| 3.3 MB 26.2 MB/s \n","\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |████████████████████████████████| 1.7 MB 5.3 MB/s \n","\u001b[K     |████████████████████████████████| 90 kB 8.8 MB/s \n","\u001b[K     |████████████████████████████████| 145 kB 42.5 MB/s \n","\u001b[K     |████████████████████████████████| 112 kB 48.5 MB/s \n","\u001b[K     |████████████████████████████████| 74 kB 3.1 MB/s \n","\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for fastBPE (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/NLP/project_nlp"],"metadata":{"id":"cD7orqk4-HRv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640250942849,"user_tz":-420,"elapsed":6,"user":{"displayName":"Trinh Truong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibwjDAT5JHasAFlEGN-kjgr2JnHi7XIqtdSxt4=s64","userId":"05537025703764185240"}},"outputId":"9ede3124-24a1-459e-dbfe-bdd4d51e6794"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/NLP/project_nlp\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","from torchcrf import CRF\n","\n","from transformers import RobertaConfig, RobertaPreTrainedModel, RobertaModel, RobertaForTokenClassification\n","from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule\n","from transformers.modeling_outputs import TokenClassifierOutput\n","\n","import seqeval\n","from seqeval.metrics import classification_report, f1_score\n","\n","import pandas as pd\n","import numpy as np\n","import argparse\n","import time\n","import tqdm\n","\n","from fairseq.data.encoders.fastbpe import fastBPE\n","from fairseq.data import Dictionary\n","\n","from sklearn.utils.class_weight import compute_class_weight\n","\n","from src.dataset import *"],"metadata":{"id":"vQKjvZWT-LUa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"],"metadata":{"id":"usUDwbTOJ47L"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## DATA - HERE"],"metadata":{"id":"c0w2mvo-fF_X"}},{"cell_type":"code","source":["data = pd.read_csv('./data/joint/data_joint.csv')\n","data"],"metadata":{"id":"_tudmS6OfM9c","colab":{"base_uri":"https://localhost:8080/","height":554},"executionInfo":{"status":"ok","timestamp":1640250956900,"user_tz":-420,"elapsed":11,"user":{"displayName":"Trinh Truong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibwjDAT5JHasAFlEGN-kjgr2JnHi7XIqtdSxt4=s64","userId":"05537025703764185240"}},"outputId":"86332b06-5bf4-4c13-e0c7-16b51374acfd"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-07296be4-c71d-40e2-a3da-31cc8f013fa1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>sentence</th>\n","      <th>word_labels</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>combo 3 cái giao có 1 cái , thành_ra đặt 6 cái...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-DES,B-DE...</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>mình mua áo có cổ màu trắng lại ship tới cho m...</td>\n","      <td>O,O,B-DES,B-DES,I-DES,B-DES,I-DES,O,O,O,O,O,B-...</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>giao sai hàng . tôi muốn trả hàng . đặt be đậm...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,B-DES,I-DES,O,B-DES,I-DES</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>sản_xuất việt_nam nhưng thấy in chữ trung_quốc...</td>\n","      <td>O,O,O,O,B-DES,I-DES,O,O,O,O,O,O,O</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>mình đặt áo sơ_mi trắng dài tay mà shop giao c...</td>\n","      <td>O,O,B-DES,I-DES,B-DES,B-DES,I-DES,O,O,O,O,O,O,...</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3652</th>\n","      <td>3652</td>\n","      <td>chất_lượng sản_phẩm giống mô tả . giao hàng nh...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3653</th>\n","      <td>3653</td>\n","      <td>hài_lòng vô_cùng , giao nhanh , nhân_viên giao...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,O,B-DES,O,B-DES</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3654</th>\n","      <td>3654</td>\n","      <td>sản_phẩm ổn . giá phải_chăng . thật_sự là nhận...</td>\n","      <td>O,O,O,B-PRI,B-PRI,O,O,O,O,O,O,O,O,O,O,O,O,O</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3655</th>\n","      <td>3655</td>\n","      <td>hàng đẹp chuẩn chất_lượng . nếu áo có đai ngan...</td>\n","      <td>O,B-DES,O,O,O,O,B-DES,O,O,O,B-DES,O,B-DES,O,O,O</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3656</th>\n","      <td>3656</td>\n","      <td>vải mặc mát , dày_dặn . không có miếng mút ngự...</td>\n","      <td>B-DES,O,B-DES,O,B-DES,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3657 rows × 4 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-07296be4-c71d-40e2-a3da-31cc8f013fa1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-07296be4-c71d-40e2-a3da-31cc8f013fa1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-07296be4-c71d-40e2-a3da-31cc8f013fa1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["      Unnamed: 0  ... label\n","0              0  ...    -1\n","1              1  ...    -1\n","2              2  ...    -1\n","3              3  ...    -1\n","4              4  ...    -1\n","...          ...  ...   ...\n","3652        3652  ...     1\n","3653        3653  ...     1\n","3654        3654  ...     1\n","3655        3655  ...     1\n","3656        3656  ...     1\n","\n","[3657 rows x 4 columns]"]},"metadata":{},"execution_count":4}]},{"cell_type":"markdown","source":["## Params"],"metadata":{"id":"38uoKuPKL0_4"}},{"cell_type":"code","source":["BATCHSIZE_TRAIN = 8\n","BATCHSIZE_VAL = 4\n","LEARNING_RATE = 5e-5\n","MAX_LEN = 128\n","NUM_EPOCH = 20\n","SEED = 42\n","NUM_CLASS = 5\n","MAX_GRAD_NORM = 1"],"metadata":{"id":"rgWrsDNLLz7N"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data"],"metadata":{"id":"5Vo_jQIvM8sX"}},{"cell_type":"code","source":["parser = argparse.ArgumentParser()\n","parser.add_argument('--bpe-codes', \n","    default=\"./PhoBERT_base_transformers/bpe.codes\",\n","    required=False,\n","    type=str,\n","    help='path to fastBPE BPE'\n",")\n","args, unknown = parser.parse_known_args()\n","bpe = fastBPE(args)\n","\n","# Load the dictionary\n","vocab = Dictionary()\n","vocab.add_from_file(\"./PhoBERT_base_transformers/dict.txt\")\n","\n","labels_to_ids = {'B-DES': 1, 'B-PRI': 3, 'I-DES': 2, 'I-PRI': 4, 'O': 0, 'X': -100}\n","ids_to_labels = {0: 'O', 1: 'B-DES', 2: 'I-DES', 3:'B-PRI', 4:'I-PRI'}\n","\n","X, Y_label, Y_mask = convert_lines(\n","    data.sentence.values, \n","    data.word_labels.values, \n","    vocab, \n","    bpe, \n","    labels_to_ids, \n","    max_sequence_length=MAX_LEN)\n","\n","print('X shape: ', X.shape)\n","print('Y label shape', Y_label.shape)\n","print('Y mask shape', Y_mask.shape)\n","\n","train_size = 0.8\n","def train_test_split(data, train_size):\n","    X_df = pd.DataFrame(data)\n","    X_train = X_df.sample(frac = train_size, random_state=200)\n","    X_test = X_df.drop(X_train.index).reset_index(drop=True)\n","    X_train = X_train.reset_index(drop=True)\n","    return X_train.values, X_test .values\n","\n","X_train, X_test = train_test_split(X, train_size)\n","Y_label_train, Y_label_test = train_test_split(Y_label, train_size)\n","Y_mask_train, Y_mask_test = train_test_split(Y_mask, train_size)\n","\n","class_weight = compute_class_weight(\n","    class_weight='balanced', \n","    classes = np.array([0,1,2,3,4]), \n","    y=Y_label_train.flatten()[Y_label_train.flatten()>=0])\n","\n","print(class_weight)"],"metadata":{"id":"ao1LEzAfB6-P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640250960981,"user_tz":-420,"elapsed":4088,"user":{"displayName":"Trinh Truong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibwjDAT5JHasAFlEGN-kjgr2JnHi7XIqtdSxt4=s64","userId":"05537025703764185240"}},"outputId":"5407b958-44d9-4489-d3b2-eb13df7d5496"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 3657/3657 [00:03<00:00, 1132.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["X shape:  (3657, 128)\n","Y label shape (3657, 128)\n","Y mask shape (3657, 128)\n","[ 0.24328616  1.55779008  6.52840909 15.97682503 31.33636364]\n"]}]},{"cell_type":"code","source":["train_dataset = TensorDataset(\n","    torch.tensor(X_train,dtype=torch.long), \n","    torch.tensor(Y_label_train,dtype=torch.long)\n","    )\n","valid_dataset = TensorDataset(\n","    torch.tensor(X_test,dtype=torch.long), \n","    torch.tensor(Y_label_test,dtype=torch.long)\n","    )\n","\n","train_loader = torch.utils.data.DataLoader(\n","    train_dataset, \n","    batch_size=BATCHSIZE_TRAIN, \n","    shuffle=True\n","    )\n","valid_loader = torch.utils.data.DataLoader(\n","    valid_dataset, \n","    batch_size=BATCHSIZE_VAL, \n","    shuffle=False\n","    )"],"metadata":{"id":"w-jI8GwDMJPw"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Config"],"metadata":{"id":"8ObdUxfbMLgI"}},{"cell_type":"code","source":["class argu():\n","    def __init__(self):\n","        self.dict_path = \"./PhoBERT_base_transformers/dict.txt\"\n","        self.config_path = \"./PhoBERT_base_transformers/config.json\"\n","        self.max_sequence_length = MAX_LEN\n","        self.accumulation_steps = 1\n","        self.epochs = NUM_EPOCH\n","        self.seed = SEED\n","        self.bpe_codes = \"./PhoBERT_base_transformers/bpe.codes\"\n","args = argu()\n","\n","config = RobertaConfig.from_pretrained(\n","    args.config_path,\n","    output_hidden_states=True,\n","    return_dict=True,\n","    num_labels=NUM_CLASS,\n","    pad_token_id = 1,\n","    bos_token_id = 0,\n","    eos_token_id = 2,\n","    attention_probs_dropout_prob = 0.1,\n","    classifier_dropout=0.5,\n","    gradient_checkpointing=False,\n","    hidden_act=\"gelu\",\n","    hidden_dropout_prob=0.1,\n","    hidden_size=768,\n","    initializer_range=0.02,\n","    intermediate_size=3072,\n","    layer_norm_eps=1e-05,\n","    max_position_embeddings=258,\n","    model_type=\"roberta\",\n","    num_attention_heads=12,\n","    num_hidden_layers=12,\n","    position_embedding_type=\"absolute\",\n","    tokenizer_class=\"PhobertTokenizer\",\n","    transformers_version=\"4.15.0\",\n","    type_vocab_size=1,\n","    use_cache=True,\n","    vocab_size=64001\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"klKRUc99MNYH","executionInfo":{"status":"ok","timestamp":1640250960982,"user_tz":-420,"elapsed":9,"user":{"displayName":"Trinh Truong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibwjDAT5JHasAFlEGN-kjgr2JnHi7XIqtdSxt4=s64","userId":"05537025703764185240"}},"outputId":"23d3bcb1-90bd-4351-e4d8-6120dc994a72"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["You are using a model of type bert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n"]}]},{"cell_type":"code","source":["def train(epoch, verbose = False):\n","    tr_loss, tr_accuracy = 0, 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","    tr_preds, tr_labels = [], []\n","    # put model in training mode\n","    model.train()\n","    \n","    for idx, batch in enumerate(train_loader):\n","        ids, labels = batch\n","        ids = ids.to(device)\n","        labels = labels.to(device)\n","        mask = ids!=1\n","\n","        outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n","        loss = outputs[0]\n","        \n","        tr_logits = outputs[1]\n","        tr_loss += loss.item()\n","\n","        nb_tr_steps += 1\n","        nb_tr_examples += labels.size(0)\n","        \n","        if idx % 100==0 and verbose:\n","            loss_step = tr_loss/nb_tr_steps\n","            print(f\"Training loss per 100 training steps: {loss_step}\")\n","           \n","        # compute training accuracy\n","        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","        \n","        # only compute accuracy at active labels\n","        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","        \n","        labels = torch.masked_select(flattened_targets, active_accuracy)\n","        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","        \n","        tr_labels.extend(labels)\n","        tr_preds.extend(predictions)\n","    \n","        # gradient clipping\n","        torch.nn.utils.clip_grad_norm_(\n","            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n","        )\n","        \n","        # backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        try: \n","            scheduler0.step()\n","        except:\n","            scheduler.step()\n","\n","    epoch_loss = tr_loss / nb_tr_steps\n","    tr_accuracy = tr_accuracy / nb_tr_steps\n","\n","    labels = [ids_to_labels[id.item()] for id in tr_labels]\n","    predictions = [ids_to_labels[id.item()] for id in tr_preds]\n","    f1 = seqeval.metrics.f1_score([labels], [predictions])\n","\n","    print(f\"Training loss epoch: {epoch_loss}\", f\"Training F1 epoch: {f1}\")\n","\n","def valid(model, test_loader, verbose=False):\n","    # put model in evaluation mode\n","    model.eval()\n","    \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_examples, nb_eval_steps = 0, 0\n","    eval_preds, eval_labels = [], []\n","    \n","    with torch.no_grad():\n","        for idx, batch in enumerate(test_loader):\n","            \n","            ids, labels = batch\n","            ids = ids.to(device)\n","            labels = labels.to(device)\n","            mask = ids!=1 \n","\n","            outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n","            loss = outputs[0]\n","            eval_logits = outputs[1]\n","            eval_loss += loss.item()\n","\n","            nb_eval_steps += 1\n","            nb_eval_examples += labels.size(0)\n","        \n","            if idx % 100==0 and verbose:\n","                loss_step = eval_loss/nb_eval_steps\n","                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n","              \n","            # compute evaluation accuracy\n","            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","            \n","            # only compute accuracy at active labels\n","            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","        \n","            labels = torch.masked_select(flattened_targets, active_accuracy)\n","            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","            \n","            eval_labels.extend(labels)\n","            eval_preds.extend(predictions)\n","            \n","\n","    labels = [ids_to_labels[id.item()] for id in eval_labels]\n","    predictions = [ids_to_labels[id.item()] for id in eval_preds]\n","    \n","    eval_loss = eval_loss / nb_eval_steps\n","    eval_accuracy = eval_accuracy / nb_eval_steps\n","    f1 = seqeval.metrics.f1_score([labels], [predictions])\n","    print(f\"Validation Loss: {eval_loss}\", f\"Validation F1: {f1}\")\n","\n","    return labels, predictions, f1\n","\n","def train_crf(epoch, verbose = False):\n","    tr_loss, tr_accuracy = 0, 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","    tr_preds, tr_labels = [], []\n","    model.train()\n","    \n","    for idx, batch in enumerate(train_loader):\n","        ids, labels = batch\n","        ids = ids.to(device)\n","        labels = labels.to(device)\n","        mask = ids!=1\n","        \n","        outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n","        loss = outputs[0]\n","                \n","        tr_loss += loss.item()\n","\n","        nb_tr_steps += 1\n","        nb_tr_examples += labels.size(0)\n","        \n","        if idx % 100==0 and verbose:\n","            loss_step = tr_loss/nb_tr_steps\n","            print(f\"Training loss per 100 training steps: {loss_step}\")\n","           \n","        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","        flattened_predictions = outputs[1].view(-1)\n","        \n","        # only compute accuracy at active labels\n","        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","        active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n","        \n","        labels = torch.masked_select(flattened_targets, active_accuracy)\n","        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","        \n","        tr_labels.extend(labels)\n","        tr_preds.extend(predictions)\n","    \n","        # gradient clipping\n","        torch.nn.utils.clip_grad_norm_(\n","            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n","        )\n","        \n","        # backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        try: \n","            scheduler0.step()\n","        except:\n","            scheduler.step()\n","\n","    epoch_loss = tr_loss / nb_tr_steps\n","    tr_accuracy = tr_accuracy / nb_tr_steps\n","\n","    labels = [ids_to_labels[id.item()] for id in tr_labels]\n","    predictions = [ids_to_labels[id.item()] for id in tr_preds]\n","    f1 = seqeval.metrics.f1_score([labels], [predictions])\n","\n","    print(f\"Training loss epoch: {epoch_loss}\", f\"Training F1 epoch: {f1}\")\n","\n","def valid_crf(model, test_loader, verbose=False):\n","    # put model in evaluation mode\n","    model.eval()\n","    \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_examples, nb_eval_steps = 0, 0\n","    eval_preds, eval_labels = [], []\n","    \n","    with torch.no_grad():\n","        for idx, batch in enumerate(test_loader):\n","            \n","            ids, labels = batch\n","            ids = ids.to(device)\n","            labels = labels.to(device)\n","            mask = ids!=1 \n","\n","            outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n","            loss = outputs[0]\n","            eval_loss += loss.item()\n","\n","            nb_eval_steps += 1\n","            nb_eval_examples += labels.size(0)\n","        \n","            if idx % 100==0 and verbose:\n","                loss_step = eval_loss/nb_eval_steps\n","                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n","              \n","            # compute evaluation accuracy\n","            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","            flattened_predictions = outputs[1].view(-1)\n","            \n","            # only compute accuracy at active labels\n","            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","            active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n","\n","            labels = torch.masked_select(flattened_targets, active_accuracy)\n","            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","            \n","            eval_labels.extend(labels)\n","            eval_preds.extend(predictions)\n","            \n","\n","    labels = [ids_to_labels[id.item()] for id in eval_labels]\n","    predictions = [ids_to_labels[id.item()] for id in eval_preds]\n","    \n","    eval_loss = eval_loss / nb_eval_steps\n","    # eval_accuracy = eval_accuracy / nb_eval_steps\n","    f1 = seqeval.metrics.f1_score([labels], [predictions])\n","    print(f\"Validation Loss: {eval_loss}\", f\"Validation F1: {f1}\")\n","\n","    return labels, predictions, f1"],"metadata":{"id":"J3Qxsp2gXq3P"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Default Phobert + Linear layer + Tiki dataset"],"metadata":{"id":"hTw8AYfHGszL"}},{"cell_type":"code","source":["class RobertaForTokenClassification(RobertaPreTrainedModel):\n","    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n","    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n","\n","    def __init__(self, config, class_weight=None):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","        self.roberta = RobertaModel(config, add_pooling_layer=False)\n","        classifier_dropout = (\n","            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n","        )\n","        self.dropout = nn.Dropout(classifier_dropout)\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","        # Initialize weights and apply final processing\n","        self.post_init()\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        outputs = self.roberta(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        sequence_output = outputs[0]\n","\n","        sequence_output = self.dropout(sequence_output)\n","        logits = self.classifier(sequence_output)\n","\n","        loss = None\n","        if labels is not None:\n","            if class_weight is not None:\n","                loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(class_weight, dtype=torch.float).to(device))\n","            else: \n","                loss_fct = nn.CrossEntropyLoss()\n","            # Only keep active parts of the loss\n","            if attention_mask is not None:\n","                active_loss = attention_mask.view(-1) == 1\n","                active_logits = logits.view(-1, self.num_labels)\n","                active_labels = torch.where(\n","                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n","                )\n","                loss = loss_fct(active_logits, active_labels)\n","            else:\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","\n","        if not return_dict:\n","            output = (logits,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return TokenClassifierOutput(\n","            loss=loss,\n","            logits=logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )\n","\n","checkpoint_path = './checkpoints/RobertaForTokenClassification_best.pth'\n","model = RobertaForTokenClassification.from_pretrained(\"vinai/phobert-base\", config=config, class_weight=class_weight)\n","model.cuda()"],"metadata":{"id":"_g1hWIJDIezR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","num_train_optimization_steps = int(args.epochs*len(train_dataset)/BATCHSIZE_TRAIN/args.accumulation_steps)\n","optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=num_train_optimization_steps)  # PyTorch scheduler\n","# scheduler0 = get_constant_schedule(optimizer)  # PyTorch scheduler\n","\n","print(\"Learning rate: \", LEARNING_RATE)\n","print(\"num_train_optimization_steps:\", num_train_optimization_steps)\n","\n","tsfm = model.roberta\n","for child in tsfm.children():\n","    for param in child.parameters():\n","        if not param.requires_grad:\n","            print(\"whoopsies\")\n","        param.requires_grad = False\n","frozen = True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qa1AGCV8QI32","executionInfo":{"status":"ok","timestamp":1640248598046,"user_tz":-420,"elapsed":21,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"fee2c2d4-dd8c-4612-a71e-d5d20714b68d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Learning rate:  5e-05\n","num_train_optimization_steps: 7315\n"]}]},{"cell_type":"code","source":["f1_best = 0\n","for epoch in range(NUM_EPOCH):\n","    if epoch > 0 and frozen:\n","        for child in tsfm.children():\n","            for param in child.parameters():\n","                param.requires_grad = True\n","        frozen = False\n","        try:\n","            del scheduler0\n","        except:\n","            pass\n","        torch.cuda.empty_cache()\n","    st = time.time()\n","    print(f\"Training epoch: {epoch + 1}\")\n","    train(epoch)\n","    labels, predictions, f1_val = valid(model, valid_loader)\n","    if f1_val > f1_best:\n","        f1_best = f1_val\n","        print(f'New best f1 {f1_best}')\n","        print(classification_report([labels], [predictions]))\n","    # save best model\n","    torch.save(model.state_dict(), checkpoint_path)\n","    for param_group in optimizer.param_groups:\n","        print('Current leanring rate: ',param_group['lr'])\n","    print('Time: ',time.time() - st)\n","    print('======================================================')"],"metadata":{"id":"jkqnn-a33hqI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Phobert + LSTM + Linear + Tiki dataset"],"metadata":{"id":"87NrC-3SG1pb"}},{"cell_type":"code","source":["class RobertaLSTM(RobertaPreTrainedModel):\n","    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n","    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n","\n","    def __init__(self, config, class_weight=None):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.roberta = RobertaModel(config, add_pooling_layer=False)\n","        classifier_dropout = (\n","            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n","        )\n","        self.dropout = nn.Dropout(classifier_dropout)\n","        self.bilstm = nn.LSTM(config.hidden_size, (config.hidden_size) // 2, dropout=0.5, batch_first=True, bidirectional=True)\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","\n","        # Initialize weights and apply final processing\n","        self.post_init()\n","\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","        r\"\"\"\n","        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n","            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n","            1]``.\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        outputs = self.roberta(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        sequence_output = outputs[0]\n","\n","        sequence_output = self.dropout(sequence_output)\n","        lstm_output, hc = self.bilstm(sequence_output)\n","        logits = self.classifier(lstm_output)\n","\n","        loss = None\n","        if labels is not None:\n","            if class_weight is not None:\n","                loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(class_weight, dtype=torch.float).to(device))\n","            else: \n","                loss_fct = nn.CrossEntropyLoss()\n","            # Only keep active parts of the loss\n","            if attention_mask is not None:\n","                active_loss = attention_mask.view(-1) == 1\n","                active_logits = logits.view(-1, self.num_labels)\n","                active_labels = torch.where(\n","                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n","                )\n","                loss = loss_fct(active_logits, active_labels)\n","            else:\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","\n","        if not return_dict:\n","            output = (logits,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return TokenClassifierOutput(\n","            loss=loss,\n","            logits=logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )\n","\n","checkpoint_path = './checkpoints/RobertaLSTM.pth'\n","model = RobertaLSTM.from_pretrained(\"vinai/phobert-base\", config=config, class_weight=class_weight)\n","model.to(device)"],"metadata":{"id":"a6YCktfxG8Ii","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640249036142,"user_tz":-420,"elapsed":2380,"user":{"displayName":"Quang Duy Trần","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvMaEX5IKcYvanQuv2HTQEdjP5mTCjGM4CkVJX=s64","userId":"16019943656023573816"}},"outputId":"bfc3efbb-5670-46d1-83f8-44fbac176035"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaLSTM: ['lm_head.decoder.weight', 'lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.dense.bias', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.layer_norm.weight']\n","- This IS expected if you are initializing RobertaLSTM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaLSTM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaLSTM were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['bilstm.weight_hh_l0', 'classifier.weight', 'bilstm.bias_ih_l0', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_hh_l0_reverse', 'bilstm.weight_ih_l0', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_hh_l0', 'classifier.bias', 'bilstm.weight_ih_l0_reverse']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["RobertaLSTM(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n","      (position_embeddings): Embedding(258, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (dropout): Dropout(p=0.5, inplace=False)\n","  (bilstm): LSTM(768, 384, batch_first=True, dropout=0.5, bidirectional=True)\n","  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",")"]},"metadata":{},"execution_count":14}]},{"cell_type":"code","source":["param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","num_train_optimization_steps = int(args.epochs*len(train_dataset)/BATCHSIZE_TRAIN/args.accumulation_steps)\n","optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=num_train_optimization_steps)  # PyTorch scheduler\n","# scheduler0 = get_constant_schedule(optimizer)  # PyTorch scheduler\n","\n","print(\"Learning rate: \", LEARNING_RATE)\n","print(\"num_train_optimization_steps:\", num_train_optimization_steps)\n","\n","tsfm = model.roberta\n","for child in tsfm.children():\n","    for param in child.parameters():\n","        if not param.requires_grad:\n","            print(\"whoopsies\")\n","        param.requires_grad = False\n","frozen = True"],"metadata":{"id":"O98Xwy0KQ-o1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640249036142,"user_tz":-420,"elapsed":12,"user":{"displayName":"Quang Duy Trần","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvMaEX5IKcYvanQuv2HTQEdjP5mTCjGM4CkVJX=s64","userId":"16019943656023573816"}},"outputId":"7f3c7dee-eef2-4265-eda8-e3ae5bf336fd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Learning rate:  5e-05\n","num_train_optimization_steps: 7315\n"]}]},{"cell_type":"code","source":["f1_best = 0\n","for epoch in range(NUM_EPOCH):\n","    if epoch > 0 and frozen:\n","        for child in tsfm.children():\n","            for param in child.parameters():\n","                param.requires_grad = True\n","        frozen = False\n","        try:\n","            del scheduler0\n","        except:\n","            pass\n","        torch.cuda.empty_cache()\n","    st = time.time()\n","    print(f\"Training epoch: {epoch + 1}\")\n","    train(epoch)\n","    labels, predictions, f1_val = valid(model, valid_loader)\n","    if f1_val > f1_best:\n","        f1_best = f1_val\n","        print(f'New best f1 {f1_best}')\n","        print(classification_report([labels], [predictions]))\n","    # save best model\n","    torch.save(model.state_dict(), checkpoint_path)\n","    for param_group in optimizer.param_groups:\n","        print('Current leanring rate: ',param_group['lr'])\n","    print('Time: ',time.time() - st)\n","    print('======================================================')"],"metadata":{"id":"PoljO15xQ-o3","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c436b42b-f06a-4799-f16c-8c05c3fab867"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training epoch: 1\n","Training loss epoch: 0.9673119274482049 Training F1 epoch: 0.3007355682858208\n","Validation Loss: 0.5241246931051295 Validation F1: 0.4832965914871969\n","New best f1 0.4832965914871969\n","              precision    recall  f1-score   support\n","\n","         DES       0.47      0.59      0.52      2274\n","         PRI       0.14      0.46      0.22       182\n","\n","   micro avg       0.41      0.58      0.48      2456\n","   macro avg       0.31      0.53      0.37      2456\n","weighted avg       0.45      0.58      0.50      2456\n","\n","Current leanring rate:  4.815661815661816e-05\n","Current leanring rate:  4.815661815661816e-05\n","Time:  92.77245831489563\n","======================================================\n","Training epoch: 2\n"]}]},{"cell_type":"markdown","source":["## Phobert + CRF + Tiki dataset"],"metadata":{"id":"FLfiCckAHPWB"}},{"cell_type":"code","source":["class RobertaCRF(RobertaPreTrainedModel):\n","\n","    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n","    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.roberta = RobertaModel(config, add_pooling_layer=False)\n","        classifier_dropout = (\n","            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n","        )\n","        self.dropout = nn.Dropout(classifier_dropout)\n","        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n","\n","        self.crf = CRF(num_tags=self.num_labels, batch_first=True)\n","        self.init_weights()\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","        r\"\"\"\n","        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n","            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n","            1]``.\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        outputs = self.roberta(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        sequence_output = outputs[0]\n","        sequence_output = self.dropout(sequence_output)\n","        logits = self.classifier(sequence_output)\n","\n","        loss = None\n","        if labels is not None:\n","            labels[labels==-100] = 0\n","            log_likelihood, tags = self.crf(logits, labels), self.crf.decode(logits)\n","            loss = 0 - log_likelihood\n","        else:\n","            tags = self.crf.decode(logits)\n","        tags = torch.Tensor(tags)\n","        tags = tags.to(device)\n","\n","        if not return_dict:\n","            output = (tags,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return loss, tags\n","\n","checkpoint_path = './checkpoints/RobertaCRF.pth'\n","model = RobertaCRF.from_pretrained('vinai/phobert-base', config=config)\n","model.cuda()"],"metadata":{"id":"HFmwsDfeHUJZ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640251582937,"user_tz":-420,"elapsed":2132,"user":{"displayName":"Trinh Truong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibwjDAT5JHasAFlEGN-kjgr2JnHi7XIqtdSxt4=s64","userId":"05537025703764185240"}},"outputId":"5dc06f81-7f5e-41ff-fc59-86c6168e784a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaCRF: ['lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.bias']\n","- This IS expected if you are initializing RobertaCRF from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaCRF from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaCRF were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['crf.transitions', 'crf.start_transitions', 'classifier.weight', 'classifier.bias', 'crf.end_transitions']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["RobertaCRF(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n","      (position_embeddings): Embedding(258, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (dropout): Dropout(p=0.5, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=5, bias=True)\n","  (crf): CRF(num_tags=5)\n",")"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","num_train_optimization_steps = int(args.epochs*len(train_dataset)/BATCHSIZE_TRAIN/args.accumulation_steps)\n","optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=num_train_optimization_steps)  # PyTorch scheduler\n","# scheduler0 = get_constant_schedule(optimizer)  # PyTorch scheduler\n","\n","print(\"Learning rate: \", LEARNING_RATE)\n","print(\"num_train_optimization_steps:\", num_train_optimization_steps)\n","\n","tsfm = model.roberta\n","for child in tsfm.children():\n","    for param in child.parameters():\n","        if not param.requires_grad:\n","            print(\"whoopsies\")\n","        param.requires_grad = False\n","frozen = True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BhSDwHo98xua","executionInfo":{"status":"ok","timestamp":1640251582938,"user_tz":-420,"elapsed":11,"user":{"displayName":"Trinh Truong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibwjDAT5JHasAFlEGN-kjgr2JnHi7XIqtdSxt4=s64","userId":"05537025703764185240"}},"outputId":"64a01f96-73c0-4955-a9ef-44671b2408d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Learning rate:  5e-05\n","num_train_optimization_steps: 7315\n"]}]},{"cell_type":"code","source":["f1_best = 0\n","for epoch in range(NUM_EPOCH):\n","    if epoch > 0 and frozen:\n","        for child in tsfm.children():\n","            for param in child.parameters():\n","                param.requires_grad = True\n","        frozen = False\n","        try:\n","            del scheduler0\n","        except:\n","            pass\n","        torch.cuda.empty_cache()\n","    st = time.time()\n","    print(f\"Training epoch: {epoch + 1}\")\n","    train_crf(epoch)\n","    labels, predictions, f1_val = valid_crf(model, valid_loader)\n","    if f1_val > f1_best:\n","        f1_best = f1_val\n","        print(f'New best f1 {f1_best}')\n","        print(classification_report([labels], [predictions]))\n","    # save best model\n","    torch.save(model.state_dict(), checkpoint_path)\n","    for param_group in optimizer.param_groups:\n","        print('Current leanring rate: ',param_group['lr'])\n","    print('Time: ',time.time() - st)\n","    print('======================================================')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"w_N4o9lufTgS","executionInfo":{"status":"ok","timestamp":1640257114857,"user_tz":-420,"elapsed":5531923,"user":{"displayName":"Trinh Truong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibwjDAT5JHasAFlEGN-kjgr2JnHi7XIqtdSxt4=s64","userId":"05537025703764185240"}},"outputId":"db993bd0-d82e-4eb6-b271-9ddb9ade5193"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training epoch: 1\n","Training loss epoch: 492.3005064667248 Training F1 epoch: 0.01878941551264547\n","Validation Loss: 79.64110606876227 Validation F1: 0.0\n","Current leanring rate:  4.815661815661816e-05\n","Current leanring rate:  4.815661815661816e-05\n","Time:  167.5227653980255\n","======================================================\n","Training epoch: 2\n","Training loss epoch: 72.08844140058007 Training F1 epoch: 0.572025971167602\n","Validation Loss: 16.068341864914192 Validation F1: 0.7617947236664742\n","New best f1 0.7617947236664742\n","              precision    recall  f1-score   support\n","\n","         DES       0.77      0.83      0.80      2276\n","         PRI       0.33      0.54      0.41       182\n","\n","   micro avg       0.72      0.80      0.76      2458\n","   macro avg       0.55      0.68      0.60      2458\n","weighted avg       0.74      0.80      0.77      2458\n","\n","Current leanring rate:  4.562023562023562e-05\n","Current leanring rate:  4.562023562023562e-05\n","Time:  282.6820502281189\n","======================================================\n","Training epoch: 3\n","Training loss epoch: 26.56274305666731 Training F1 epoch: 0.8111370321533624\n","Validation Loss: 11.39215754941513 Validation F1: 0.8680000000000001\n","New best f1 0.8680000000000001\n","              precision    recall  f1-score   support\n","\n","         DES       0.86      0.88      0.87      2276\n","         PRI       0.81      0.86      0.84       182\n","\n","   micro avg       0.85      0.88      0.87      2458\n","   macro avg       0.83      0.87      0.85      2458\n","weighted avg       0.85      0.88      0.87      2458\n","\n","Current leanring rate:  4.3083853083853084e-05\n","Current leanring rate:  4.3083853083853084e-05\n","Time:  282.8778009414673\n","======================================================\n","Training epoch: 4\n","Training loss epoch: 18.46705952628714 Training F1 epoch: 0.8745849297573435\n","Validation Loss: 9.587735869193988 Validation F1: 0.8910217956408718\n","New best f1 0.8910217956408718\n","              precision    recall  f1-score   support\n","\n","         DES       0.88      0.91      0.89      2276\n","         PRI       0.82      0.88      0.85       182\n","\n","   micro avg       0.88      0.91      0.89      2458\n","   macro avg       0.85      0.90      0.87      2458\n","weighted avg       0.88      0.91      0.89      2458\n","\n","Current leanring rate:  4.054747054747055e-05\n","Current leanring rate:  4.054747054747055e-05\n","Time:  285.1398570537567\n","======================================================\n","Training epoch: 5\n","Training loss epoch: 13.601385814896046 Training F1 epoch: 0.9116169383779349\n","Validation Loss: 9.106737814314378 Validation F1: 0.8954380298748486\n","New best f1 0.8954380298748486\n","              precision    recall  f1-score   support\n","\n","         DES       0.89      0.91      0.90      2276\n","         PRI       0.85      0.84      0.84       182\n","\n","   micro avg       0.89      0.90      0.90      2458\n","   macro avg       0.87      0.87      0.87      2458\n","weighted avg       0.89      0.90      0.90      2458\n","\n","Current leanring rate:  3.8011088011088013e-05\n","Current leanring rate:  3.8011088011088013e-05\n","Time:  286.4828019142151\n","======================================================\n","Training epoch: 6\n","Training loss epoch: 10.416882124103484 Training F1 epoch: 0.9296541801551821\n","Validation Loss: 9.832620589459529 Validation F1: 0.9070840858920329\n","New best f1 0.9070840858920329\n","              precision    recall  f1-score   support\n","\n","         DES       0.90      0.92      0.91      2276\n","         PRI       0.89      0.90      0.89       182\n","\n","   micro avg       0.90      0.92      0.91      2458\n","   macro avg       0.89      0.91      0.90      2458\n","weighted avg       0.90      0.92      0.91      2458\n","\n","Current leanring rate:  3.5474705474705475e-05\n","Current leanring rate:  3.5474705474705475e-05\n","Time:  283.02180767059326\n","======================================================\n","Training epoch: 7\n","Training loss epoch: 8.22558093461834 Training F1 epoch: 0.9446874196038076\n","Validation Loss: 8.54500058700478 Validation F1: 0.9193095142513047\n","New best f1 0.9193095142513047\n","              precision    recall  f1-score   support\n","\n","         DES       0.91      0.93      0.92      2276\n","         PRI       0.90      0.90      0.90       182\n","\n","   micro avg       0.91      0.93      0.92      2458\n","   macro avg       0.90      0.92      0.91      2458\n","weighted avg       0.91      0.93      0.92      2458\n","\n","Current leanring rate:  3.293832293832294e-05\n","Current leanring rate:  3.293832293832294e-05\n","Time:  285.74919390678406\n","======================================================\n","Training epoch: 8\n","Training loss epoch: 6.700083831620347 Training F1 epoch: 0.952723905550697\n","Validation Loss: 9.200979430818819 Validation F1: 0.9232315112540194\n","New best f1 0.9232315112540194\n","              precision    recall  f1-score   support\n","\n","         DES       0.91      0.94      0.92      2276\n","         PRI       0.91      0.91      0.91       182\n","\n","   micro avg       0.91      0.93      0.92      2458\n","   macro avg       0.91      0.92      0.92      2458\n","weighted avg       0.91      0.93      0.92      2458\n","\n","Current leanring rate:  3.0401940401940404e-05\n","Current leanring rate:  3.0401940401940404e-05\n","Time:  286.84679317474365\n","======================================================\n","Training epoch: 9\n","Training loss epoch: 5.331202856178492 Training F1 epoch: 0.9602184891270741\n","Validation Loss: 9.37878251206028 Validation F1: 0.916733708769107\n","Current leanring rate:  2.7865557865557868e-05\n","Current leanring rate:  2.7865557865557868e-05\n","Time:  281.05869364738464\n","======================================================\n","Training epoch: 10\n","Training loss epoch: 4.6029789825606215 Training F1 epoch: 0.9651510464996392\n","Validation Loss: 9.038360428940402 Validation F1: 0.9244104011288047\n","New best f1 0.9244104011288047\n","              precision    recall  f1-score   support\n","\n","         DES       0.92      0.94      0.93      2276\n","         PRI       0.90      0.89      0.89       182\n","\n","   micro avg       0.92      0.93      0.92      2458\n","   macro avg       0.91      0.91      0.91      2458\n","weighted avg       0.92      0.93      0.92      2458\n","\n","Current leanring rate:  2.532917532917533e-05\n","Current leanring rate:  2.532917532917533e-05\n","Time:  279.1389129161835\n","======================================================\n","Training epoch: 11\n","Training loss epoch: 3.493776581977886 Training F1 epoch: 0.9745492014425553\n","Validation Loss: 12.88669273501537 Validation F1: 0.8995650454725188\n","Current leanring rate:  2.2792792792792794e-05\n","Current leanring rate:  2.2792792792792794e-05\n","Time:  277.20170044898987\n","======================================================\n","Training epoch: 12\n","Training loss epoch: 3.567013599833504 Training F1 epoch: 0.9719433719433719\n","Validation Loss: 9.853860156783641 Validation F1: 0.9204040404040403\n","Current leanring rate:  2.025641025641026e-05\n","Current leanring rate:  2.025641025641026e-05\n","Time:  282.7664620876312\n","======================================================\n","Training epoch: 13\n","Training loss epoch: 2.742746655406848 Training F1 epoch: 0.9783402788496167\n","Validation Loss: 10.76506734545765 Validation F1: 0.9250353606789251\n","New best f1 0.9250353606789251\n","              precision    recall  f1-score   support\n","\n","         DES       0.92      0.94      0.93      2276\n","         PRI       0.92      0.87      0.89       182\n","\n","   micro avg       0.92      0.93      0.93      2458\n","   macro avg       0.92      0.90      0.91      2458\n","weighted avg       0.92      0.93      0.92      2458\n","\n","Current leanring rate:  1.772002772002772e-05\n","Current leanring rate:  1.772002772002772e-05\n","Time:  279.59890627861023\n","======================================================\n","Training epoch: 14\n","Training loss epoch: 2.57815926973937 Training F1 epoch: 0.9783817171093268\n","Validation Loss: 9.897261822809938 Validation F1: 0.9225452327708885\n","Current leanring rate:  1.5183645183645184e-05\n","Current leanring rate:  1.5183645183645184e-05\n","Time:  279.8894670009613\n","======================================================\n","Training epoch: 15\n","Training loss epoch: 1.9761731924255037 Training F1 epoch: 0.9851299202469772\n","Validation Loss: 10.998557835980192 Validation F1: 0.9242546333601934\n","Current leanring rate:  1.2647262647262647e-05\n","Current leanring rate:  1.2647262647262647e-05\n","Time:  280.933393239975\n","======================================================\n","Training epoch: 16\n","Training loss epoch: 1.921938119690275 Training F1 epoch: 0.9854566010586361\n","Validation Loss: 10.51316858510502 Validation F1: 0.9230145867098866\n","Current leanring rate:  1.0110880110880111e-05\n","Current leanring rate:  1.0110880110880111e-05\n","Time:  281.8663101196289\n","======================================================\n","Training epoch: 17\n","Training loss epoch: 1.4384382070739412 Training F1 epoch: 0.9898137668484411\n","Validation Loss: 11.765669692409494 Validation F1: 0.9281968535699879\n","New best f1 0.9281968535699879\n","              precision    recall  f1-score   support\n","\n","         DES       0.92      0.94      0.93      2276\n","         PRI       0.90      0.90      0.90       182\n","\n","   micro avg       0.92      0.94      0.93      2458\n","   macro avg       0.91      0.92      0.91      2458\n","weighted avg       0.92      0.94      0.93      2458\n","\n","Current leanring rate:  7.574497574497574e-06\n","Current leanring rate:  7.574497574497574e-06\n","Time:  283.4319589138031\n","======================================================\n","Training epoch: 18\n","Training loss epoch: 1.2852456348189891 Training F1 epoch: 0.9913097135805008\n","Validation Loss: 10.721923161074113 Validation F1: 0.9236980218005653\n","Current leanring rate:  5.038115038115039e-06\n","Current leanring rate:  5.038115038115039e-06\n","Time:  282.335275888443\n","======================================================\n","Training epoch: 19\n","Training loss epoch: 1.1316443271324284 Training F1 epoch: 0.9924409934694298\n","Validation Loss: 11.785404059404884 Validation F1: 0.9228915662650603\n","Current leanring rate:  2.5017325017325016e-06\n","Current leanring rate:  2.5017325017325016e-06\n","Time:  281.02868390083313\n","======================================================\n","Training epoch: 20\n","Training loss epoch: 0.9923385870261271 Training F1 epoch: 0.9933141328944661\n","Validation Loss: 11.597642241931352 Validation F1: 0.9279224712295577\n","Current leanring rate:  0.0\n","Current leanring rate:  0.0\n","Time:  281.8811812400818\n","======================================================\n"]}]},{"cell_type":"markdown","source":["## Phobert + LSTM + CRF + Tiki dataset"],"metadata":{"id":"Y5i4fuQrHUne"}},{"cell_type":"markdown","source":["https://github.com/hemingkx/CLUENER2020/tree/main/BERT-Softmax"],"metadata":{"id":"-YkZxXqfF2vY"}},{"cell_type":"code","source":["class RobertaLSTMCRF(RobertaPreTrainedModel):\n","\n","    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n","    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.roberta = RobertaModel(config, add_pooling_layer=False)\n","        classifier_dropout = (\n","            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n","        )\n","        self.dropout = nn.Dropout(classifier_dropout)\n","        self.bilstm = nn.LSTM(config.hidden_size, (config.hidden_size) // 2, dropout=0.1, batch_first=True, bidirectional=True)\n","        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n","\n","        self.crf = CRF(num_tags=self.num_labels, batch_first=True)\n","        self.post_init()\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","        r\"\"\"\n","        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n","            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n","            1]``.\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        outputs = self.roberta(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        sequence_output = outputs[0]\n","        sequence_output = self.dropout(sequence_output)\n","        lstm_output, hc = self.bilstm(sequence_output)\n","        logits = self.classifier(lstm_output)\n","\n","        loss = None\n","        if labels is not None:\n","            labels[labels==-100] = 0\n","            log_likelihood, tags = self.crf(logits, labels), self.crf.decode(logits)\n","            loss = 0 - log_likelihood\n","        else:\n","            tags = self.crf.decode(logits)\n","        tags = torch.Tensor(tags)\n","        tags = tags.to(device)\n","\n","        if not return_dict:\n","            output = (tags,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return loss, tags"],"metadata":{"id":"U4AjvocHHY12"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = RobertaLSTMCRF.from_pretrained('vinai/phobert-base', num_labels=5)\n","model.cuda()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"moBnXhdFpb2M","executionInfo":{"status":"ok","timestamp":1640101176865,"user_tz":-420,"elapsed":2111,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"59ba7779-dd2d-47cc-c503-906a645e8054"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaLSTMCRF: ['lm_head.layer_norm.bias', 'lm_head.decoder.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.weight', 'lm_head.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaLSTMCRF from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaLSTMCRF from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaLSTMCRF were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['bilstm.weight_hh_l0_reverse', 'bilstm.weight_ih_l0', 'bilstm.weight_hh_l0', 'bilstm.bias_hh_l0_reverse', 'classifier.bias', 'bilstm.bias_hh_l0', 'bilstm.weight_ih_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'bilstm.bias_ih_l0', 'crf.start_transitions', 'crf.end_transitions', 'crf.transitions', 'classifier.weight']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["RobertaLSTMCRF(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n","      (position_embeddings): Embedding(258, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (bilstm): LSTM(768, 384, batch_first=True, dropout=0.1, bidirectional=True)\n","  (classifier): Linear(in_features=768, out_features=5, bias=True)\n","  (crf): CRF(num_tags=5)\n",")"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["# Creating optimizer and lr schedulers\n","param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","num_train_optimization_steps = int(args.epochs*len(train_dataset)/args.batch_size/args.accumulation_steps)\n","optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=num_train_optimization_steps)  # PyTorch scheduler\n","scheduler0 = get_constant_schedule(optimizer)  # PyTorch scheduler\n","\n","tsfm = model.roberta\n","for child in tsfm.children():\n","    for param in child.parameters():\n","        if not param.requires_grad:\n","            print(\"whoopsies\")\n","        param.requires_grad = False\n","frozen = True\n","\n","EPOCHS = 10\n","MAX_GRAD_NORM = 1\n","\n","import time\n","\n","for epoch in range(EPOCHS):\n","    if epoch > 0 and frozen:\n","        for child in tsfm.children():\n","            for param in child.parameters():\n","                param.requires_grad = True\n","        frozen = False\n","        del scheduler0\n","        torch.cuda.empty_cache()\n","    st = time.time()\n","    print(f\"Training epoch: {epoch + 1}\")\n","    train_modified(epoch)\n","    labels, predictions = valid_modified(model, valid_loader)\n","    \n","    print('Time: ',time.time() - st)"],"metadata":{"id":"Ygafo_w0WzoM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640106347996,"user_tz":-420,"elapsed":4311859,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"36240787-4c49-46ac-e9ae-17c06cff9fe4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training loss epoch: 20.55725689663913 Training F1 epoch: 0.6859557171890042\n","Validation Loss: 8.222302045978484 Validation F1: 0.7467652495378928\n","Time:  397.2945246696472\n","Training epoch: 2\n","Training loss epoch: 15.062786915263192 Training F1 epoch: 0.8032530497341257\n","Validation Loss: 5.09008538918417 Validation F1: 0.8728949478748999\n","Time:  473.24821758270264\n","Training epoch: 3\n","Training loss epoch: 8.17585737718259 Training F1 epoch: 0.8943106312292359\n","Validation Loss: 4.090535315008111 Validation F1: 0.9047714918461849\n","Time:  472.9101982116699\n","Training epoch: 4\n","Training loss epoch: 5.621717192436177 Training F1 epoch: 0.9266417290108062\n","Validation Loss: 3.6883598285946038 Validation F1: 0.9090909090909091\n","Time:  472.8960266113281\n","Training epoch: 5\n","Training loss epoch: 4.464758409177019 Training F1 epoch: 0.9421178426686064\n","Validation Loss: 3.531988592095714 Validation F1: 0.915011263567479\n","Time:  471.5756878852844\n","Training epoch: 6\n","Training loss epoch: 3.35708157482043 Training F1 epoch: 0.9528365659612085\n","Validation Loss: 4.023460012967469 Validation F1: 0.9135702746365106\n","Time:  471.38697576522827\n","Training epoch: 7\n","Training loss epoch: 2.712613725922798 Training F1 epoch: 0.9606029106029107\n","Validation Loss: 4.4713851845329575 Validation F1: 0.9121054734318818\n","Time:  472.0966091156006\n","Training epoch: 8\n","Training loss epoch: 2.5252524301654002 Training F1 epoch: 0.9629513997294203\n","Validation Loss: 3.8572265124711835 Validation F1: 0.9115508885298869\n","Time:  471.18241119384766\n","Training epoch: 9\n","Training loss epoch: 2.021754098150248 Training F1 epoch: 0.9709586759654418\n","Validation Loss: 4.1755393241924015 Validation F1: 0.9247311827956989\n","Time:  471.5348255634308\n","Training epoch: 10\n","Training loss epoch: 1.8250824066408264 Training F1 epoch: 0.9730123238521138\n","Validation Loss: 3.7752516961814275 Validation F1: 0.9343598055105348\n","Time:  469.37305521965027\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"1tl_ObVfpWv4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Temp\n"],"metadata":{"id":"7k1WqTO59A5Z"}},{"cell_type":"code","source":["from vncorenlp import VnCoreNLP\n","\n","# To perform word segmentation, POS tagging, NER and then dependency parsing\n","annotator = VnCoreNLP(\"/content/VnCoreNLP/VnCoreNLP-1.1.1.jar\", annotators=\"wseg,pos,ner,parse\", max_heap_size='-Xmx2g') \n"," \n","    \n","# Input \n","text = \"Ông Nguyễn Khắc Chúc  đang làm việc tại Đại học Quốc gia Hà Nội. Bà Lan, vợ ông Chúc, cũng làm việc tại đây.\"\n","\n","# To perform word segmentation, POS tagging, NER and then dependency parsing\n","annotated_text = annotator.annotate(text)\n","\n","# To perform word segmentation only\n","word_segmented_text = annotator.tokenize(text) \n"],"metadata":{"id":"mYzxKxlX9CYp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Input \n","text = \"thô lỗ\"\n","\n","# To perform word segmentation, POS tagging, NER and then dependency parsing\n","annotated_text = annotator.annotate(text)\n","\n","# To perform word segmentation only\n","word_segmented_text = annotator.tokenize(text) \n","word_segmented_text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"32MdsTte-B-R","executionInfo":{"status":"ok","timestamp":1640146896575,"user_tz":-420,"elapsed":405,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"56323918-7024-491d-b21d-a07ae9a78c5a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['thô_lỗ']]"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["lines = ['mua được giá tốt lại được freeship mừng rơi nước_mắt đối_với tỉnh_lẻ thì tiền ship là 1 trở_ngại sản_phẩm quá ổn cảm_ơn shop cảm_ơn tiki this is english sentences cảm_ơn'] \n","tags = ['O,O,B-PRI,O,O,O,B-PRI,O,O,O,O,O,O,B-PRI,I-PRI,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O']\n","\n","ids, labels, masks = convert_lines(lines, tags, vocab, bpe, labels_to_ids)\n","for item in zip(ids[0], labels[0], masks[0]):\n","    print(f'{item[0]} \\t {item[1]} \\t {item[2]}')"],"metadata":{"id":"eSobWje-MLGt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"neLtVN_fXfeM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data - legacy\n","\n"],"metadata":{"id":"CAnYaObi9HXf"}},{"cell_type":"code","source":["# data_path = './data/seq_tag/tokens_labeled_final.xlsx'\n","data_path = './data/seq_tag/tokens_labeled_no_whitelist.csv'\n","data = prepare_dataset(data_path)\n","data.head(10)"],"metadata":{"id":"Lx7keeHx_A0F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = data_path\n","if path.split('.')[-1]=='xlsx':\n","    with open(path, 'rb') as f:\n","        data = pd.read_excel(f)\n","else:\n","    data = pd.read_csv(path, encoding='utf-8')\n","data.drop(columns=['Unnamed: 0'], inplace=True)\n","\n","data.rename(columns={'sentence': 'Sentence #', 'tokens': 'Word', 'tag': 'Tag'}, inplace=True)\n","print(data.shape)\n","label = pd.read_csv('./data/seq_tag/tokens_labeled_sentiment.csv')\n","\n","print(label.shape)\n","# data['label'] =\n"],"metadata":{"id":"Sj2LlnvoQX_A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data['Sentence #'] = data['Sentence #'].apply(lambda x: f'Sentence: {int(x+1)}')\n","\n","print(\"Number of tags: {}\".format(len(data.Tag.unique())))\n","\n","frequencies = data.Tag.value_counts()\n","tags = {}\n","for tag, count in zip(frequencies.index, frequencies):\n","    if tag != \"O\":\n","        if tag[2:5] not in tags.keys():\n","            tags[tag[2:5]] = count\n","        else:\n","            tags[tag[2:5]] += count\n","    continue\n","\n","print(sorted(tags.items(), key=lambda x: x[1], reverse=True))\n","\n","labels_to_ids = {k: v for v, k in enumerate(data.Tag.unique())}\n","ids_to_labels = {v: k for v, k in enumerate(data.Tag.unique())}\n","print(labels_to_ids)\n","\n","data = data.fillna(method='ffill')\n","print(data)\n","if path.split('.')[-1]=='csv':\n","    data['sentence'] = data[['Sentence #','Word','Tag', 'label']].groupby(['Sentence #'])['Word'].transform(lambda x: ' '.join(x))\n","    data['word_labels'] = data[['Sentence #','Word','Tag', 'label']].groupby(['Sentence #'])['Tag'].transform(lambda x: ','.join(x))\n","if path.split('.')[-1]=='xlsx':\n","    data['sentence'] = data.groupby(['Sentence #'])['Word'].transform(lambda x: ' '.join(str(v) for v in x))\n","    data['word_labels'] = data.groupby(['Sentence #'])['Tag'].transform(lambda x: ','.join(str(v) for v in x))\n","    # data['label'] = data.groupby(['Sentence #'])['label'].mean()\n","data = data.drop_duplicates(subset='sentence').reset_index(drop=True)\n"],"metadata":{"id":"Jl6IRWjWZxDI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label = []\n","for sent in data['Sentence #']:\n","    sent_id = int(sent.split(' ')[-1])\n","    label.append(df.label.iloc[sent_id-1])"],"metadata":{"id":"DfwcqFZBddmY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_excel('./data/clean/final6.xlsx')\n","df.drop(columns='Unnamed: 0', inplace=True)\n","df"],"metadata":{"id":"gMgiFxq-dUT4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data['label'] = label"],"metadata":{"id":"5SXWwbpieH2e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data.drop(columns=['Sentence #','Word','Tag'], inplace=True)\n","data.drop_duplicates(inplace=True)\n","data"],"metadata":{"id":"A0whzbX0eLJO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.to_csv('./data/joint/data_joint.csv')"],"metadata":{"id":"r4o7Ymn2enJ_"},"execution_count":null,"outputs":[]}]}