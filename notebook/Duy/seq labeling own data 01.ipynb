{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"seq labeling own data 01.ipynb","provenance":[{"file_id":"1bGb3oakV_VcEj8rORlff6ZAZTjPm1ZnM","timestamp":1639822270628},{"file_id":"https://github.com/NielsRogge/Transformers-Tutorials/blob/master/BERT/Custom_Named_Entity_Recognition_with_BERT_only_first_wordpiece.ipynb","timestamp":1639584213203}],"collapsed_sections":["sqDklprSqB5d","yQwQSVCAXPaX"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive._mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"j4yH5oIECEv7","executionInfo":{"status":"ok","timestamp":1639990133046,"user_tz":-420,"elapsed":20411,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"9dea93dd-abe0-49a1-e9ea-4f365ed4c864"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/NLP/project_nlp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nA2kNk4ULadb","executionInfo":{"status":"ok","timestamp":1640029252871,"user_tz":-420,"elapsed":6,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"a591e9fd-60c3-44f3-8d15-70b27f91ccc6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/NLP/project_nlp\n"]}]},{"cell_type":"markdown","source":["### Environment"],"metadata":{"id":"tW-3xdHzjf7v"}},{"cell_type":"markdown","source":["#### Install dependency"],"metadata":{"id":"6rAvHW6ti7-g"}},{"cell_type":"code","metadata":{"id":"j-sgUbzBXPZK","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640029271357,"user_tz":-420,"elapsed":18129,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"8c4a4e56-e39b-452c-ee0f-5469253e2a63"},"source":["!pip install transformers seqeval[gpu] -q\n","!pip install fairseq -q\n","!pip install fastBPE -q\n","!pip install pytorch-crf"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: pytorch-crf in /usr/local/lib/python3.7/dist-packages (0.7.2)\n"]}]},{"cell_type":"markdown","source":["#### Import libs and check environment"],"metadata":{"id":"wXy6Wq5rjFul"}},{"cell_type":"code","metadata":{"id":"IEnlUbgm8z3B"},"source":["import pandas as pd\n","import numpy as np\n","from sklearn.metrics import accuracy_score\n","import re\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","from transformers import BertTokenizerFast, BertConfig, BertForTokenClassification\n","from transformers import RobertaModel, RobertaConfig, BertPreTrainedModel, RobertaForTokenClassification\n","from transformers.modeling_outputs  import TokenClassifierOutput\n","\n","from torch.utils.data import TensorDataset\n","\n","import seqeval\n","from seqeval.metrics import classification_report, f1_score\n","\n","from torchcrf import CRF"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sm1krxJtKxpx","tags":[],"colab":{"base_uri":"https://localhost:8080/"},"outputId":"4ac2c127-e1e4-43ba-dd54-a2cf1fa262ee","executionInfo":{"status":"ok","timestamp":1640029275356,"user_tz":-420,"elapsed":25,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}}},"source":["from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","print(device)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["cuda\n"]}]},{"cell_type":"markdown","source":["### Dataset"],"metadata":{"id":"LIXPEAKxjjq9"}},{"cell_type":"markdown","source":["#### Read dataframe"],"metadata":{"id":"XH9amch_jNsJ"}},{"cell_type":"code","source":["data = pd.read_csv(\"./data/seq_tag/tokens_labeled_no_whitelist.csv\", encoding='utf-8')\n","data.drop(columns=['Unnamed: 0'], inplace=True)\n","data.rename(columns={'sentence': 'Sentence #', 'tokens': 'Word', 'tag': 'Tag'}, inplace=True)\n","data['Sentence #'] = data['Sentence #'].apply(lambda x: f'Sentence: {int(x+1)}')\n","data.head()"],"metadata":{"id":"VySdvWqlbYuR","colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"status":"ok","timestamp":1640029275357,"user_tz":-420,"elapsed":23,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"89110778-b178-4c24-f100-c97166764223"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-7b00274c-795e-4f4e-9d34-c9a58e3392c2\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence #</th>\n","      <th>Word</th>\n","      <th>Tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Sentence: 1</td>\n","      <td>combo</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Sentence: 1</td>\n","      <td>3</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Sentence: 1</td>\n","      <td>cái</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Sentence: 1</td>\n","      <td>giao</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Sentence: 1</td>\n","      <td>có</td>\n","      <td>O</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7b00274c-795e-4f4e-9d34-c9a58e3392c2')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-7b00274c-795e-4f4e-9d34-c9a58e3392c2 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-7b00274c-795e-4f4e-9d34-c9a58e3392c2');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["    Sentence #   Word Tag\n","0  Sentence: 1  combo   O\n","1  Sentence: 1      3   O\n","2  Sentence: 1    cái   O\n","3  Sentence: 1   giao   O\n","4  Sentence: 1     có   O"]},"metadata":{},"execution_count":5}]},{"cell_type":"code","source":["data.info()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qfU7IHjfEOlV","executionInfo":{"status":"ok","timestamp":1640029275358,"user_tz":-420,"elapsed":23,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"6626df26-4158-4906-8e4d-bd1467a1c79c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 89860 entries, 0 to 89859\n","Data columns (total 3 columns):\n"," #   Column      Non-Null Count  Dtype \n","---  ------      --------------  ----- \n"," 0   Sentence #  89860 non-null  object\n"," 1   Word        89860 non-null  object\n"," 2   Tag         89860 non-null  object\n","dtypes: object(3)\n","memory usage: 2.1+ MB\n"]}]},{"cell_type":"code","metadata":{"id":"6gMibEJXTKDw","colab":{"base_uri":"https://localhost:8080/"},"outputId":"5e8a5fcd-f69f-4d6e-c236-39529432eac0","executionInfo":{"status":"ok","timestamp":1640029275359,"user_tz":-420,"elapsed":20,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}}},"source":["data.count()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Sentence #    89860\n","Word          89860\n","Tag           89860\n","dtype: int64"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","metadata":{"tags":[],"id":"76iAeu2_XPZb","colab":{"base_uri":"https://localhost:8080/"},"outputId":"3bc5221c-c36c-4d7b-855c-0868ee70b2f7","executionInfo":{"status":"ok","timestamp":1640029275359,"user_tz":-420,"elapsed":17,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}}},"source":["print(\"Number of tags: {}\".format(len(data.Tag.unique())))\n","frequencies = data.Tag.value_counts()\n","frequencies"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of tags: 5\n"]},{"output_type":"execute_result","data":{"text/plain":["O        74100\n","B-DES    11522\n","I-DES     2757\n","B-PRI      985\n","I-PRI      496\n","Name: Tag, dtype: int64"]},"metadata":{},"execution_count":8}]},{"cell_type":"code","metadata":{"tags":[],"id":"LTFTa17UXPZf","colab":{"base_uri":"https://localhost:8080/"},"outputId":"a732be13-2845-4020-8909-34d20ac8f746","executionInfo":{"status":"ok","timestamp":1640029275360,"user_tz":-420,"elapsed":14,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}}},"source":["tags = {}\n","for tag, count in zip(frequencies.index, frequencies):\n","    if tag != \"O\":\n","        if tag[2:5] not in tags.keys():\n","            tags[tag[2:5]] = count\n","        else:\n","            tags[tag[2:5]] += count\n","    continue\n","\n","print(sorted(tags.items(), key=lambda x: x[1], reverse=True))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('DES', 14279), ('PRI', 1481)]\n"]}]},{"cell_type":"code","metadata":{"id":"CFRDM8WsQXvL","colab":{"base_uri":"https://localhost:8080/"},"outputId":"c2a609be-3899-4e19-85a4-07aaf9e75766","executionInfo":{"status":"ok","timestamp":1640029275884,"user_tz":-420,"elapsed":9,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}}},"source":["labels_to_ids = {k: v for v, k in enumerate(data.Tag.unique())}\n","ids_to_labels = {v: k for v, k in enumerate(data.Tag.unique())}\n","labels_to_ids"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'B-DES': 1, 'B-PRI': 3, 'I-DES': 2, 'I-PRI': 4, 'O': 0}"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","metadata":{"id":"zkW2vNcO-uMH","colab":{"base_uri":"https://localhost:8080/","height":677},"outputId":"07808556-d2a7-498e-976c-15a963f5d1bc","executionInfo":{"status":"ok","timestamp":1640029275885,"user_tz":-420,"elapsed":7,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}}},"source":["# pandas has a very handy \"forward fill\" function to fill missing values based on the last upper non-nan value\n","data = data.fillna(method='ffill')\n","data.head(20)"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-5b0e5703-4ad5-4a01-8c6b-972badcdb828\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence #</th>\n","      <th>Word</th>\n","      <th>Tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Sentence: 1</td>\n","      <td>combo</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Sentence: 1</td>\n","      <td>3</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Sentence: 1</td>\n","      <td>cái</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Sentence: 1</td>\n","      <td>giao</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Sentence: 1</td>\n","      <td>có</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Sentence: 1</td>\n","      <td>1</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Sentence: 1</td>\n","      <td>cái</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Sentence: 1</td>\n","      <td>,</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Sentence: 1</td>\n","      <td>thành_ra</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Sentence: 1</td>\n","      <td>đặt</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Sentence: 1</td>\n","      <td>6</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Sentence: 1</td>\n","      <td>cái</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>Sentence: 1</td>\n","      <td>nhận</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>Sentence: 1</td>\n","      <td>được</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>Sentence: 1</td>\n","      <td>4</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>Sentence: 1</td>\n","      <td>,</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>Sentence: 1</td>\n","      <td>hàng</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>Sentence: 1</td>\n","      <td>thì</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>Sentence: 1</td>\n","      <td>vải</td>\n","      <td>B-DES</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>Sentence: 1</td>\n","      <td>xấu</td>\n","      <td>B-DES</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-5b0e5703-4ad5-4a01-8c6b-972badcdb828')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-5b0e5703-4ad5-4a01-8c6b-972badcdb828 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-5b0e5703-4ad5-4a01-8c6b-972badcdb828');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["     Sentence #      Word    Tag\n","0   Sentence: 1     combo      O\n","1   Sentence: 1         3      O\n","2   Sentence: 1       cái      O\n","3   Sentence: 1      giao      O\n","4   Sentence: 1        có      O\n","5   Sentence: 1         1      O\n","6   Sentence: 1       cái      O\n","7   Sentence: 1         ,      O\n","8   Sentence: 1  thành_ra      O\n","9   Sentence: 1       đặt      O\n","10  Sentence: 1         6      O\n","11  Sentence: 1       cái      O\n","12  Sentence: 1      nhận      O\n","13  Sentence: 1      được      O\n","14  Sentence: 1         4      O\n","15  Sentence: 1         ,      O\n","16  Sentence: 1      hàng      O\n","17  Sentence: 1       thì      O\n","18  Sentence: 1       vải  B-DES\n","19  Sentence: 1       xấu  B-DES"]},"metadata":{},"execution_count":11}]},{"cell_type":"code","metadata":{"id":"Hmd-ow389k6Y","colab":{"base_uri":"https://localhost:8080/","height":206},"outputId":"7b1114cf-da9e-45e8-a58c-7d8567ae888b","executionInfo":{"status":"ok","timestamp":1640029278016,"user_tz":-420,"elapsed":2136,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}}},"source":["\n","# let's create a new column called \"sentence\" which groups the words by sentence \n","data['sentence'] = data[['Sentence #','Word','Tag']].groupby(['Sentence #'])['Word'].transform(lambda x: ' '.join(x))\n","# let's also create a new column called \"word_labels\" which groups the tags by sentence \n","data['word_labels'] = data[['Sentence #','Word','Tag']].groupby(['Sentence #'])['Tag'].transform(lambda x: ','.join(x))\n","data.head()"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-d50db98f-ff65-4f23-9ea1-3d29987a1acb\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence #</th>\n","      <th>Word</th>\n","      <th>Tag</th>\n","      <th>sentence</th>\n","      <th>word_labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Sentence: 1</td>\n","      <td>combo</td>\n","      <td>O</td>\n","      <td>combo 3 cái giao có 1 cái , thành_ra đặt 6 cái...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-DES,B-DE...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Sentence: 1</td>\n","      <td>3</td>\n","      <td>O</td>\n","      <td>combo 3 cái giao có 1 cái , thành_ra đặt 6 cái...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-DES,B-DE...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Sentence: 1</td>\n","      <td>cái</td>\n","      <td>O</td>\n","      <td>combo 3 cái giao có 1 cái , thành_ra đặt 6 cái...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-DES,B-DE...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Sentence: 1</td>\n","      <td>giao</td>\n","      <td>O</td>\n","      <td>combo 3 cái giao có 1 cái , thành_ra đặt 6 cái...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-DES,B-DE...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Sentence: 1</td>\n","      <td>có</td>\n","      <td>O</td>\n","      <td>combo 3 cái giao có 1 cái , thành_ra đặt 6 cái...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-DES,B-DE...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d50db98f-ff65-4f23-9ea1-3d29987a1acb')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-d50db98f-ff65-4f23-9ea1-3d29987a1acb button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-d50db98f-ff65-4f23-9ea1-3d29987a1acb');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["    Sentence #  ...                                        word_labels\n","0  Sentence: 1  ...  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-DES,B-DE...\n","1  Sentence: 1  ...  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-DES,B-DE...\n","2  Sentence: 1  ...  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-DES,B-DE...\n","3  Sentence: 1  ...  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-DES,B-DE...\n","4  Sentence: 1  ...  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-DES,B-DE...\n","\n","[5 rows x 5 columns]"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","metadata":{"id":"SrEgd4PZUgmF","colab":{"base_uri":"https://localhost:8080/","height":424},"outputId":"7b976655-02f4-4943-dd23-6e0b36f9f332","executionInfo":{"status":"ok","timestamp":1640029278017,"user_tz":-420,"elapsed":14,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}}},"source":["data = data[[\"sentence\", \"word_labels\"]].drop_duplicates().reset_index(drop=True)\n","data"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-b397c2e4-238e-4b67-bd53-76b6c2634d37\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>word_labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>combo 3 cái giao có 1 cái , thành_ra đặt 6 cái...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-DES,B-DE...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>mình mua áo có cổ màu trắng lại ship tới cho m...</td>\n","      <td>O,O,B-DES,B-DES,I-DES,B-DES,I-DES,O,O,O,O,O,B-...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>giao sai hàng . tôi muốn trả hàng . đặt be đậm...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,B-DES,I-DES,O,B-DES,I-DES</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>sản_xuất việt_nam nhưng thấy in chữ trung_quốc...</td>\n","      <td>O,O,O,O,B-DES,I-DES,O,O,O,O,O,O,O</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>mình đặt áo sơ_mi trắng dài tay mà shop giao c...</td>\n","      <td>O,O,B-DES,I-DES,B-DES,B-DES,I-DES,O,O,O,O,O,O,...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3652</th>\n","      <td>chất_lượng sản_phẩm giống mô tả.giao hàng nhan...</td>\n","      <td>O,O,O,O,O,O,O,O</td>\n","    </tr>\n","    <tr>\n","      <th>3653</th>\n","      <td>hài_lòng vô_cùng , giao nhanh , nhân_viên giao...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,O,B-DES,O,B-DES</td>\n","    </tr>\n","    <tr>\n","      <th>3654</th>\n","      <td>sản_phẩm ổn . giá phải_chăng . thật_sự là nhận...</td>\n","      <td>O,O,O,B-PRI,B-PRI,O,O,O,O,O,O,O,O,O,O,O,O,O</td>\n","    </tr>\n","    <tr>\n","      <th>3655</th>\n","      <td>hàng đẹp chuẩn chất_lượng . nếu áo có đai ngan...</td>\n","      <td>O,B-DES,O,O,O,O,B-DES,O,O,O,B-DES,O,B-DES,O,O,O</td>\n","    </tr>\n","    <tr>\n","      <th>3656</th>\n","      <td>vải mặc mát , dày_dặn . không có miếng mút ngự...</td>\n","      <td>B-DES,O,B-DES,O,B-DES,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3657 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-b397c2e4-238e-4b67-bd53-76b6c2634d37')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-b397c2e4-238e-4b67-bd53-76b6c2634d37 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-b397c2e4-238e-4b67-bd53-76b6c2634d37');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                               sentence                                        word_labels\n","0     combo 3 cái giao có 1 cái , thành_ra đặt 6 cái...  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-DES,B-DE...\n","1     mình mua áo có cổ màu trắng lại ship tới cho m...  O,O,B-DES,B-DES,I-DES,B-DES,I-DES,O,O,O,O,O,B-...\n","2     giao sai hàng . tôi muốn trả hàng . đặt be đậm...      O,O,O,O,O,O,O,O,O,O,B-DES,I-DES,O,B-DES,I-DES\n","3     sản_xuất việt_nam nhưng thấy in chữ trung_quốc...                  O,O,O,O,B-DES,I-DES,O,O,O,O,O,O,O\n","4     mình đặt áo sơ_mi trắng dài tay mà shop giao c...  O,O,B-DES,I-DES,B-DES,B-DES,I-DES,O,O,O,O,O,O,...\n","...                                                 ...                                                ...\n","3652  chất_lượng sản_phẩm giống mô tả.giao hàng nhan...                                    O,O,O,O,O,O,O,O\n","3653  hài_lòng vô_cùng , giao nhanh , nhân_viên giao...                O,O,O,O,O,O,O,O,O,O,O,B-DES,O,B-DES\n","3654  sản_phẩm ổn . giá phải_chăng . thật_sự là nhận...        O,O,O,B-PRI,B-PRI,O,O,O,O,O,O,O,O,O,O,O,O,O\n","3655  hàng đẹp chuẩn chất_lượng . nếu áo có đai ngan...    O,B-DES,O,O,O,O,B-DES,O,O,O,B-DES,O,B-DES,O,O,O\n","3656  vải mặc mát , dày_dặn . không có miếng mút ngự...  B-DES,O,B-DES,O,B-DES,O,O,O,O,O,O,O,O,O,O,O,O,...\n","\n","[3657 rows x 2 columns]"]},"metadata":{},"execution_count":13}]},{"cell_type":"markdown","source":["#### Expand the labels with subword based tokenizer"],"metadata":{"id":"xg3Fz7_NjsD7"}},{"cell_type":"markdown","source":["*PhoBERT sử dụng RDRSegmenter của VnCoreNLP (đã thực hiện trong dataframe) để tách từ cho dữ liệu đầu vào trước khi qua BPE encoder.*"],"metadata":{"id":"OE6nV8ydL_Zy"}},{"cell_type":"code","source":["from fairseq.data.encoders.fastbpe import fastBPE\n","from fairseq.data import Dictionary\n","import argparse\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--bpe-codes', \n","    default=\"./PhoBERT_base_transformers/bpe.codes\",\n","    required=False,\n","    type=str,\n","    help='path to fastBPE BPE'\n",")\n","args, unknown = parser.parse_known_args()\n","bpe = fastBPE(args)\n","\n","# Load the dictionary\n","vocab = Dictionary()\n","vocab.add_from_file(\"./PhoBERT_base_transformers/dict.txt\")"],"metadata":{"id":"a5p-6_eHLuqW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_to_ids['X'] = -100"],"metadata":{"id":"0_RR_SSV5cqa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_to_ids"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hpmdI2IP7kmY","executionInfo":{"status":"ok","timestamp":1640029278486,"user_tz":-420,"elapsed":5,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"bb0098e3-5ea5-4473-f6b9-413e972bb4a9"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'B-DES': 1, 'B-PRI': 3, 'I-DES': 2, 'I-PRI': 4, 'O': 0, 'X': -100}"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["from tqdm import tqdm\n","def convert_lines(lines, tags, vocab, bpe, max_sequence_length=256):\n","    \"\"\"\n","    lines: list các văn bản input\n","    tags: list các chuỗi tag\n","    vocab: từ điển dùng để encoding subwords\n","    bpe: \n","    \"\"\"\n","    # Index của các token cls (đầu câu), eos (cuối câu), padding (padding token)\n","    outputs = np.zeros((len(lines), max_sequence_length), dtype=np.int32) # --> shape (number_lines, max_seq_len)\n","    outputs_labels = np.zeros((len(lines), max_sequence_length), dtype=np.int32)\n","    outputs_attention_mask = np.zeros((len(lines), max_sequence_length), dtype=np.int32)\n","    # Index của các token cls (đầu câu), eos (cuối câu), padding (padding token)\n","    cls_id = 0\n","    eos_id = 2\n","    pad_id = 1\n","    \n","    for idx, row in tqdm(enumerate(lines), total=len(lines)): \n","        # Mã hóa subwords theo byte pair encoding(bpe)\n","        subwords = bpe.encode(row)\n","        subwords = '<s> '+ subwords +' </s>'\n","        input_ids = vocab.encode_line(subwords, append_eos=False, add_if_not_exist=False).long().tolist()\n","        \n","        tag_list = ['O'] + tags[idx].split(',') + ['O']\n","        subword_idx = [subwords.split().index(word) for word in subwords.split() if '@@' in word]\n","        for i, orig_idx in enumerate(subword_idx):\n","            tag_list.insert(orig_idx+1, 'X')\n","        # print(tag_list)\n","        labels = [labels_to_ids[label] for label in tag_list] \n","\n","        # Truncate input nếu độ dài vượt quá max_seq_len\n","        if len(input_ids) > max_sequence_length: \n","            input_ids = input_ids[:max_sequence_length]\n","            input_ids[-1] = eos_id\n","            labels = labels[:max_sequence_length]\n","            labels[-1] = -100\n","        else:\n","        # Padding nếu độ dài câu chưa bằng max_seq_len\n","            input_ids = input_ids + [pad_id, ]*(max_sequence_length - len(input_ids))\n","            labels = labels + [-100, ]*(max_sequence_length - len(labels))\n","        \n","        labels[0] = -100\n","        # print(len(labels))\n","        labels[np.where(np.array(input_ids)==eos_id)[0][0]] = -100\n","        # print(np.where(np.array(input_ids)==eos_id)[0][0])\n","        # labels[input_ids==eos_id] = -100\n","        outputs[idx,:] = np.array(input_ids)\n","        outputs_labels[idx,:] = np.array(labels)\n","        outputs_attention_mask[idx, np.array(input_ids)!=pad_id] = 1\n","\n","    return outputs, outputs_labels, outputs_attention_mask\n","\n","lines = ['mua được giá tốt lại được freeship mừng rơi nước_mắt đối_với tỉnh_lẻ thì tiền ship là 1 trở_ngại sản_phẩm quá ổn cảm_ơn shop cảm_ơn tiki this is english sentences cảm_ơn'] \n","tags = ['O,O,B-PRI,O,O,O,B-PRI,O,O,O,O,O,O,B-PRI,I-PRI,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O']\n","\n","ids, labels, masks = convert_lines(lines, tags, vocab, bpe)\n","# print('input_ids tensor encode: {}\\n, shape: {}\\n'.format(ids[:10], ids.size))\n","# print('label_ids tensor encode: {}\\n, shape: {}\\n'.format(labels[:10], labels.size))\n","# print('masks tensor encode: {}\\n, shape: {}\\n'.format(masks[:10], masks.size))\n","# print('x1 tensor decode: ', phoBERT_cls.decode(torch.tensor(x1))[:103])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"09vbPI7Yj57U","executionInfo":{"status":"ok","timestamp":1640029278867,"user_tz":-420,"elapsed":18,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"17ad61ff-14ad-479d-ac93-2266f91e1036"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00, 423.03it/s]\n"]}]},{"cell_type":"code","source":["for item in zip(ids[0], labels[0], masks[0]):\n","    print(item)"],"metadata":{"id":"SnQb1qHR5h4H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640029278868,"user_tz":-420,"elapsed":14,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"7f0eba27-5a49-45dd-ff5c-f3b3becf7d0a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(0, -100, 1)\n","(188, 0, 1)\n","(11, 0, 1)\n","(133, 3, 1)\n","(167, 0, 1)\n","(44, 0, 1)\n","(11, 0, 1)\n","(18288, 3, 1)\n","(2438, -100, 1)\n","(56679, -100, 1)\n","(2766, 0, 1)\n","(891, 0, 1)\n","(2396, 0, 1)\n","(190, 0, 1)\n","(27159, 0, 1)\n","(54, 0, 1)\n","(123, 3, 1)\n","(16132, 4, 1)\n","(8, 0, 1)\n","(99, 0, 1)\n","(5769, 0, 1)\n","(265, 0, 1)\n","(204, 0, 1)\n","(4752, 0, 1)\n","(2321, 0, 1)\n","(9405, 0, 1)\n","(2321, 0, 1)\n","(2081, 0, 1)\n","(5418, -100, 1)\n","(22304, 0, 1)\n","(2573, 0, 1)\n","(15601, 0, 1)\n","(2455, -100, 1)\n","(14641, -100, 1)\n","(1302, 0, 1)\n","(6502, -100, 1)\n","(26442, -100, 1)\n","(2321, 0, 1)\n","(2, -100, 1)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n","(1, -100, 0)\n"]}]},{"cell_type":"code","metadata":{"id":"lgNSM8Xz79Mg","tags":[]},"source":["MAX_LEN = 128\n","TRAIN_BATCH_SIZE = 4\n","VALID_BATCH_SIZE = 2\n","EPOCHS = 1\n","LEARNING_RATE = 1e-05\n","MAX_GRAD_NORM = 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X, Y_label, Y_mask = convert_lines(data.sentence.values, data.word_labels.values, vocab, bpe, max_sequence_length=MAX_LEN)\n","print('X shape: ', X.shape)\n","print('Y label shape', Y_label.shape)\n","print('Y mask shape', Y_mask.shape)"],"metadata":{"id":"a1qgmZJX9t-x","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640029280023,"user_tz":-420,"elapsed":1165,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"b496326c-8dd2-47a1-8fb0-cc6b3b673489"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 3657/3657 [00:01<00:00, 2933.63it/s]"]},{"output_type":"stream","name":"stdout","text":["X shape:  (3657, 128)\n","Y label shape (3657, 128)\n","Y mask shape (3657, 128)\n"]},{"output_type":"stream","name":"stderr","text":["\n"]}]},{"cell_type":"code","source":["import pickle\n","\n","def _save_pkl(path, obj):\n","  with open(path, 'wb') as f:\n","    pickle.dump(obj, f)\n","\n","def _load_pkl(path):\n","  with open(path, 'rb') as f:\n","    obj = pickle.load(f)\n","  return obj\n","\n","_save_pkl('./data/processed/X.pkl', X)\n","_save_pkl('./data/processed/Y_label.pkl', Y_label)\n","_save_pkl('./data/processed/Y_mask.pkl', Y_mask)"],"metadata":{"id":"cDL0RTsU-fzG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["X = _load_pkl('./data/processed/X.pkl')\n","Y_label = _load_pkl('./data/processed/Y_label.pkl')\n","Y_mask = _load_pkl('./data/processed/Y_mask.pkl')\n","\n","print('length of X: ', len(X))\n","print('length of y: ', len(Y_label))\n","print('length of y: ', len(Y_mask))"],"metadata":{"id":"K7noI0nS-hwg","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640029280024,"user_tz":-420,"elapsed":15,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"3e5e8f93-5688-4c32-f21d-c125dce478f4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["length of X:  3657\n","length of y:  3657\n","length of y:  3657\n"]}]},{"cell_type":"markdown","metadata":{"id":"Ptv5AT_iTb7W"},"source":["Let's have a look at the first training example:"]},{"cell_type":"markdown","metadata":{"id":"VvU4nzL2W2Xo"},"source":["Let's verify that the input ids and corresponding targets are correct:"]},{"cell_type":"code","source":["def decode(tokens: torch.LongTensor, labels: torch.LongTensor):\n","    print(tokens.dim())\n","    assert tokens.dim() == 1\n","    assert labels.dim() == 1\n","    tokens = tokens.numpy()\n","    labels - labels.numpy()\n","    if tokens[0] == vocab.bos():\n","        tokens = tokens[1:]  # remove <s>\n","    eos_mask = tokens == vocab.eos()\n","    doc_mask = eos_mask[1:] & eos_mask[:-1]\n","    sentences = np.split(tokens,  doc_mask.nonzero()[0] + 1)\n","    labels = np.split(labels, doc_mask.nonzero()[0] + 1)\n","    sentences = [\n","        bpe.decode(vocab.string(s)) for s in sentences\n","    ]\n","    labels = [np.delete(l, np.where(l == -100))[:-1] for l in labels]\n","    if len(sentences) == 1:\n","        return sentences[0], labels[0]\n","    return sentences, labels"],"metadata":{"id":"4JyI2WGbpIO1"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DWgnNJrYW2GP","tags":[],"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640029280025,"user_tz":-420,"elapsed":12,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"f6ceefb5-9410-4698-fb39-0432bdf06e7f"},"source":["test_idx = 0\n","sentences, labels = decode(torch.tensor(X[test_idx]), torch.tensor(Y_label[test_idx]))\n","for token, label in zip(sentences.split(), labels):\n","  print('{0:10}  {1:10} {2:10}'.format(token, label, ids_to_labels[int(label)]))"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["1\n","combo                0 O         \n","3                    0 O         \n","cái                  0 O         \n","giao                 0 O         \n","có                   0 O         \n","1                    0 O         \n","cái                  0 O         \n",",                    0 O         \n","thành_ra             0 O         \n","đặt                  0 O         \n","6                    0 O         \n","cái                  0 O         \n","nhận                 0 O         \n","được                 0 O         \n","4                    0 O         \n",",                    0 O         \n","hàng                 0 O         \n","thì                  0 O         \n","vải                  1 B-DES     \n","xấu                  1 B-DES     \n","giống                0 O         \n","vải                  1 B-DES     \n","áo_mưa               1 B-DES     \n",",                    0 O         \n","con                  0 O         \n","trai                 0 O         \n","chê                  0 O         \n","vứt                  0 O         \n","đi                   0 O         \n"]}]},{"cell_type":"markdown","source":["### Training"],"metadata":{"id":"9d7dOlWeJ8z8"}},{"cell_type":"markdown","metadata":{"id":"73OzU7oXRxR8"},"source":["#### **Defining the model**"]},{"cell_type":"code","source":["class argu():\n","    def __init__(self):\n","        # self.train_path = './data/train.csv'\n","        self.dict_path = \"./PhoBERT_base_transformers/dict.txt\"\n","        self.config_path = \"./PhoBERT_base_transformers/config.json\"\n","        # self.rdrsegmenter_path = '/content/vncorenlp/VnCoreNLP-1.1.1.jar'\n","        self.pretrained_path = './PhoBERT_base_transformers/model.bin'\n","        self.max_sequence_length = 128\n","        self.batch_size = 8\n","        self.accumulation_steps = 1\n","        self.epochs = 10\n","        self.seed = 69\n","        self.fold = 0\n","        self.lr= 1e-3\n","        self.ckpt_path = './checkpoints'\n","        self.bpe_codes = \"./PhoBERT_base_transformers/bpe.codes\"\n","args = argu()"],"metadata":{"id":"pEWFugm23Vcg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["config = RobertaConfig.from_pretrained(\n","    args.config_path,\n","    output_hidden_states=True,\n","    return_dict=True,\n","    num_labels=5,\n","    classifier_dropout = True,\n","    pad_token_id = 1,\n","    bos_token_id = 0,\n","    eos_token_id = 2\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xVJlt2kL3OjH","executionInfo":{"status":"ok","timestamp":1640029280026,"user_tz":-420,"elapsed":10,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"21bf424a-73cb-4fbe-e6ce-612df7558d7b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["You are using a model of type bert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n"]}]},{"cell_type":"code","source":["train_size = 0.8\n","def train_test_split(data, train_size):\n","    X_df = pd.DataFrame(data)\n","    X_train = X_df.sample(frac = train_size, random_state=200)\n","    X_test = X_df.drop(X_train.index).reset_index(drop=True)\n","    X_train = X_train.reset_index(drop=True)\n","    return X_train.values, X_test .values\n","\n","X_train, X_test = train_test_split(X, train_size)\n","Y_label_train, Y_label_test = train_test_split(Y_label, train_size)\n","Y_mask_train, Y_mask_test = train_test_split(Y_mask, train_size)"],"metadata":{"id":"EL8MbWWd79U_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_dataset = TensorDataset(torch.tensor(X_train,dtype=torch.long), \n","                              torch.tensor(Y_label_train,dtype=torch.long))\n","\n","valid_dataset = TensorDataset(torch.tensor(X_test,dtype=torch.long), \n","                              torch.tensor(Y_label_test,dtype=torch.long))\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n","valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=VALID_BATCH_SIZE, shuffle=False)"],"metadata":{"id":"lG9-xqY8_0xT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.utils.class_weight import compute_class_weight\n","y_org = Y_label_train\n","class_weight = compute_class_weight(class_weight='balanced', classes = np.array([0,1,2,3,4]), y=y_org.flatten()[y_org.flatten()>=0])"],"metadata":{"id":"lv4vnxZifVPk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class_weight"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ifs-50S6gaIX","executionInfo":{"status":"ok","timestamp":1640029280482,"user_tz":-420,"elapsed":5,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"334a46e8-f1d5-4532-d4c4-3af79748bfda"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0.24253478,  1.55774552,  6.60394231, 17.83922078, 36.43554377])"]},"metadata":{},"execution_count":30}]},{"cell_type":"code","source":["from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","class Roberta_SeqTag(BertPreTrainedModel):\n","    config_class = RobertaConfig\n","    base_model_prefix = \"roberta\"\n","    def __init__(self, config):\n","        super(Roberta_SeqTag, self).__init__(config)\n","        self.num_labels = config.num_labels\n","        self.roberta = RobertaModel(config, add_pooling_layer=False)\n","\n","        classifier_dropout = (\n","                config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n","            )\n","        self.dropout = nn.Dropout(classifier_dropout)\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","        # self.lstm = nn.LSTM(config.hidden_size, 256, num_layers=1, bidirectional=True)\n","        # self.classifier = nn.Linear(256*2, config.num_labels)\n","        self.post_init()\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","\n","        outputs = self.roberta(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        sequence_output = outputs[0]\n","\n","        sequence_output = self.dropout(sequence_output)\n","        # x, _ = self.lstm(sequence_output)\n","        # sequence_output = torch.tanh(x)\n","        logits = self.classifier(sequence_output)\n","        # print(logits.shape)\n","        loss = None\n","        if labels is not None:\n","            loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(class_weight, dtype=torch.float).to(device))\n","            # Only keep active parts of the loss\n","            if attention_mask is not None:\n","                active_loss = attention_mask.view(-1) == 1\n","                active_logits = logits.view(-1, self.num_labels)\n","                active_labels = torch.where(\n","                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n","                )\n","                # print(active_logits, active_labels)\n","                loss = loss_fct(active_logits, active_labels)\n","            else:\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","\n","        if not return_dict:\n","            output = (logits,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return TokenClassifierOutput(\n","            loss=loss,\n","            logits=logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )\n","\n","\n","class BertNER(BertPreTrainedModel):\n","    def __init__(self, config):\n","        super(BertNER, self).__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.bert = RobertaModel(config, add_pooling_layer=False)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","        self.crf = CRF(config.num_labels, batch_first=True)\n","\n","        self.init_weights()\n","\n","    def forward(self, input_data, token_type_ids=None, attention_mask=None, labels=None,\n","                position_ids=None, inputs_embeds=None, head_mask=None):\n","        input_ids, input_token_starts = input_data\n","        outputs = self.bert(input_ids,\n","                            attention_mask=attention_mask,\n","                            token_type_ids=token_type_ids,\n","                            position_ids=position_ids,\n","                            head_mask=head_mask,\n","                            inputs_embeds=inputs_embeds)\n","        sequence_output = outputs[0]\n","\n","        # 去除[CLS]标签等位置，获得与label对齐的pre_label表示\n","        origin_sequence_output = [layer[starts.nonzero().squeeze(1)]\n","                                  for layer, starts in zip(sequence_output, input_token_starts)]\n","        # 将sequence_output的pred_label维度padding到最大长度\n","        padded_sequence_output = pad_sequence(origin_sequence_output, batch_first=True)\n","        # dropout pred_label的一部分feature\n","        padded_sequence_output = self.dropout(padded_sequence_output)\n","        # 得到判别值\n","        logits = self.classifier(padded_sequence_output)\n","        outputs = (logits,)\n","        if labels is not None:\n","            loss_mask = labels.gt(-1)\n","            loss = self.crf(logits, labels, loss_mask) * (-1)\n","            outputs = (loss,) + outputs\n","\n","        # contain: (loss), scores\n","        return outputs\n","\n"],"metadata":{"id":"qa8o3Sn03Ae0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# model = Roberta_SeqTag.from_pretrained(args.pretrained_path, config=config)\n","# model.cuda()\n","\n","# model = BertNER.from_pretrained(args.pretrained_path, config=config)\n","# model.cuda()\n","\n","\n","# model = RobertaForTokenClassification.from_pretrained(args.pretrained_path, config=config)\n","# model.cuda()\n","\n","from transformers import AutoModel, AutoTokenizer, AutoModelForTokenClassification\n","\n","model = AutoModelForTokenClassification.from_pretrained(\"vinai/phobert-base\", num_labels=5)\n","model.cuda()"],"metadata":{"id":"CdnBtsTP2XiN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def check(ids, mask, labels):\n","    print(ids.shape, mask.shape, labels.shape)\n","    for id, mask, label in zip(ids, mask, labels):\n","        for item in zip(id, mask, label):\n","            print(item)\n","        # break"],"metadata":{"id":"uLdpyscp3Lyp"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GLFivpkwW1HY"},"source":["# Defining the training function on the 80% of the dataset for tuning the bert model\n","def train(epoch, verbose = False):\n","    tr_loss, tr_accuracy = 0, 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","    tr_preds, tr_labels = [], []\n","    # put model in training mode\n","    model.train()\n","    \n","    for idx, batch in enumerate(train_loader):\n","        ids, labels = batch\n","        ids = ids.to(device)\n","        labels = labels.to(device)\n","        mask = ids!=1\n","        \n","        # check(ids, mask, labels)\n","        # break\n","\n","        outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n","        loss = outputs[0]\n","        \n","        tr_logits = outputs[1]\n","        tr_loss += loss.item()\n","\n","        nb_tr_steps += 1\n","        nb_tr_examples += labels.size(0)\n","        \n","        if idx % 100==0 and verbose:\n","            loss_step = tr_loss/nb_tr_steps\n","            print(f\"Training loss per 100 training steps: {loss_step}\")\n","           \n","        # compute training accuracy\n","        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","        # print(active_logits)\n","        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","        # print(flattened_predictions)\n","        \n","        # only compute accuracy at active labels\n","        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n","        \n","        labels = torch.masked_select(flattened_targets, active_accuracy)\n","        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","        \n","        tr_labels.extend(labels)\n","        tr_preds.extend(predictions)\n","\n","        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n","        tr_accuracy += tmp_tr_accuracy\n","    \n","        # gradient clipping\n","        torch.nn.utils.clip_grad_norm_(\n","            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n","        )\n","        \n","        # backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    epoch_loss = tr_loss / nb_tr_steps\n","    tr_accuracy = tr_accuracy / nb_tr_steps\n","\n","    labels = [ids_to_labels[id.item()] for id in tr_labels]\n","    predictions = [ids_to_labels[id.item()] for id in tr_preds]\n","    f1 = seqeval.metrics.f1_score([labels], [predictions])\n","\n","    print(f\"Training loss epoch: {epoch_loss}\", f\"Training F1 epoch: {f1}\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RIVVfFHi7Aw7"},"source":["def valid(model, test_loader, verbose=False):\n","    # put model in evaluation mode\n","    model.eval()\n","    \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_examples, nb_eval_steps = 0, 0\n","    eval_preds, eval_labels = [], []\n","    \n","    with torch.no_grad():\n","        for idx, batch in enumerate(test_loader):\n","            \n","            ids, labels = batch\n","            ids = ids.to(device)\n","            labels = labels.to(device)\n","            mask = ids!=1 \n","\n","            outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n","            loss = outputs[0]\n","            eval_logits = outputs[1]\n","            eval_loss += loss.item()\n","\n","            nb_eval_steps += 1\n","            nb_eval_examples += labels.size(0)\n","        \n","            if idx % 100==0 and verbose:\n","                loss_step = eval_loss/nb_eval_steps\n","                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n","              \n","            # compute evaluation accuracy\n","            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","            \n","            # only compute accuracy at active labels\n","            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","        \n","            labels = torch.masked_select(flattened_targets, active_accuracy)\n","            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","            \n","            eval_labels.extend(labels)\n","            eval_preds.extend(predictions)\n","            \n","            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n","            eval_accuracy += tmp_eval_accuracy\n","\n","    labels = [ids_to_labels[id.item()] for id in eval_labels]\n","    predictions = [ids_to_labels[id.item()] for id in eval_preds]\n","    \n","    eval_loss = eval_loss / nb_eval_steps\n","    eval_accuracy = eval_accuracy / nb_eval_steps\n","    f1 = seqeval.metrics.f1_score([labels], [predictions])\n","    print(f\"Validation Loss: {eval_loss}\", f\"Validation F1: {f1}\")\n","\n","    return labels, predictions"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"k2dsCyP7dcF3"},"source":["And let's train the model!"]},{"cell_type":"code","source":["from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule"],"metadata":{"id":"_S2yCqwNo-1J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating optimizer and lr schedulers\n","param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","num_train_optimization_steps = int(args.epochs*len(train_dataset)/args.batch_size/args.accumulation_steps)\n","optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=num_train_optimization_steps)  # PyTorch scheduler\n","scheduler0 = get_constant_schedule(optimizer)  # PyTorch scheduler\n"],"metadata":{"id":"Cw1s-qMai4fY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tsfm = model.roberta\n","# tsfm = model.bert\n","for child in tsfm.children():\n","    for param in child.parameters():\n","        if not param.requires_grad:\n","            print(\"whoopsies\")\n","        param.requires_grad = False\n","frozen = True"],"metadata":{"id":"hA4_1KRAaA63"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"y07Ybw8rZeZ7","tags":[],"colab":{"base_uri":"https://localhost:8080/","height":953},"outputId":"6b8bbc21-e540-4379-d620-fdf4aa839d96","executionInfo":{"status":"error","timestamp":1640028207868,"user_tz":-420,"elapsed":600214,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}}},"source":["EPOCHS = 50\n","import time\n","\n","for epoch in range(EPOCHS):\n","    if epoch > 0 and frozen:\n","        for child in tsfm.children():\n","            for param in child.parameters():\n","                param.requires_grad = True\n","        frozen = False\n","        del scheduler0\n","        torch.cuda.empty_cache()\n","    st = time.time()\n","    print(f\"Training epoch: {epoch + 1}\")\n","    train(epoch)\n","    labels, predictions = valid(model, valid_loader)\n","    \n","    print('Time: ',time.time() - st)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training epoch: 1\n","Training loss epoch: 0.5083288725852315 Training F1 epoch: 0.3209125475285171\n","Validation Loss: 0.3665421425766958 Validation F1: 0.5707162284678151\n","Time:  23.1031973361969\n","Training epoch: 2\n","Training loss epoch: 0.1407387124016993 Training F1 epoch: 0.8277699420153581\n","Validation Loss: 0.07579330769952773 Validation F1: 0.9144363341443633\n","Time:  72.79584550857544\n","Training epoch: 3\n","Training loss epoch: 0.05879130292651396 Training F1 epoch: 0.9288209834697815\n","Validation Loss: 0.0694106772027196 Validation F1: 0.9253974724826742\n","Time:  72.46054148674011\n","Training epoch: 4\n","Training loss epoch: 0.03974647820874561 Training F1 epoch: 0.9483019458500703\n","Validation Loss: 0.08786922149225634 Validation F1: 0.9119804400977995\n","Time:  72.0927665233612\n","Training epoch: 5\n","Training loss epoch: 0.023982190624452126 Training F1 epoch: 0.9653699942717284\n","Validation Loss: 0.06650996672552076 Validation F1: 0.9375127213515164\n","Time:  72.27134609222412\n","Training epoch: 6\n","Training loss epoch: 0.018434886977703454 Training F1 epoch: 0.9721078150252853\n","Validation Loss: 0.0772315645607521 Validation F1: 0.9257892578925788\n","Time:  72.12101197242737\n","Training epoch: 7\n","Training loss epoch: 0.015218196629794362 Training F1 epoch: 0.9790960746494292\n","Validation Loss: 0.07679231313544772 Validation F1: 0.9289371288142536\n","Time:  75.70016765594482\n","Training epoch: 8\n","Training loss epoch: 0.017128514833142407 Training F1 epoch: 0.974554176660757\n","Validation Loss: 0.07574338403915408 Validation F1: 0.9377576257213521\n","Time:  72.32974123954773\n","Training epoch: 9\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-145-181a4d416318>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training epoch: {epoch + 1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-138-b4bdcca12be7>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, verbose)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mtr_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtr_accuracy\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnb_tr_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mids_to_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtr_labels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mids_to_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtr_preds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseqeval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-138-b4bdcca12be7>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mtr_accuracy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtr_accuracy\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnb_tr_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mids_to_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtr_labels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mids_to_labels\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mid\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtr_preds\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m     \u001b[0mf1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseqeval\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mpredictions\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","metadata":{"id":"0jDNXrjr-6BW","tags":[]},"source":["print(f1_score([labels], [predictions]))\n","print(classification_report([labels], [predictions]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### BiLSTM-CRF"],"metadata":{"id":"0Z1tCKaDTMYU"}},{"cell_type":"code","source":["class BertLstmCRF(BertPreTrainedModel):\n","\n","    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.bert = RobertaModel(config, add_pooling_layer=False)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.bilstm = nn.LSTM(config.hidden_size, (config.hidden_size) // 2, dropout=0.1, batch_first=True,\n","                              bidirectional=True)\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","        self.crf = CRF(num_tags=config.num_labels, batch_first=True)\n","        self.init_weights()\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","        r\"\"\"\n","        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n","            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n","            1]``.\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        sequence_output = outputs[0]\n","        sequence_output = self.dropout(sequence_output)\n","        lstm_output, hc = self.bilstm(sequence_output)\n","        logits = self.classifier(lstm_output)\n","\n","        loss = None\n","        if labels is not None:\n","            # print(labels)\n","            labels[labels==-100] = 0\n","            log_likelihood, tags = self.crf(logits, labels), self.crf.decode(logits)\n","            loss = 0 - log_likelihood\n","        else:\n","            tags = self.crf.decode(logits)\n","        tags = torch.Tensor(tags)\n","\n","        if not return_dict:\n","            output = (tags,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return loss, tags"],"metadata":{"id":"gUySqAQ9TPsw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import BertPreTrainedModel, BertModel, RobertaModel\n","\n","model = BertLstmCRF.from_pretrained(\"vinai/phobert-base\", num_labels=5)\n","model.cuda()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0PW5LApGTUAh","executionInfo":{"status":"ok","timestamp":1640029293935,"user_tz":-420,"elapsed":5050,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"e678ee77-328d-4bfe-9b7b-317ed8a01c15"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["You are using a model of type roberta to instantiate a model of type bert. This is not supported for all configurations of models and can yield errors.\n","/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","Some weights of the model checkpoint at vinai/phobert-base were not used when initializing BertLstmCRF: ['roberta.encoder.layer.1.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.self.value.weight', 'roberta.encoder.layer.2.attention.output.LayerNorm.weight', 'roberta.encoder.layer.10.attention.self.query.bias', 'roberta.encoder.layer.11.attention.self.value.bias', 'roberta.encoder.layer.7.output.LayerNorm.weight', 'roberta.encoder.layer.1.output.LayerNorm.weight', 'roberta.encoder.layer.9.intermediate.dense.bias', 'roberta.encoder.layer.7.attention.output.dense.weight', 'roberta.encoder.layer.6.attention.self.query.bias', 'roberta.encoder.layer.0.intermediate.dense.bias', 'roberta.encoder.layer.11.output.dense.bias', 'roberta.encoder.layer.5.attention.self.query.weight', 'roberta.encoder.layer.5.output.dense.bias', 'roberta.encoder.layer.6.attention.self.value.weight', 'roberta.encoder.layer.5.attention.self.value.bias', 'roberta.encoder.layer.1.attention.self.query.weight', 'roberta.encoder.layer.0.attention.self.value.bias', 'roberta.encoder.layer.11.attention.self.key.bias', 'roberta.encoder.layer.2.attention.self.key.weight', 'roberta.encoder.layer.10.attention.self.value.bias', 'roberta.encoder.layer.11.output.dense.weight', 'roberta.encoder.layer.4.attention.self.key.weight', 'roberta.encoder.layer.4.attention.self.value.bias', 'roberta.encoder.layer.7.output.dense.bias', 'roberta.encoder.layer.6.attention.self.query.weight', 'roberta.encoder.layer.8.attention.self.value.bias', 'roberta.encoder.layer.10.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.self.query.weight', 'roberta.encoder.layer.10.attention.output.dense.weight', 'roberta.encoder.layer.6.intermediate.dense.bias', 'roberta.encoder.layer.3.output.dense.bias', 'roberta.encoder.layer.1.output.LayerNorm.bias', 'lm_head.decoder.weight', 'roberta.encoder.layer.8.output.dense.weight', 'roberta.encoder.layer.9.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.key.bias', 'roberta.encoder.layer.8.intermediate.dense.bias', 'roberta.encoder.layer.3.intermediate.dense.weight', 'roberta.encoder.layer.8.attention.output.dense.bias', 'lm_head.decoder.bias', 'roberta.encoder.layer.1.attention.self.query.bias', 'roberta.encoder.layer.9.output.dense.weight', 'roberta.encoder.layer.0.output.dense.weight', 'roberta.encoder.layer.6.attention.self.key.weight', 'roberta.encoder.layer.10.output.dense.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.attention.self.key.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.bias', 'roberta.encoder.layer.9.output.dense.bias', 'roberta.embeddings.LayerNorm.bias', 'roberta.encoder.layer.9.attention.self.key.weight', 'roberta.encoder.layer.2.attention.self.query.bias', 'roberta.encoder.layer.9.attention.self.key.bias', 'roberta.encoder.layer.2.attention.output.LayerNorm.bias', 'roberta.encoder.layer.10.attention.self.key.bias', 'roberta.encoder.layer.10.attention.output.LayerNorm.bias', 'roberta.encoder.layer.2.attention.output.dense.weight', 'roberta.encoder.layer.7.attention.output.dense.bias', 'roberta.encoder.layer.5.output.LayerNorm.bias', 'roberta.encoder.layer.6.output.LayerNorm.bias', 'roberta.encoder.layer.10.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.self.key.weight', 'roberta.encoder.layer.10.output.dense.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.dense.weight', 'roberta.encoder.layer.4.attention.output.dense.weight', 'roberta.encoder.layer.5.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.output.LayerNorm.bias', 'roberta.embeddings.word_embeddings.weight', 'roberta.encoder.layer.4.output.dense.bias', 'roberta.encoder.layer.10.attention.self.value.weight', 'roberta.encoder.layer.0.attention.output.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.bias', 'roberta.encoder.layer.4.intermediate.dense.weight', 'roberta.encoder.layer.3.attention.self.query.weight', 'roberta.encoder.layer.4.output.LayerNorm.bias', 'roberta.encoder.layer.1.attention.self.value.bias', 'roberta.encoder.layer.2.attention.self.value.bias', 'roberta.encoder.layer.4.output.dense.weight', 'roberta.encoder.layer.0.output.LayerNorm.bias', 'roberta.encoder.layer.3.output.LayerNorm.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.value.weight', 'roberta.encoder.layer.3.attention.output.dense.bias', 'roberta.encoder.layer.5.attention.output.dense.weight', 'roberta.encoder.layer.3.attention.output.LayerNorm.bias', 'roberta.encoder.layer.4.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.output.dense.weight', 'roberta.encoder.layer.7.intermediate.dense.bias', 'roberta.encoder.layer.8.attention.output.LayerNorm.bias', 'lm_head.layer_norm.bias', 'roberta.encoder.layer.10.output.LayerNorm.weight', 'lm_head.dense.bias', 'roberta.encoder.layer.7.attention.self.key.bias', 'roberta.embeddings.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.query.bias', 'roberta.encoder.layer.8.attention.self.value.weight', 'roberta.encoder.layer.9.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.query.weight', 'roberta.encoder.layer.0.attention.output.dense.bias', 'roberta.encoder.layer.2.output.LayerNorm.bias', 'roberta.encoder.layer.8.output.LayerNorm.weight', 'roberta.encoder.layer.0.intermediate.dense.weight', 'roberta.encoder.layer.2.output.dense.bias', 'roberta.encoder.layer.6.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.value.weight', 'roberta.encoder.layer.6.attention.output.dense.bias', 'roberta.encoder.layer.1.attention.output.dense.weight', 'roberta.encoder.layer.7.output.LayerNorm.bias', 'roberta.embeddings.position_embeddings.weight', 'lm_head.layer_norm.weight', 'roberta.encoder.layer.11.output.LayerNorm.bias', 'roberta.encoder.layer.3.attention.self.value.bias', 'roberta.encoder.layer.0.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.self.value.weight', 'roberta.encoder.layer.1.attention.output.LayerNorm.bias', 'roberta.encoder.layer.7.intermediate.dense.weight', 'roberta.encoder.layer.9.attention.self.query.bias', 'roberta.encoder.layer.9.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.output.dense.weight', 'roberta.encoder.layer.8.attention.self.query.weight', 'roberta.encoder.layer.1.output.dense.bias', 'roberta.encoder.layer.3.output.LayerNorm.bias', 'roberta.encoder.layer.11.attention.self.query.weight', 'roberta.encoder.layer.7.attention.output.LayerNorm.bias', 'roberta.encoder.layer.1.output.dense.weight', 'roberta.encoder.layer.2.intermediate.dense.weight', 'roberta.encoder.layer.4.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.key.weight', 'roberta.encoder.layer.6.attention.self.value.bias', 'roberta.encoder.layer.1.attention.output.dense.bias', 'roberta.encoder.layer.3.attention.self.query.bias', 'roberta.encoder.layer.9.attention.output.dense.weight', 'roberta.encoder.layer.9.output.LayerNorm.bias', 'roberta.encoder.layer.10.output.LayerNorm.bias', 'roberta.encoder.layer.7.attention.self.query.weight', 'roberta.encoder.layer.7.attention.self.value.bias', 'roberta.encoder.layer.11.attention.self.query.bias', 'lm_head.dense.weight', 'roberta.encoder.layer.3.output.dense.weight', 'roberta.encoder.layer.7.output.dense.weight', 'roberta.encoder.layer.2.attention.self.value.weight', 'roberta.encoder.layer.3.intermediate.dense.bias', 'roberta.encoder.layer.5.attention.self.key.bias', 'roberta.encoder.layer.10.attention.output.dense.bias', 'roberta.encoder.layer.7.attention.self.value.weight', 'roberta.encoder.layer.8.output.dense.bias', 'roberta.encoder.layer.1.intermediate.dense.weight', 'roberta.encoder.layer.6.intermediate.dense.weight', 'roberta.encoder.layer.5.attention.self.value.weight', 'roberta.encoder.layer.11.attention.output.dense.bias', 'roberta.embeddings.token_type_embeddings.weight', 'roberta.encoder.layer.7.attention.self.query.bias', 'roberta.encoder.layer.4.output.LayerNorm.weight', 'roberta.encoder.layer.11.intermediate.dense.weight', 'roberta.encoder.layer.1.attention.self.key.bias', 'roberta.encoder.layer.9.output.LayerNorm.weight', 'roberta.encoder.layer.2.output.dense.weight', 'roberta.encoder.layer.6.attention.self.key.bias', 'roberta.encoder.layer.0.attention.output.LayerNorm.bias', 'roberta.encoder.layer.5.output.LayerNorm.weight', 'roberta.encoder.layer.1.attention.self.key.weight', 'roberta.encoder.layer.6.output.LayerNorm.weight', 'roberta.encoder.layer.4.attention.self.query.bias', 'roberta.encoder.layer.4.intermediate.dense.bias', 'roberta.encoder.layer.4.attention.output.dense.bias', 'roberta.embeddings.position_ids', 'roberta.encoder.layer.3.attention.self.key.bias', 'roberta.encoder.layer.9.attention.self.value.weight', 'roberta.encoder.layer.10.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.output.LayerNorm.weight', 'roberta.encoder.layer.5.intermediate.dense.weight', 'roberta.encoder.layer.0.attention.self.key.weight', 'roberta.encoder.layer.2.attention.output.dense.bias', 'roberta.encoder.layer.11.attention.output.LayerNorm.bias', 'roberta.encoder.layer.11.intermediate.dense.bias', 'roberta.encoder.layer.3.attention.self.value.weight', 'roberta.encoder.layer.2.attention.self.query.weight', 'roberta.encoder.layer.2.attention.self.key.bias', 'roberta.encoder.layer.6.attention.output.LayerNorm.weight', 'roberta.encoder.layer.11.attention.self.key.weight', 'roberta.encoder.layer.5.attention.self.query.bias', 'lm_head.bias', 'roberta.encoder.layer.5.attention.output.dense.bias', 'roberta.encoder.layer.5.intermediate.dense.bias', 'roberta.encoder.layer.6.output.dense.weight', 'roberta.encoder.layer.9.attention.self.query.weight', 'roberta.encoder.layer.10.attention.self.query.weight', 'roberta.encoder.layer.9.attention.self.value.bias', 'roberta.encoder.layer.11.output.LayerNorm.weight', 'roberta.encoder.layer.7.attention.self.key.weight', 'roberta.encoder.layer.0.attention.self.query.bias', 'roberta.encoder.layer.10.attention.self.key.weight', 'roberta.encoder.layer.2.output.LayerNorm.weight', 'roberta.encoder.layer.0.attention.self.key.bias', 'roberta.encoder.layer.8.attention.output.dense.weight', 'roberta.encoder.layer.0.attention.output.LayerNorm.weight', 'roberta.encoder.layer.8.attention.self.key.bias', 'roberta.encoder.layer.8.intermediate.dense.weight', 'roberta.encoder.layer.0.output.dense.bias', 'roberta.encoder.layer.6.output.dense.bias']\n","- This IS expected if you are initializing BertLstmCRF from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertLstmCRF from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertLstmCRF were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['encoder.layer.6.attention.self.value.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.11.output.dense.bias', 'encoder.layer.4.attention.self.value.weight', 'bilstm.bias_ih_l0_reverse', 'encoder.layer.6.attention.self.value.bias', 'encoder.layer.9.attention.self.query.bias', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.2.attention.self.key.bias', 'encoder.layer.6.output.LayerNorm.weight', 'bilstm.weight_ih_l0', 'encoder.layer.9.attention.self.value.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.6.attention.self.key.bias', 'encoder.layer.9.output.dense.bias', 'bilstm.bias_hh_l0', 'encoder.layer.6.attention.self.key.weight', 'encoder.layer.5.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.6.attention.self.query.bias', 'classifier.weight', 'encoder.layer.7.intermediate.dense.weight', 'bilstm.bias_hh_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'encoder.layer.8.attention.self.value.weight', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.0.attention.output.LayerNorm.bias', 'encoder.layer.5.attention.self.key.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.1.attention.self.value.weight', 'encoder.layer.10.output.dense.weight', 'crf.start_transitions', 'encoder.layer.8.attention.self.query.weight', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.4.attention.self.query.bias', 'encoder.layer.11.attention.output.dense.weight', 'encoder.layer.10.attention.output.dense.weight', 'bilstm.weight_ih_l0_reverse', 'encoder.layer.3.attention.self.query.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.bias', 'encoder.layer.7.attention.output.LayerNorm.weight', 'embeddings.position_embeddings.weight', 'encoder.layer.11.output.dense.weight', 'encoder.layer.4.attention.self.key.bias', 'embeddings.LayerNorm.weight', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.7.attention.self.query.bias', 'encoder.layer.1.attention.self.value.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.8.attention.self.key.bias', 'encoder.layer.8.attention.output.LayerNorm.bias', 'encoder.layer.2.attention.output.LayerNorm.weight', 'encoder.layer.0.attention.output.dense.bias', 'encoder.layer.5.attention.self.value.weight', 'encoder.layer.8.output.dense.weight', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.0.attention.self.key.weight', 'encoder.layer.9.attention.self.query.weight', 'crf.transitions', 'encoder.layer.5.attention.self.query.weight', 'bilstm.bias_ih_l0', 'encoder.layer.11.attention.output.LayerNorm.bias', 'encoder.layer.9.attention.output.LayerNorm.weight', 'encoder.layer.7.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.weight', 'encoder.layer.8.attention.self.query.bias', 'encoder.layer.2.attention.output.dense.weight', 'encoder.layer.8.attention.self.key.weight', 'encoder.layer.1.attention.output.LayerNorm.weight', 'encoder.layer.4.attention.self.key.weight', 'encoder.layer.10.attention.self.key.bias', 'encoder.layer.8.attention.self.value.bias', 'encoder.layer.5.attention.self.value.bias', 'encoder.layer.2.attention.self.query.weight', 'encoder.layer.5.attention.output.dense.weight', 'encoder.layer.5.attention.self.query.bias', 'encoder.layer.9.attention.self.key.bias', 'classifier.bias', 'encoder.layer.2.attention.self.value.bias', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.3.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.value.bias', 'encoder.layer.0.attention.self.query.weight', 'encoder.layer.11.attention.self.query.bias', 'encoder.layer.8.attention.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.4.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'crf.end_transitions', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.9.attention.self.value.bias', 'encoder.layer.11.attention.self.query.weight', 'encoder.layer.9.attention.self.key.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.attention.self.query.bias', 'embeddings.word_embeddings.weight', 'encoder.layer.1.attention.output.dense.bias', 'encoder.layer.0.attention.self.value.bias', 'encoder.layer.2.attention.self.query.bias', 'encoder.layer.8.attention.output.LayerNorm.weight', 'encoder.layer.9.attention.output.dense.bias', 'encoder.layer.1.attention.self.key.bias', 'encoder.layer.5.attention.output.LayerNorm.weight', 'encoder.layer.11.attention.self.value.bias', 'encoder.layer.2.attention.self.key.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.3.attention.self.key.weight', 'encoder.layer.3.attention.self.key.bias', 'encoder.layer.9.attention.output.LayerNorm.bias', 'encoder.layer.11.attention.self.value.weight', 'encoder.layer.11.attention.output.dense.bias', 'encoder.layer.2.attention.self.value.weight', 'encoder.layer.10.attention.output.LayerNorm.weight', 'encoder.layer.2.attention.output.dense.bias', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.7.attention.self.key.weight', 'encoder.layer.3.attention.output.dense.bias', 'encoder.layer.7.attention.self.value.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.10.attention.self.value.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.9.attention.output.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.7.attention.output.dense.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.6.attention.output.dense.weight', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.attention.self.query.weight', 'encoder.layer.1.attention.output.LayerNorm.bias', 'embeddings.LayerNorm.bias', 'encoder.layer.0.attention.self.value.weight', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.6.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.key.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.1.attention.self.query.bias', 'encoder.layer.1.attention.self.query.weight', 'encoder.layer.4.attention.output.dense.bias', 'encoder.layer.0.attention.output.LayerNorm.weight', 'encoder.layer.3.output.dense.weight', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.3.attention.self.query.bias', 'encoder.layer.1.attention.output.dense.weight', 'encoder.layer.10.attention.output.LayerNorm.bias', 'encoder.layer.7.attention.self.query.weight', 'embeddings.token_type_embeddings.weight', 'encoder.layer.4.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.11.attention.output.LayerNorm.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.10.attention.self.value.weight', 'encoder.layer.1.attention.self.key.weight', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.5.attention.output.dense.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.bias', 'encoder.layer.2.output.dense.bias', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.attention.self.query.bias', 'encoder.layer.6.attention.output.dense.bias', 'encoder.layer.10.attention.self.key.weight', 'encoder.layer.1.output.dense.weight', 'encoder.layer.4.attention.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.9.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.6.attention.self.query.weight', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.2.output.dense.weight', 'encoder.layer.8.attention.output.dense.bias', 'encoder.layer.10.output.dense.bias', 'encoder.layer.3.attention.output.dense.weight', 'encoder.layer.0.attention.output.dense.weight', 'encoder.layer.4.attention.self.query.weight', 'encoder.layer.0.attention.self.key.bias', 'encoder.layer.4.output.dense.bias', 'encoder.layer.3.attention.output.LayerNorm.weight', 'encoder.layer.3.attention.self.value.weight', 'encoder.layer.2.attention.output.LayerNorm.bias', 'encoder.layer.4.attention.self.value.bias', 'encoder.layer.6.output.dense.bias', 'bilstm.weight_hh_l0', 'encoder.layer.11.attention.self.key.weight', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.attention.output.LayerNorm.bias', 'encoder.layer.1.output.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.6.attention.output.LayerNorm.weight', 'encoder.layer.10.attention.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.11.attention.self.key.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertLstmCRF(\n","  (bert): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n","      (position_embeddings): Embedding(258, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (bilstm): LSTM(768, 384, batch_first=True, dropout=0.1, bidirectional=True)\n","  (classifier): Linear(in_features=768, out_features=5, bias=True)\n","  (crf): CRF(num_tags=5)\n",")"]},"metadata":{},"execution_count":37}]},{"cell_type":"code","source":["# Defining the training function on the 80% of the dataset for tuning the bert model\n","def train_modified(epoch, verbose = False):\n","    tr_loss, tr_accuracy = 0, 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","    tr_preds, tr_labels = [], []\n","    # put model in training mode\n","    model.train()\n","    \n","    for idx, batch in enumerate(train_loader):\n","        ids, labels = batch\n","        ids = ids.to(device)\n","        labels = labels.to(device)\n","        mask = ids!=1\n","        \n","        # check(ids, mask, labels)\n","        # break\n","\n","        outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n","        loss = outputs[0]\n","        \n","        print(outputs[1])\n","        \n","        tr_logits = outputs[1]\n","        tr_loss += loss.item()\n","\n","        nb_tr_steps += 1\n","        nb_tr_examples += labels.size(0)\n","        \n","        if idx % 100==0 and verbose:\n","            loss_step = tr_loss/nb_tr_steps\n","            print(f\"Training loss per 100 training steps: {loss_step}\")\n","           \n","        # compute training accuracy\n","        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","        # print(active_logits)\n","        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","        # print(flattened_predictions)\n","        \n","        # only compute accuracy at active labels\n","        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n","        \n","        labels = torch.masked_select(flattened_targets, active_accuracy)\n","        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","        \n","        tr_labels.extend(labels)\n","        tr_preds.extend(predictions)\n","\n","        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n","        tr_accuracy += tmp_tr_accuracy\n","    \n","        # gradient clipping\n","        torch.nn.utils.clip_grad_norm_(\n","            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n","        )\n","        \n","        # backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    epoch_loss = tr_loss / nb_tr_steps\n","    tr_accuracy = tr_accuracy / nb_tr_steps\n","\n","    labels = [ids_to_labels[id.item()] for id in tr_labels]\n","    predictions = [ids_to_labels[id.item()] for id in tr_preds]\n","    f1 = seqeval.metrics.f1_score([labels], [predictions])\n","\n","    print(f\"Training loss epoch: {epoch_loss}\", f\"Training F1 epoch: {f1}\")"],"metadata":{"id":"ASZEaQgFTtz3"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating optimizer and lr schedulers\n","param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","num_train_optimization_steps = int(args.epochs*len(train_dataset)/args.batch_size/args.accumulation_steps)\n","optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=num_train_optimization_steps)  # PyTorch scheduler\n","scheduler0 = get_constant_schedule(optimizer)  # PyTorch scheduler\n","\n","# tsfm = model.roberta\n","tsfm = model.bert\n","for child in tsfm.children():\n","    for param in child.parameters():\n","        if not param.requires_grad:\n","            print(\"whoopsies\")\n","        param.requires_grad = False\n","frozen = True\n","\n","EPOCHS = 50\n","import time\n","\n","for epoch in range(EPOCHS):\n","    if epoch > 0 and frozen:\n","        for child in tsfm.children():\n","            for param in child.parameters():\n","                param.requires_grad = True\n","        frozen = False\n","        del scheduler0\n","        torch.cuda.empty_cache()\n","    st = time.time()\n","    print(f\"Training epoch: {epoch + 1}\")\n","    train_modified(epoch)\n","    labels, predictions = valid(model, valid_loader)\n","    \n","    print('Time: ',time.time() - st)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"pna9V5qRTfr3","executionInfo":{"status":"error","timestamp":1640029542143,"user_tz":-420,"elapsed":288,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"01898719-fa0d-439e-f8ca-df1085f095cb"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","whoopsies\n","Training epoch: 1\n","tensor([[2., 0., 2., 2., 0., 1., 2., 0., 1., 2., 2., 2., 2., 2., 2., 2., 0., 1.,\n","         2., 3., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 2., 2.,\n","         2., 2., 2., 2., 2., 1., 2., 2., 2., 2., 1., 2., 2., 0., 1., 2., 2., 2.,\n","         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n","         2., 2., 2., 3., 2., 4., 4., 4., 4., 4., 4., 4., 4., 4., 2., 2., 2., 2.,\n","         0., 1., 2., 3., 3., 3., 2., 3., 2., 0., 1., 2., 2., 0., 1., 2., 2., 2.,\n","         2., 2., 2., 0., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 3., 1., 2., 3.,\n","         2., 0.],\n","        [2., 2., 2., 2., 0., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n","         2., 4., 4., 4., 2., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n","         2., 2., 2., 2., 3., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n","         2., 2., 2., 3., 2., 2., 2., 2., 2., 2., 2., 1., 2., 2., 2., 2., 2., 1.,\n","         2., 2., 2., 2., 2., 1., 2., 2., 2., 2., 2., 2., 3., 2., 3., 2., 2., 3.,\n","         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n","         2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 1., 2., 2., 2., 2., 2., 2.,\n","         3., 2.],\n","        [4., 4., 2., 2., 2., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4.,\n","         4., 4., 4., 4., 2., 0., 1., 2., 2., 2., 2., 0., 1., 2., 1., 2., 2., 0.,\n","         2., 3., 2., 3., 1., 2., 2., 3., 2., 2., 2., 4., 4., 4., 4., 4., 2., 2.,\n","         2., 2., 2., 4., 4., 2., 3., 2., 2., 2., 0., 1., 2., 1., 2., 1., 2., 2.,\n","         1., 2., 2., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 4., 2., 2., 2.,\n","         2., 2., 2., 2., 2., 2., 2., 4., 4., 4., 4., 4., 2., 0., 1., 2., 3., 2.,\n","         2., 2., 3., 2., 1., 2., 2., 2., 1., 2., 2., 3., 2., 1., 2., 2., 2., 2.,\n","         0., 1.],\n","        [2., 2., 2., 0., 1., 2., 2., 4., 4., 2., 2., 2., 2., 2., 2., 2., 2., 2.,\n","         2., 2., 2., 2., 0., 2., 2., 3., 4., 4., 4., 4., 4., 2., 3., 2., 2., 2.,\n","         0., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 3., 0.,\n","         1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 1., 2., 0., 1., 2., 2.,\n","         0., 1., 2., 2., 2., 2., 1., 2., 1., 2., 2., 2., 2., 2., 2., 1., 2., 2.,\n","         2., 0., 1., 2., 2., 2., 2., 2., 2., 2., 2., 2., 2., 1., 2., 2., 2., 2.,\n","         1., 2., 1., 2., 1., 2., 2., 3., 2., 3., 1., 2., 2., 3., 2., 2., 3., 2.,\n","         2., 0.]])\n"]},{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-43-8c5ff32d7ba9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training epoch: {epoch + 1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mtrain_modified\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-42-bd39af0c55bd>\u001b[0m in \u001b[0;36mtrain_modified\u001b[0;34m(epoch, verbose)\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0;31m# compute training accuracy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m         \u001b[0mflattened_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# shape (batch_size * seq_len,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m         \u001b[0mactive_logits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtr_logits\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# shape (batch_size * seq_len, num_labels)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m         \u001b[0;31m# print(active_logits)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mflattened_predictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactive_logits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# shape (batch_size * seq_len,)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: shape '[-1, 5]' is invalid for input of size 512"]}]},{"cell_type":"code","source":["512/128"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"G-H-ucGkXYkG","executionInfo":{"status":"ok","timestamp":1640029331504,"user_tz":-420,"elapsed":277,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"181e8237-09da-4cc3-acc3-f068946a7c52"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["4.0"]},"metadata":{},"execution_count":40}]},{"cell_type":"markdown","metadata":{"id":"4Gz-wHAw3xMk"},"source":["#### **Inference**\n","\n","The fun part is when we can quickly test the model on new, unseen sentences. \n","Here, we use the prediction of the **first word piece of every word** (which is how the model was trained). \n","\n","*In other words, the code below does not take into account when predictions of different word pieces that belong to the same word do not match.*"]},{"cell_type":"code","source":["ids_to_labels[-100] = 'X'"],"metadata":{"id":"gGe0F93pOuZa"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zPDla1mmZiax","tags":[],"colab":{"base_uri":"https://localhost:8080/"},"outputId":"6b7b74f3-30ad-49a4-d784-dd28dbd82da8","executionInfo":{"status":"ok","timestamp":1639910654279,"user_tz":-420,"elapsed":473,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}}},"source":["sentence = [\"Adam is a company based in New York, but is also has employees working in Paris\"]\n","ids, labels, masks = convert_lines(lines, tags, vocab, bpe)\n","\n","# move to gpu\n","ids = torch.tensor(ids).to(device)\n","mask = torch.tensor(masks).to(device)\n","# forward pass\n","outputs = model(ids, attention_mask=mask)\n","logits = outputs[0]\n","\n","active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n","\n","sentences, predictions = decode(ids.squeeze(0).cpu(), flattened_predictions.cpu())\n","_, labels = decode(ids.squeeze(0).cpu(), torch.tensor(labels).squeeze())\n","for token, pred, label in zip(sentences.split(), predictions, labels.squeeze()):\n","  print('{0:10}  {1:10} {2:10} {3:10}'.format(token, pred, ids_to_labels[int(pred)], ids_to_labels[int(label)]))\n","\n","\n","# tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n","# token_predictions = [ids_to_labels[i] for i in flattened_predictions.cpu().numpy()]\n","# wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n","\n","# prediction = []\n","# for token_pred, mapping in zip(wp_preds, inputs[\"offset_mapping\"].squeeze().tolist()):\n","#   #only predictions on first word pieces are important\n","#   if mapping[0] == 0 and mapping[1] != 0:\n","#     prediction.append(token_pred[1])\n","#   else:\n","#     continue\n","\n","# print(sentence.split())\n","# print(prediction)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1/1 [00:00<00:00, 188.83it/s]\n"]},{"output_type":"stream","name":"stdout","text":["1\n","1\n","mua                  0 O          O         \n","được                 0 O          O         \n","giá                  0 O          B-PRI     \n","tốt                  0 O          O         \n","lại                  0 O          O         \n","được                 0 O          O         \n","freeship             0 O          B-PRI     \n","mừng                 0 O          O         \n","rơi                  0 O          O         \n","nước_mắt             0 O          O         \n","đối_với              0 O          O         \n","tỉnh_lẻ              0 O          O         \n","thì                  0 O          O         \n","tiền                 0 O          B-PRI     \n","ship                 0 O          I-PRI     \n","là                   0 O          O         \n","1                    0 O          O         \n","trở_ngại             0 O          O         \n","sản_phẩm             0 O          O         \n","quá                  0 O          O         \n","ổn                   0 O          O         \n","cảm_ơn               0 O          O         \n","shop                 0 O          O         \n","cảm_ơn               0 O          O         \n","tiki                 0 O          O         \n","this                 0 O          O         \n","is                   0 O          O         \n","english              0 O          O         \n","sentences            0 O          O         \n"]}]},{"cell_type":"markdown","metadata":{"id":"sqDklprSqB5d"},"source":["#### **Saving the model for future use**"]},{"cell_type":"markdown","metadata":{"id":"VuUdX_fImswO"},"source":["Finally, let's save the vocabulary (.txt) file, model weights (.bin) and the model's configuration (.json) to a directory, so that both the tokenizer and model can be re-loaded using the `from_pretrained()` class method.\n"]},{"cell_type":"code","metadata":{"id":"sDZtSsKKntuI","tags":[]},"source":["import os\n","\n","directory = \"./model\"\n","\n","if not os.path.exists(directory):\n","    os.makedirs(directory)\n","\n","# save vocabulary of the tokenizer\n","tokenizer.save_vocabulary(directory)\n","# save the model weights and its configuration file\n","model.save_pretrained(directory)\n","print('All files saved')\n","print('This tutorial is completed')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Test new data"],"metadata":{"id":"P0PK_JS5hMEw"}},{"cell_type":"code","source":["from google.colab import drive\n","drive._mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9Th0ypfAhOGy","executionInfo":{"status":"ok","timestamp":1640022959804,"user_tz":-420,"elapsed":16426,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"8e9d7f26-5127-460a-974e-657b60afc027"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/NLP/project_nlp"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oE1kY9atjURP","executionInfo":{"status":"ok","timestamp":1640022959805,"user_tz":-420,"elapsed":6,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"816d7bf8-8702-4f48-969a-b7c26d33d093"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/NLP/project_nlp\n"]}]},{"cell_type":"code","source":["import pandas as pd"],"metadata":{"id":"oRUXCzwXjbc4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["https://github.com/minhpqn/vietner/tree/master/vlsp2016_exp"],"metadata":{"id":"lAalSDkYlPjk"}},{"cell_type":"code","source":["test_path = './data/VSLP/test.txt'\n","train_path = './data/VSLP/train.txt'\n","\n","train_data = pd.read_csv(train_path,  sep='\\t', header=None, usecols=[0,3], names=['Word', 'Tag'], skip_blank_lines=False)\n","test_data = pd.read_csv(test_path,  sep='\\t', header=None, usecols=[0,3], names=['Word', 'Tag'], skip_blank_lines=False)"],"metadata":{"id":"Y0nbbBZCh04W"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_data['Sentence #'] = [None] * len(train_data)\n","sent_count = 1\n","for idx in range(len(train_data)):\n","    train_data['Sentence #'][idx] = f'Sentence: {sent_count}'\n","    if train_data[['Word']].iloc[idx].isnull().any():\n","        # print(idx)\n","        sent_count += 1\n","train_data.dropna(inplace=True)\n","train_data.isnull().sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"npag4HrRpCqk","executionInfo":{"status":"ok","timestamp":1640023399696,"user_tz":-420,"elapsed":2595,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"e3c8f04a-10e9-4402-b2f9-1dc25c0a6376"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Word          0\n","Tag           0\n","Sentence #    0\n","dtype: int64"]},"metadata":{},"execution_count":34}]},{"cell_type":"code","source":["test_data['Sentence #'] = [None] * len(test_data)\n","sent_count = 1\n","for idx in range(len(test_data)):\n","    test_data['Sentence #'][idx] = f'Sentence: {sent_count}'\n","    if test_data[['Word']].iloc[idx].isnull().any():\n","        # print(idx)\n","        sent_count += 1\n","test_data.dropna(inplace=True)\n","test_data.isnull().sum()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"n-VaPK87_zo8","executionInfo":{"status":"ok","timestamp":1640023400434,"user_tz":-420,"elapsed":741,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"ff87a4b4-7b0e-48dd-a0cf-bd18665d14bc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["Word          0\n","Tag           0\n","Sentence #    0\n","dtype: int64"]},"metadata":{},"execution_count":35}]},{"cell_type":"code","source":["print(\"Number of tags: {}\".format(len(train_data.Tag.unique())))\n","frequencies = train_data.Tag.value_counts()\n","frequencies"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"dhZLDijfATPr","executionInfo":{"status":"ok","timestamp":1640023456631,"user_tz":-420,"elapsed":4,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"467dc5ff-d236-42d0-ddaa-f454b60cb5fd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Number of tags: 9\n"]},{"output_type":"execute_result","data":{"text/plain":["O         1793\n","B-LOC       55\n","B-PER       23\n","I-LOC       22\n","I-PER       22\n","B-ORG        8\n","I-ORG        7\n","I-MISC       1\n","B-MISC       1\n","Name: Tag, dtype: int64"]},"metadata":{},"execution_count":38}]},{"cell_type":"code","source":["tags = {}\n","for tag, count in zip(frequencies.index, frequencies):\n","    if tag != \"O\":\n","        if tag[2:5] not in tags.keys():\n","            tags[tag[2:5]] = count\n","        else:\n","            tags[tag[2:5]] += count\n","    continue\n","\n","print(sorted(tags.items(), key=lambda x: x[1], reverse=True))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ULyIPqKCBA7M","executionInfo":{"status":"ok","timestamp":1640023463799,"user_tz":-420,"elapsed":350,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"5de59a2b-ccb9-425b-8e9a-346edc5e541c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["[('LOC', 77), ('PER', 45), ('ORG', 15), ('MIS', 2)]\n"]}]},{"cell_type":"code","source":["labels_to_ids = {k: v for v, k in enumerate(train_data.Tag.unique())}\n","ids_to_labels = {v: k for v, k in enumerate(train_data.Tag.unique())}\n","labels_to_ids"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"B0a9_udhBCnM","executionInfo":{"status":"ok","timestamp":1640023483794,"user_tz":-420,"elapsed":426,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"b3817716-c88c-4f9b-831d-85063711c839"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'B-LOC': 1,\n"," 'B-MISC': 7,\n"," 'B-ORG': 2,\n"," 'B-PER': 4,\n"," 'I-LOC': 3,\n"," 'I-MISC': 8,\n"," 'I-ORG': 6,\n"," 'I-PER': 5,\n"," 'O': 0}"]},"metadata":{},"execution_count":41}]},{"cell_type":"code","source":["# let's create a new column called \"sentence\" which groups the words by sentence \n","train_data['sentence'] = train_data[['Sentence #','Word','Tag']].groupby(['Sentence #'])['Word'].transform(lambda x: ' '.join(x))\n","# let's also create a new column called \"word_labels\" which groups the tags by sentence \n","train_data['word_labels'] = train_data[['Sentence #','Word','Tag']].groupby(['Sentence #'])['Tag'].transform(lambda x: ','.join(x))\n","train_data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"dLHYTralBLkJ","executionInfo":{"status":"ok","timestamp":1640023546173,"user_tz":-420,"elapsed":266,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"b4fcf9a0-ce2a-44aa-fad8-f825e68efbe1"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-691c233c-35c4-431f-8039-470d711d9239\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Word</th>\n","      <th>Tag</th>\n","      <th>Sentence #</th>\n","      <th>sentence</th>\n","      <th>word_labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Đó</td>\n","      <td>O</td>\n","      <td>Sentence: 1</td>\n","      <td>Đó là con đường biển ngắn nhất để đi từ Ấn_Độ_...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,B-LOC,O,B-LOC,O,O,O,O,O,O,...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>là</td>\n","      <td>O</td>\n","      <td>Sentence: 1</td>\n","      <td>Đó là con đường biển ngắn nhất để đi từ Ấn_Độ_...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,B-LOC,O,B-LOC,O,O,O,O,O,O,...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>con</td>\n","      <td>O</td>\n","      <td>Sentence: 1</td>\n","      <td>Đó là con đường biển ngắn nhất để đi từ Ấn_Độ_...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,B-LOC,O,B-LOC,O,O,O,O,O,O,...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>đường</td>\n","      <td>O</td>\n","      <td>Sentence: 1</td>\n","      <td>Đó là con đường biển ngắn nhất để đi từ Ấn_Độ_...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,B-LOC,O,B-LOC,O,O,O,O,O,O,...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>biển</td>\n","      <td>O</td>\n","      <td>Sentence: 1</td>\n","      <td>Đó là con đường biển ngắn nhất để đi từ Ấn_Độ_...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,B-LOC,O,B-LOC,O,O,O,O,O,O,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-691c233c-35c4-431f-8039-470d711d9239')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-691c233c-35c4-431f-8039-470d711d9239 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-691c233c-35c4-431f-8039-470d711d9239');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["    Word  ...                                        word_labels\n","0     Đó  ...  O,O,O,O,O,O,O,O,O,O,B-LOC,O,B-LOC,O,O,O,O,O,O,...\n","1     là  ...  O,O,O,O,O,O,O,O,O,O,B-LOC,O,B-LOC,O,O,O,O,O,O,...\n","2    con  ...  O,O,O,O,O,O,O,O,O,O,B-LOC,O,B-LOC,O,O,O,O,O,O,...\n","3  đường  ...  O,O,O,O,O,O,O,O,O,O,B-LOC,O,B-LOC,O,O,O,O,O,O,...\n","4   biển  ...  O,O,O,O,O,O,O,O,O,O,B-LOC,O,B-LOC,O,O,O,O,O,O,...\n","\n","[5 rows x 5 columns]"]},"metadata":{},"execution_count":42}]},{"cell_type":"code","source":["# let's create a new column called \"sentence\" which groups the words by sentence \n","test_data['sentence'] = test_data[['Sentence #','Word','Tag']].groupby(['Sentence #'])['Word'].transform(lambda x: ' '.join(x))\n","# let's also create a new column called \"word_labels\" which groups the tags by sentence \n","test_data['word_labels'] = test_data[['Sentence #','Word','Tag']].groupby(['Sentence #'])['Tag'].transform(lambda x: ','.join(x))\n","test_data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"iV_SgpwWBPnu","executionInfo":{"status":"ok","timestamp":1640023546173,"user_tz":-420,"elapsed":4,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"298aebe8-4826-4e32-cc40-fb0f3fc8e559"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-ff044c9b-1331-43c1-9f2b-76eaa1288d74\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Word</th>\n","      <th>Tag</th>\n","      <th>Sentence #</th>\n","      <th>sentence</th>\n","      <th>word_labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Chị</td>\n","      <td>O</td>\n","      <td>Sentence: 1</td>\n","      <td>Chị Minh ôm đứa con_gái mới hơn hai tháng rưỡi...</td>\n","      <td>O,B-PER,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Minh</td>\n","      <td>B-PER</td>\n","      <td>Sentence: 1</td>\n","      <td>Chị Minh ôm đứa con_gái mới hơn hai tháng rưỡi...</td>\n","      <td>O,B-PER,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>ôm</td>\n","      <td>O</td>\n","      <td>Sentence: 1</td>\n","      <td>Chị Minh ôm đứa con_gái mới hơn hai tháng rưỡi...</td>\n","      <td>O,B-PER,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>đứa</td>\n","      <td>O</td>\n","      <td>Sentence: 1</td>\n","      <td>Chị Minh ôm đứa con_gái mới hơn hai tháng rưỡi...</td>\n","      <td>O,B-PER,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>con_gái</td>\n","      <td>O</td>\n","      <td>Sentence: 1</td>\n","      <td>Chị Minh ôm đứa con_gái mới hơn hai tháng rưỡi...</td>\n","      <td>O,B-PER,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ff044c9b-1331-43c1-9f2b-76eaa1288d74')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-ff044c9b-1331-43c1-9f2b-76eaa1288d74 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-ff044c9b-1331-43c1-9f2b-76eaa1288d74');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["      Word  ...                                        word_labels\n","0      Chị  ...  O,B-PER,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n","1     Minh  ...  O,B-PER,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n","2       ôm  ...  O,B-PER,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n","3      đứa  ...  O,B-PER,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n","4  con_gái  ...  O,B-PER,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n","\n","[5 rows x 5 columns]"]},"metadata":{},"execution_count":43}]},{"cell_type":"code","source":["from fairseq.data.encoders.fastbpe import fastBPE\n","from fairseq.data import Dictionary\n","import argparse\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--bpe-codes', \n","    default=\"./PhoBERT_base_transformers/bpe.codes\",\n","    required=False,\n","    type=str,\n","    help='path to fastBPE BPE'\n",")\n","args, unknown = parser.parse_known_args()\n","bpe = fastBPE(args)\n","\n","# Load the dictionary\n","vocab = Dictionary()\n","vocab.add_from_file(\"./PhoBERT_base_transformers/dict.txt\")"],"metadata":{"id":"Rzh_hgAdBoeR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from fairseq.data.encoders.fastbpe import fastBPE\n","from fairseq.data import Dictionary\n","import argparse\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--bpe-codes', \n","    default=\"./PhoBERT_base_transformers/bpe.codes\",\n","    required=False,\n","    type=str,\n","    help='path to fastBPE BPE'\n",")\n","args, unknown = parser.parse_known_args()\n","bpe = fastBPE(args)\n","\n","# Load the dictionary\n","vocab = Dictionary()\n","vocab.add_from_file(\"./PhoBERT_base_transformers/dict.txt\")"],"metadata":{"id":"OM6F9HkABZR5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["labels_to_ids['X'] = -100\n","labels_to_ids"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"oIM3B6j2Ba8G","executionInfo":{"status":"ok","timestamp":1640023666971,"user_tz":-420,"elapsed":4,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"9bc92def-2669-4b0b-b0d1-ef842ac80d05"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'B-LOC': 1,\n"," 'B-MISC': 7,\n"," 'B-ORG': 2,\n"," 'B-PER': 4,\n"," 'I-LOC': 3,\n"," 'I-MISC': 8,\n"," 'I-ORG': 6,\n"," 'I-PER': 5,\n"," 'O': 0,\n"," 'X': -100}"]},"metadata":{},"execution_count":50}]},{"cell_type":"code","source":["from tqdm import tqdm\n","def convert_lines(lines, tags, vocab, bpe, max_sequence_length=256):\n","    \"\"\"\n","    lines: list các văn bản input\n","    tags: list các chuỗi tag\n","    vocab: từ điển dùng để encoding subwords\n","    bpe: \n","    \"\"\"\n","    # Index của các token cls (đầu câu), eos (cuối câu), padding (padding token)\n","    outputs = np.zeros((len(lines), max_sequence_length), dtype=np.int32) # --> shape (number_lines, max_seq_len)\n","    outputs_labels = np.zeros((len(lines), max_sequence_length), dtype=np.int32)\n","    outputs_attention_mask = np.zeros((len(lines), max_sequence_length), dtype=np.int32)\n","    # Index của các token cls (đầu câu), eos (cuối câu), padding (padding token)\n","    cls_id = 0\n","    eos_id = 2\n","    pad_id = 1\n","    \n","    for idx, row in tqdm(enumerate(lines), total=len(lines)): \n","        # Mã hóa subwords theo byte pair encoding(bpe)\n","        subwords = bpe.encode(row)\n","        subwords = '<s> '+ subwords +' </s>'\n","        input_ids = vocab.encode_line(subwords, append_eos=False, add_if_not_exist=False).long().tolist()\n","        \n","        tag_list = ['O'] + tags[idx].split(',') + ['O']\n","        subword_idx = [subwords.split().index(word) for word in subwords.split() if '@@' in word]\n","        for i, orig_idx in enumerate(subword_idx):\n","            tag_list.insert(orig_idx+1, 'X')\n","        # print(tag_list)\n","        labels = [labels_to_ids[label] for label in tag_list] \n","\n","        # Truncate input nếu độ dài vượt quá max_seq_len\n","        if len(input_ids) > max_sequence_length: \n","            input_ids = input_ids[:max_sequence_length]\n","            input_ids[-1] = eos_id\n","            labels = labels[:max_sequence_length]\n","            labels[-1] = -100\n","        else:\n","        # Padding nếu độ dài câu chưa bằng max_seq_len\n","            input_ids = input_ids + [pad_id, ]*(max_sequence_length - len(input_ids))\n","            labels = labels + [-100, ]*(max_sequence_length - len(labels))\n","        \n","        labels[0] = -100\n","        outputs[idx,:] = np.array(input_ids)\n","        outputs_labels[idx,:] = np.array(labels)\n","        outputs_attention_mask[idx, np.array(input_ids)!=pad_id] = 1\n","\n","    return outputs, outputs_labels, outputs_attention_mask\n","\n","lines = ['Chị Minh ôm đứa con_gái mới hơn hai tháng rưỡi tuổi nấc lên từng tiếng thảm_thiết khi kể lại cho chúng_tôi nghe về cái chết của chồng .'] \n","tags = ['O,B-PER,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O']\n","\n","ids, labels, masks = convert_lines(lines, tags, vocab, bpe)\n","print('input_ids tensor encode: {}\\n, shape: {}\\n'.format(ids[:10], ids.size))\n","print('label_ids tensor encode: {}\\n, shape: {}\\n'.format(labels[:10], labels.size))\n","print('masks tensor encode: {}\\n, shape: {}\\n'.format(masks[:10], masks.size))\n","# print('x1 tensor decode: ', phoBERT_cls.decode(torch.tensor(x1))[:103])"],"metadata":{"id":"DzjiG7EMBdBJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["MAX_LEN = 256\n","TRAIN_BATCH_SIZE = 4\n","VALID_BATCH_SIZE = 2\n","EPOCHS = 1\n","LEARNING_RATE = 1e-05\n","MAX_GRAD_NORM = 10"],"metadata":{"id":"Eznom-ilByMP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class argu():\n","    def __init__(self):\n","        # self.train_path = './data/train.csv'\n","        self.dict_path = \"./PhoBERT_base_transformers/dict.txt\"\n","        self.config_path = \"./PhoBERT_base_transformers/config.json\"\n","        # self.rdrsegmenter_path = '/content/vncorenlp/VnCoreNLP-1.1.1.jar'\n","        self.pretrained_path = './PhoBERT_base_transformers/model.bin'\n","        self.max_sequence_length = 256\n","        self.batch_size = 8\n","        self.accumulation_steps = 1\n","        self.epochs = 10\n","        self.seed = 69\n","        self.fold = 0\n","        self.lr= 1e-5\n","        self.ckpt_path = './checkpoints'\n","        self.bpe_codes = \"./PhoBERT_base_transformers/bpe.codes\"\n","args = argu()\n","\n","config = RobertaConfig.from_pretrained(\n","    args.config_path,\n","    output_hidden_states=True,\n","    return_dict=True,\n","    num_labels=9,\n","    classifier_dropout = True\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bUbl5AKEB_o6","executionInfo":{"status":"ok","timestamp":1640025523226,"user_tz":-420,"elapsed":289,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"53ff1574-0ee7-4690-c3ac-f142b6da741f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["You are using a model of type bert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n"]}]},{"cell_type":"code","source":["X_train, Y_label_train, Y_mask_train = convert_lines(train_data.sentence.values, train_data.word_labels.values, vocab, bpe, max_sequence_length=MAX_LEN)\n","X_test, Y_label_test, Y_mask_test = convert_lines(test_data.sentence.values, test_data.word_labels.values, vocab, bpe, max_sequence_length=MAX_LEN)\n","\n","train_dataset = TensorDataset(torch.tensor(X_train,dtype=torch.long), \n","                              torch.tensor(Y_label_train,dtype=torch.long))\n","\n","valid_dataset = TensorDataset(torch.tensor(X_test,dtype=torch.long), \n","                              torch.tensor(Y_label_test,dtype=torch.long))\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n","valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=VALID_BATCH_SIZE, shuffle=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZjLxJGnGCKN0","executionInfo":{"status":"ok","timestamp":1640023965157,"user_tz":-420,"elapsed":2356,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"32eefc01-c1a3-4016-8cb2-aa286ddb8c99"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 1932/1932 [00:01<00:00, 1278.14it/s]\n","100%|██████████| 934/934 [00:00<00:00, 1273.62it/s]\n"]}]},{"cell_type":"code","source":["from sklearn.utils.class_weight import compute_class_weight\n","y_org = Y_label_train\n","class_weight = compute_class_weight(class_weight='balanced', classes = np.array([0,1,2,3,4,5,6,7,8]), y=y_org.flatten()[y_org.flatten()>=0])"],"metadata":{"id":"115wGGzbC2JF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","class Roberta_SeqTag(BertPreTrainedModel):\n","    config_class = RobertaConfig\n","    base_model_prefix = \"roberta\"\n","    def __init__(self, config):\n","        super(Roberta_SeqTag, self).__init__(config)\n","        self.num_labels = config.num_labels\n","        self.roberta = RobertaModel(config, add_pooling_layer=False)\n","\n","        classifier_dropout = (\n","                config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n","            )\n","        self.dropout = nn.Dropout(classifier_dropout)\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","        # self.lstm = nn.LSTM(config.hidden_size, 256, num_layers=1, bidirectional=True)\n","        # self.classifier = nn.Linear(256*2, config.num_labels)\n","        self.post_init()\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","\n","        outputs = self.roberta(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        sequence_output = outputs[0]\n","\n","        sequence_output = self.dropout(sequence_output)\n","        # x, _ = self.lstm(sequence_output)\n","        # sequence_output = torch.tanh(x)\n","        logits = self.classifier(sequence_output)\n","        # print(logits.shape)\n","        loss = None\n","        if labels is not None:\n","            loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(class_weight, dtype=torch.float).to(device))\n","            # Only keep active parts of the loss\n","            if attention_mask is not None:\n","                active_loss = attention_mask.view(-1) == 1\n","                active_logits = logits.view(-1, self.num_labels)\n","                active_labels = torch.where(\n","                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n","                )\n","                # print(\n","                #     'active loss', active_loss, active_loss.shape,\n","                #     'active_logits', active_logits, active_logits.shape,\n","                #     'active_labels', active_labels, active_labels.shape)\n","                loss = loss_fct(active_logits, active_labels)\n","                # quit()\n","            else:\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","\n","        if not return_dict:\n","            output = (logits,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return TokenClassifierOutput(\n","            loss=loss,\n","            logits=logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )\n"],"metadata":{"id":"MoLXl6LuC9HP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = Roberta_SeqTag.from_pretrained(args.pretrained_path, config=config)\n","model.cuda()\n"],"metadata":{"id":"vJGZgTq-C-p1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Defining the training function on the 80% of the dataset for tuning the bert model\n","def train(epoch, verbose = False):\n","    tr_loss, tr_accuracy = 0, 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","    tr_preds, tr_labels = [], []\n","    # put model in training mode\n","    model.train()\n","    \n","    for idx, batch in enumerate(train_loader):\n","        ids, labels = batch\n","        ids = ids.to(device)\n","        labels = labels.to(device)\n","        mask = ids!=1\n","        \n","        # check(ids, mask, labels)\n","        # break\n","\n","        outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n","        loss = outputs[0]\n","        tr_logits = outputs[1]\n","        tr_loss += loss.item()\n","\n","        nb_tr_steps += 1\n","        nb_tr_examples += labels.size(0)\n","        \n","        if idx % 100==0 and verbose:\n","            loss_step = tr_loss/nb_tr_steps\n","            print(f\"Training loss per 100 training steps: {loss_step}\")\n","           \n","        # compute training accuracy\n","        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","        # print(active_logits)\n","        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","        # print(flattened_predictions)\n","        \n","        # only compute accuracy at active labels\n","        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n","        \n","        labels = torch.masked_select(flattened_targets, active_accuracy)\n","        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","        \n","        tr_labels.extend(labels)\n","        tr_preds.extend(predictions)\n","\n","        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n","        tr_accuracy += tmp_tr_accuracy\n","    \n","        # gradient clipping\n","        torch.nn.utils.clip_grad_norm_(\n","            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n","        )\n","        \n","        # backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","        optimizer.step()\n","\n","    epoch_loss = tr_loss / nb_tr_steps\n","    tr_accuracy = tr_accuracy / nb_tr_steps\n","\n","    labels = [ids_to_labels[id.item()] for id in tr_labels]\n","    predictions = [ids_to_labels[id.item()] for id in tr_preds]\n","    f1 = seqeval.metrics.f1_score([labels], [predictions])\n","\n","    print(f\"Training loss epoch: {epoch_loss}\", f\"Training F1 epoch: {f1}\")"],"metadata":{"id":"oLsG6Ko9DAoE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def valid(model, test_loader, verbose=False):\n","    # put model in evaluation mode\n","    model.eval()\n","    \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_examples, nb_eval_steps = 0, 0\n","    eval_preds, eval_labels = [], []\n","    \n","    with torch.no_grad():\n","        for idx, batch in enumerate(test_loader):\n","            \n","            ids, labels = batch\n","            ids = ids.to(device)\n","            labels = labels.to(device)\n","            mask = ids!=1 \n","\n","            outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n","            loss = outputs[0]\n","            eval_logits = outputs[1]\n","            eval_loss += loss.item()\n","\n","            nb_eval_steps += 1\n","            nb_eval_examples += labels.size(0)\n","        \n","            if idx % 100==0 and verbose:\n","                loss_step = eval_loss/nb_eval_steps\n","                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n","              \n","            # compute evaluation accuracy\n","            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","            \n","            # only compute accuracy at active labels\n","            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","        \n","            labels = torch.masked_select(flattened_targets, active_accuracy)\n","            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","            \n","            eval_labels.extend(labels)\n","            eval_preds.extend(predictions)\n","            \n","            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n","            eval_accuracy += tmp_eval_accuracy\n","\n","    labels = [ids_to_labels[id.item()] for id in eval_labels]\n","    predictions = [ids_to_labels[id.item()] for id in eval_preds]\n","    \n","    eval_loss = eval_loss / nb_eval_steps\n","    eval_accuracy = eval_accuracy / nb_eval_steps\n","    f1 = seqeval.metrics.f1_score([labels], [predictions])\n","    print(f\"Validation Loss: {eval_loss}\", f\"Validation F1: {f1}\")\n","\n","    return labels, predictions"],"metadata":{"id":"XSsv_7wDDByM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule\n","\n","# Creating optimizer and lr schedulers\n","param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","num_train_optimization_steps = int(args.epochs*len(train_dataset)/args.batch_size/args.accumulation_steps)\n","optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=num_train_optimization_steps)  # PyTorch scheduler\n","scheduler0 = get_constant_schedule(optimizer)  # PyTorch scheduler\n","\n","tsfm = model.roberta\n","for child in tsfm.children():\n","    for param in child.parameters():\n","        if not param.requires_grad:\n","            print(\"whoopsies\")\n","        param.requires_grad = False\n","frozen = True\n","\n","EPOCHS = 5\n","import time\n","\n","for epoch in range(EPOCHS):\n","    if epoch > 0 and frozen:\n","        for child in tsfm.children():\n","            for param in child.parameters():\n","                param.requires_grad = True\n","        frozen = False\n","        del scheduler0\n","        torch.cuda.empty_cache()\n","    st = time.time()\n","    print(f\"Training epoch: {epoch + 1}\")\n","    train(epoch)\n","    labels, predictions = valid(model, valid_loader)\n","    \n","    print('Time: ',time.time() - st)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Wt1aNCj7DDPK","executionInfo":{"status":"ok","timestamp":1640027119530,"user_tz":-420,"elapsed":363701,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"5c36c863-4f14-434c-9d25-65099f69e88b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training epoch: 1\n","Training loss epoch: 0.41969581694953434 Training F1 epoch: 0.059007832898172324\n","Validation Loss: 0.26131024922401797 Validation F1: 0.0\n","Time:  32.54197573661804\n","Training epoch: 2\n","Training loss epoch: 0.018631261992490232 Training F1 epoch: 0.9401363050285503\n","Validation Loss: 0.046891400712337265 Validation F1: 0.7998414585810543\n","Time:  83.0831093788147\n","Training epoch: 3\n","Training loss epoch: 0.0003756325251053782 Training F1 epoch: 1.0\n","Validation Loss: 0.04141283615487836 Validation F1: 0.8848776574408344\n","Time:  82.51424026489258\n","Training epoch: 4\n","Training loss epoch: 0.0017992888610816045 Training F1 epoch: 0.9979496738117428\n","Validation Loss: 0.043928374911619855 Validation F1: 0.923015873015873\n","Time:  82.69051885604858\n","Training epoch: 5\n","Training loss epoch: 0.006399250457575617 Training F1 epoch: 0.9865420560747664\n","Validation Loss: 0.02750404675098408 Validation F1: 0.9050807404489958\n","Time:  82.56903052330017\n"]}]},{"cell_type":"code","source":["for item in zip(labels, predictions):\n","    print(item)"],"metadata":{"id":"t589xKV0DMmi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from seqeval.metrics import classification_report\n","print(classification_report([labels], [predictions]))"],"metadata":{"id":"ASyV7oJgDLGu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModel, AutoTokenizer, AutoModelForTokenClassification\n","phobert = AutoModel.from_pretrained(\"vinai/phobert-base\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tyviNHv6ETSP","executionInfo":{"status":"ok","timestamp":1640026343809,"user_tz":-420,"elapsed":2004,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"8905f62e-5778-48e3-f31e-c82e3ae8056a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.decoder.bias', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"]}]},{"cell_type":"code","source":["phobert"],"metadata":{"id":"5JiAdKYnEqrM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# model = RobertaForTokenClassification.from_pretrained(\"vinai/phobert-base\", config=config)\n","# model.cuda()\n","\n","model = AutoModelForTokenClassification.from_pretrained(\"vinai/phobert-base\", num_labels=9)\n","model.cuda()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lhS3m55QEzfw","executionInfo":{"status":"ok","timestamp":1640026754656,"user_tz":-420,"elapsed":1876,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"86130c59-960f-41c1-f916-e88e945e6dd6"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at vinai/phobert-base were not used when initializing RobertaForTokenClassification: ['lm_head.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.dense.weight', 'lm_head.dense.bias']\n","- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at vinai/phobert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["RobertaForTokenClassification(\n","  (roberta): RobertaModel(\n","    (embeddings): RobertaEmbeddings(\n","      (word_embeddings): Embedding(64001, 768, padding_idx=1)\n","      (position_embeddings): Embedding(258, 768, padding_idx=1)\n","      (token_type_embeddings): Embedding(1, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): RobertaEncoder(\n","      (layer): ModuleList(\n","        (0): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): RobertaLayer(\n","          (attention): RobertaAttention(\n","            (self): RobertaSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): RobertaSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): RobertaIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","          )\n","          (output): RobertaOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=9, bias=True)\n",")"]},"metadata":{},"execution_count":103}]},{"cell_type":"code","source":["from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer"],"metadata":{"id":"XOD20IXTIZr6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name = 'phobert'\n","args = TrainingArguments(\n","    f\"{model_name}-finetuned-{'ner'}\",\n","    evaluation_strategy = \"epoch\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=8,\n","    per_device_eval_batch_size=4,\n","    num_train_epochs=3,\n","    weight_decay=0.01,\n","    push_to_hub=True,\n",")"],"metadata":{"id":"nV76MjRpIWkx"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yQwQSVCAXPaX"},"source":["## Legacy"]},{"cell_type":"markdown","metadata":{"id":"QWslPozfnnPt"},"source":["The following code blocks were used during the development of this notebook, but are not included anymore."]},{"cell_type":"markdown","source":["label for subword "],"metadata":{"id":"zZmdgtpumBfH"}},{"cell_type":"code","source":["# text = '<s> '+'Hôm_nay trời nóng quá nên tôi ở nhà viết Viblo!' +' </s>'\n","text = 'nên xem_lại tư_duy bán hàng , bán thua hàng_chợ thì cần xem_lại , đặt giá gốc cao xong hạ saleoff lừa à !' \n","subwords = bpe.encode(text)\n","subwords = '<s> '+subwords +' </s>'\n","input_ids = vocab.encode_line(subwords, append_eos=False, add_if_not_exist=False).long().tolist()\n","print(subwords)\n","print(input_ids)\n","subword_idx = [subwords.split().index(word) for word in subwords.split() if '@@' in word]\n","print(subword_idx)\n","tag_list = 'O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-PRI,I-PRI,I-PRI,O,B-PRI,B-PRI,O,O,O,O'.split(',')\n","for i, idx in enumerate(subword_idx):\n","    orig_idx = idx - i\n","    tag_list.insert(orig_idx+1, tag_list[orig_idx])\n","for pair in zip(subwords.split(), tag_list):\n","    print(pair)"],"metadata":{"id":"Gzu7uvhCmAS4"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k3okbiKoXPaY"},"source":["def prepare_sentence(sentence, tokenizer, maxlen):    \n","      # step 1: tokenize the sentence\n","      tokenized_sentence = tokenizer.tokenize(sentence)\n","      \n","      # step 2: add special tokens \n","      tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] \n","\n","      # step 3: truncating/padding\n","      if (len(tokenized_sentence) > maxlen):\n","        # truncate\n","        tokenized_sentence = tokenized_sentence[:maxlen]\n","      else:\n","        # pad\n","        tokenized_sentence = tokenized_sentence + ['[PAD]'for _ in range(maxlen - len(tokenized_sentence))]\n","\n","      # step 4: obtain the attention mask\n","      attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n","      \n","      # step 5: convert tokens to input ids\n","      ids = tokenizer.convert_tokens_to_ids(tokenized_sentence)\n","      \n","      return {\n","            'ids': torch.tensor(ids, dtype=torch.long),\n","            'mask': torch.tensor(attn_mask, dtype=torch.long),\n","            #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n","      }"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dyAL9FIRRcTg"},"source":["def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n","    \"\"\"\n","    Word piece tokenization makes it difficult to match word labels\n","    back up with individual word pieces. This function tokenizes each\n","    word one at a time so that it is easier to preserve the correct\n","    label for each subword. It is, of course, a bit slower in processing\n","    time, but it will help our model achieve higher accuracy.\n","    \"\"\"\n","\n","    tokenized_sentence = []\n","    labels = []\n","\n","    sentence = sentence.strip()\n","\n","    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n","\n","        # Tokenize the word and count # of subwords the word is broken into\n","        tokenized_word = tokenizer.tokenize(word)\n","        n_subwords = len(tokenized_word)\n","\n","        # Add the tokenized word to the final tokenized word list\n","        tokenized_sentence.extend(tokenized_word)\n","\n","        # Add the same label to the new list of labels `n_subwords` times\n","        labels.extend([label] * n_subwords)\n","\n","    return tokenized_sentence, labels"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xWSn_glJRksn"},"source":["class dataset(Dataset):\n","    def __init__(self, dataframe, tokenizer, max_len):\n","        self.len = len(dataframe)\n","        self.data = dataframe\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","        \n","    def __getitem__(self, index):\n","        # step 1: tokenize (and adapt corresponding labels)\n","        sentence = self.data.sentence[index]  \n","        word_labels = self.data.word_labels[index]  \n","        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n","        \n","        # step 2: add special tokens (and corresponding labels)\n","        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] # add special tokens\n","        labels.insert(0, \"O\") # add outside label for [CLS] token\n","        labels.insert(-1, \"O\") # add outside label for [SEP] token\n","\n","        # step 3: truncating/padding\n","        maxlen = self.max_len\n","\n","        if (len(tokenized_sentence) > maxlen):\n","          # truncate\n","          tokenized_sentence = tokenized_sentence[:maxlen]\n","          labels = labels[:maxlen]\n","        else:\n","          # pad\n","          tokenized_sentence = tokenized_sentence + ['[PAD]'for _ in range(maxlen - len(tokenized_sentence))]\n","          labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n","\n","        # step 4: obtain the attention mask\n","        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n","        \n","        # step 5: convert tokens to input ids\n","        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n","\n","        label_ids = [labels_to_ids[label] for label in labels]\n","        # the following line is deprecated\n","        #label_ids = [label if label != 0 else -100 for label in label_ids]\n","        \n","        return {\n","              'ids': torch.tensor(ids, dtype=torch.long),\n","              'mask': torch.tensor(attn_mask, dtype=torch.long),\n","              #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n","              'targets': torch.tensor(label_ids, dtype=torch.long)\n","        } \n","    \n","    def __len__(self):\n","        return self.len"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a9-BqpvYjkzN"},"source":["sentence = \"this is a test @huggingface\".strip().split()\n","\n","inputs = tokenizer(sentence, is_pretokenized=True, return_offsets_mapping=True, padding='max_length', truncation=True)\n","tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])\n","token_offsets = inputs[\"offset_mapping\"]\n","print(tokens)\n","print(token_offsets)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pw4AfDxJlU_Z"},"source":["word = \"@huggingface\"\n","\n","inputs = tokenizer(word, return_offsets_mapping=True, padding='max_length', truncation=True)\n","tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])\n","token_offsets = inputs[\"offset_mapping\"]\n","print(tokens)\n","print(token_offsets)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W9LHi_V5WZ9z"},"source":["# now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n","        active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n","        targets = torch.masked_select(flattened_targets, active_accuracy)\n","        predictions = torch.masked_select(flattened_predictions, active_accuracy)"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# check for initial loss\n","input_ids, labels = next(iter(train_loader))\n","input_ids.shape, labels.shape\n","\n","input_ids = input_ids.to(device)\n","labels = labels.to(device)\n","\n","outputs = model(input_ids, attention_mask=input_ids!=1, labels=labels)\n","initial_loss = outputs[0]\n","initial_loss"],"metadata":{"id":"qfLFlRHXH-gN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","from src.loss import FocalLoss\n","from torch.nn import CrossEntropyLoss\n","# test focal loss\n","logits = torch.rand(3, 3, 3)\n","labels = torch.LongTensor([[0,1,1],[1, 2, 2],[2,0,1]])\n","focal_loss = FocalLoss(gamma = 0, alpha = 1)\n","print(focal_loss(logits, labels))\n","loss_fct = CrossEntropyLoss()\n","seq_loss = loss_fct(logits.permute(0, 2, 1), labels)\n","print(seq_loss)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"PMFiEy-as5Qr","executionInfo":{"status":"ok","timestamp":1639968702781,"user_tz":-420,"elapsed":524,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"86bd4a57-5f25-4b59-cf37-c0f4afe6a148"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(1.1434)\n","tensor(1.0161)\n"]},{"output_type":"stream","name":"stderr","text":["/content/drive/My Drive/NLP/project_nlp/src/loss.py:43: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n","  log_p = F.log_softmax(logits)\n"]}]},{"cell_type":"code","source":["import torch\n","from src.loss import *\n","\n","criterion = SelfAdjDiceLoss(reduction=\"none\")\n","# (batch_size, num_tokens, num_classes)\n","logits = torch.rand(3, 3, 3)\n","targets = torch.LongTensor([[0,1,1],[1, 2, 2],[2,0,1]])\n","# logits = torch.rand(128, 40, 10, requires_grad=True)\n","# targets = torch.randint(0, 10, size=(128, 40))\n","\n","loss = criterion(logits.view(-1, 3), targets.view(-1))\n","loss = loss.reshape(-1, 9).mean(-1).mean()\n","# loss.backward()\n","seq_loss = loss_fct(logits.permute(0, 2, 1), labels)\n","print(seq_loss)\n","loss"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"3XkOjkZDt0nN","executionInfo":{"status":"ok","timestamp":1639968723247,"user_tz":-420,"elapsed":357,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"b7540b1a-3dc0-4515-9795-093e6879a89c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor(1.0425)\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor(0.3492)"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["import torch\n","from src.loss import *\n","\n","criterion = SelfAdjDiceLoss(reduction=\"none\")\n","# (batch_size, num_tokens, num_classes)\n","logits = torch.rand(3, 3, 3)\n","targets = torch.LongTensor([[0,1,1],[1, 2, 2],[2,0,1]])\n","# logits = torch.rand(128, 40, 10, requires_grad=True)\n","# targets = torch.randint(0, 10, size=(128, 40))\n","\n","loss = criterion(logits.view(-1, 3), targets.view(-1))\n","loss = loss.reshape(-1, 9).mean(-1).mean()\n","# loss.backward()\n","loss"],"metadata":{"id":"Gjwi3jA3uOYh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["%cd /content"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Ve0wKflPz5o2","executionInfo":{"status":"ok","timestamp":1639969701995,"user_tz":-420,"elapsed":383,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"47444c23-5e14-4d58-c5ad-1d51afae656f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content\n"]}]},{"cell_type":"code","source":["!git clone https://github.com/Gxzzz/BiLSTM-CRF.git"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GfGSEeEKz3sS","executionInfo":{"status":"ok","timestamp":1639969747190,"user_tz":-420,"elapsed":6234,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"48e93909-cc79-4f41-d6a3-165a37459ec3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Cloning into 'BiLSTM-CRF'...\n","remote: Enumerating objects: 38, done.\u001b[K\n","remote: Counting objects: 100% (38/38), done.\u001b[K\n","remote: Compressing objects: 100% (30/30), done.\u001b[K\n","remote: Total 38 (delta 12), reused 32 (delta 7), pack-reused 0\u001b[K\n","Unpacking objects: 100% (38/38), done.\n"]}]},{"cell_type":"code","source":["%cd  BiLSTM-CRF"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sLxxy51g0FnP","executionInfo":{"status":"ok","timestamp":1639969758179,"user_tz":-420,"elapsed":2,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"4e7a8142-9d08-43b3-b162-4d92f91b5bad"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/BiLSTM-CRF\n"]}]},{"cell_type":"code","source":["!sh run.sh train"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MBv8k5UI0H0M","executionInfo":{"status":"ok","timestamp":1639970611033,"user_tz":-420,"elapsed":850728,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"f712e9b0-129f-4ffe-f7c3-6050931170a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["num of training examples: 40526\n","num of development examples: 10132\n","start training...\n","log: epoch 1, iter 10, 19481.0 words/sec, avg_loss 99.965345, time 0.8 sec\n","log: epoch 1, iter 20, 20281.8 words/sec, avg_loss 67.051926, time 0.7 sec\n","log: epoch 1, iter 30, 20410.7 words/sec, avg_loss 57.057752, time 0.7 sec\n","log: epoch 1, iter 40, 21280.2 words/sec, avg_loss 56.877848, time 0.7 sec\n","log: epoch 1, iter 50, 20123.4 words/sec, avg_loss 53.036066, time 0.7 sec\n","log: epoch 1, iter 60, 18880.7 words/sec, avg_loss 48.014140, time 0.7 sec\n","log: epoch 1, iter 70, 20524.0 words/sec, avg_loss 51.402844, time 0.7 sec\n","log: epoch 1, iter 80, 20496.1 words/sec, avg_loss 47.476018, time 0.7 sec\n","log: epoch 1, iter 90, 19278.2 words/sec, avg_loss 46.728068, time 0.7 sec\n","log: epoch 1, iter 100, 19415.3 words/sec, avg_loss 48.263848, time 0.8 sec\n","log: epoch 1, iter 110, 20988.8 words/sec, avg_loss 44.111340, time 0.7 sec\n","log: epoch 1, iter 120, 20159.1 words/sec, avg_loss 44.468773, time 0.7 sec\n","log: epoch 1, iter 130, 20108.6 words/sec, avg_loss 41.337513, time 0.7 sec\n","log: epoch 1, iter 140, 20118.7 words/sec, avg_loss 38.348198, time 0.7 sec\n","log: epoch 1, iter 150, 20537.2 words/sec, avg_loss 41.166132, time 0.7 sec\n","log: epoch 1, iter 160, 20257.1 words/sec, avg_loss 41.577662, time 0.7 sec\n","log: epoch 1, iter 170, 20383.8 words/sec, avg_loss 40.191600, time 0.7 sec\n","log: epoch 1, iter 180, 20925.0 words/sec, avg_loss 37.693908, time 0.7 sec\n","log: epoch 1, iter 190, 22230.5 words/sec, avg_loss 38.693996, time 0.7 sec\n","log: epoch 1, iter 200, 20466.6 words/sec, avg_loss 37.582080, time 0.7 sec\n","log: epoch 1, iter 210, 20937.1 words/sec, avg_loss 36.674812, time 0.7 sec\n","log: epoch 1, iter 220, 20110.0 words/sec, avg_loss 35.893419, time 0.7 sec\n","log: epoch 1, iter 230, 19921.7 words/sec, avg_loss 35.338857, time 0.7 sec\n","log: epoch 1, iter 240, 19687.5 words/sec, avg_loss 34.758796, time 0.7 sec\n","log: epoch 1, iter 250, 19048.9 words/sec, avg_loss 33.781070, time 0.7 sec\n","dev: epoch 1, iter 250, 20223.3 words/sec, avg_loss 46.299680, time 17.8 sec\n","dev: epoch 1, iter 250, dev_loss 10.934281, patience 0, decay_num 0\n","log: epoch 1, iter 260, 9491.3 words/sec, avg_loss 34.841412, time 1.6 sec\n","log: epoch 1, iter 270, 12488.3 words/sec, avg_loss 34.784444, time 1.2 sec\n","log: epoch 1, iter 280, 12224.5 words/sec, avg_loss 32.835330, time 1.2 sec\n","log: epoch 1, iter 290, 17278.7 words/sec, avg_loss 31.891047, time 0.8 sec\n","log: epoch 1, iter 300, 19966.5 words/sec, avg_loss 31.985559, time 0.7 sec\n","log: epoch 1, iter 310, 19783.2 words/sec, avg_loss 31.104577, time 0.7 sec\n","log: epoch 1, iter 320, 19998.1 words/sec, avg_loss 30.885633, time 0.7 sec\n","log: epoch 1, iter 330, 19847.2 words/sec, avg_loss 31.772088, time 0.7 sec\n","log: epoch 1, iter 340, 19843.2 words/sec, avg_loss 28.670574, time 0.7 sec\n","log: epoch 1, iter 350, 20720.2 words/sec, avg_loss 27.997496, time 0.7 sec\n","log: epoch 1, iter 360, 20624.8 words/sec, avg_loss 28.763523, time 0.7 sec\n","log: epoch 1, iter 370, 20751.5 words/sec, avg_loss 28.328058, time 0.7 sec\n","log: epoch 1, iter 380, 19508.1 words/sec, avg_loss 26.980623, time 0.7 sec\n","log: epoch 1, iter 390, 20390.3 words/sec, avg_loss 28.299733, time 0.7 sec\n","log: epoch 1, iter 400, 21017.0 words/sec, avg_loss 26.958809, time 0.7 sec\n","log: epoch 1, iter 410, 19964.0 words/sec, avg_loss 25.541257, time 0.7 sec\n","log: epoch 1, iter 420, 20315.2 words/sec, avg_loss 26.314578, time 0.7 sec\n","log: epoch 1, iter 430, 20448.8 words/sec, avg_loss 26.348939, time 0.7 sec\n","log: epoch 1, iter 440, 20216.9 words/sec, avg_loss 24.501040, time 0.7 sec\n","log: epoch 1, iter 450, 20528.9 words/sec, avg_loss 25.019165, time 0.7 sec\n","log: epoch 1, iter 460, 19407.0 words/sec, avg_loss 23.699189, time 0.7 sec\n","log: epoch 1, iter 470, 19958.1 words/sec, avg_loss 24.190450, time 0.7 sec\n","log: epoch 1, iter 480, 20354.0 words/sec, avg_loss 22.791757, time 0.7 sec\n","log: epoch 1, iter 490, 19312.3 words/sec, avg_loss 21.288452, time 0.7 sec\n","log: epoch 1, iter 500, 19189.3 words/sec, avg_loss 22.567341, time 0.7 sec\n","dev: epoch 1, iter 500, 18204.8 words/sec, avg_loss 27.934443, time 19.6 sec\n","dev: epoch 1, iter 500, dev_loss 6.750303, patience 0, decay_num 0\n","log: epoch 1, iter 510, 20799.8 words/sec, avg_loss 23.835563, time 0.7 sec\n","log: epoch 1, iter 520, 20390.2 words/sec, avg_loss 21.036652, time 0.7 sec\n","log: epoch 1, iter 530, 20004.1 words/sec, avg_loss 22.214608, time 0.7 sec\n","log: epoch 1, iter 540, 20509.2 words/sec, avg_loss 21.742855, time 0.7 sec\n","log: epoch 1, iter 550, 20078.3 words/sec, avg_loss 21.612525, time 0.7 sec\n","log: epoch 1, iter 560, 20042.8 words/sec, avg_loss 21.226640, time 0.7 sec\n","log: epoch 1, iter 570, 19711.6 words/sec, avg_loss 19.828271, time 0.7 sec\n","log: epoch 1, iter 580, 19528.0 words/sec, avg_loss 19.767827, time 0.7 sec\n","log: epoch 1, iter 590, 21193.3 words/sec, avg_loss 20.119997, time 0.7 sec\n","log: epoch 1, iter 600, 19582.2 words/sec, avg_loss 19.061978, time 0.7 sec\n","log: epoch 1, iter 610, 19685.3 words/sec, avg_loss 19.896927, time 0.7 sec\n","log: epoch 1, iter 620, 20253.0 words/sec, avg_loss 18.457099, time 0.7 sec\n","log: epoch 1, iter 630, 21148.0 words/sec, avg_loss 18.363396, time 0.7 sec\n","log: epoch 1, iter 640, 19649.3 words/sec, avg_loss 18.394722, time 0.7 sec\n","log: epoch 1, iter 650, 19432.2 words/sec, avg_loss 16.703816, time 0.7 sec\n","log: epoch 1, iter 660, 20054.3 words/sec, avg_loss 16.755880, time 0.7 sec\n","log: epoch 1, iter 670, 19836.8 words/sec, avg_loss 16.258210, time 0.7 sec\n","log: epoch 1, iter 680, 20367.7 words/sec, avg_loss 17.059714, time 0.7 sec\n","log: epoch 1, iter 690, 21826.3 words/sec, avg_loss 17.028792, time 0.7 sec\n","log: epoch 1, iter 700, 19838.2 words/sec, avg_loss 16.245626, time 0.7 sec\n","log: epoch 1, iter 710, 21393.9 words/sec, avg_loss 16.618006, time 0.7 sec\n","log: epoch 1, iter 720, 20782.4 words/sec, avg_loss 15.780601, time 0.7 sec\n","log: epoch 1, iter 730, 20778.1 words/sec, avg_loss 16.006494, time 0.7 sec\n","log: epoch 1, iter 740, 19014.9 words/sec, avg_loss 15.502578, time 0.8 sec\n","log: epoch 1, iter 750, 20870.8 words/sec, avg_loss 14.672719, time 0.7 sec\n","dev: epoch 1, iter 750, 20255.7 words/sec, avg_loss 18.567660, time 17.7 sec\n","dev: epoch 1, iter 750, dev_loss 4.594445, patience 0, decay_num 0\n","log: epoch 1, iter 760, 19404.5 words/sec, avg_loss 14.818978, time 0.7 sec\n","log: epoch 1, iter 770, 20105.9 words/sec, avg_loss 14.207657, time 0.7 sec\n","log: epoch 1, iter 780, 21267.7 words/sec, avg_loss 15.416889, time 0.7 sec\n","log: epoch 1, iter 790, 18970.7 words/sec, avg_loss 15.107757, time 0.8 sec\n","log: epoch 1, iter 800, 20469.8 words/sec, avg_loss 14.415446, time 0.7 sec\n","log: epoch 1, iter 810, 20417.1 words/sec, avg_loss 13.552506, time 0.7 sec\n","log: epoch 1, iter 820, 19961.6 words/sec, avg_loss 12.887880, time 0.7 sec\n","log: epoch 1, iter 830, 20099.2 words/sec, avg_loss 12.781316, time 0.7 sec\n","log: epoch 1, iter 840, 20335.9 words/sec, avg_loss 13.099041, time 0.7 sec\n","log: epoch 1, iter 850, 20964.1 words/sec, avg_loss 13.756572, time 0.7 sec\n","log: epoch 1, iter 860, 20714.3 words/sec, avg_loss 13.312661, time 0.7 sec\n","log: epoch 1, iter 870, 20012.3 words/sec, avg_loss 12.429857, time 0.7 sec\n","log: epoch 1, iter 880, 19375.9 words/sec, avg_loss 12.256513, time 0.7 sec\n","log: epoch 1, iter 890, 21150.3 words/sec, avg_loss 12.825531, time 0.7 sec\n","log: epoch 1, iter 900, 19956.7 words/sec, avg_loss 12.942728, time 0.7 sec\n","log: epoch 1, iter 910, 20244.2 words/sec, avg_loss 13.123302, time 0.7 sec\n","log: epoch 1, iter 920, 19574.9 words/sec, avg_loss 12.731136, time 0.7 sec\n","log: epoch 1, iter 930, 20700.7 words/sec, avg_loss 11.729402, time 0.7 sec\n","log: epoch 1, iter 940, 20221.8 words/sec, avg_loss 10.535092, time 0.7 sec\n","log: epoch 1, iter 950, 20839.2 words/sec, avg_loss 11.835815, time 0.7 sec\n","log: epoch 1, iter 960, 20346.4 words/sec, avg_loss 10.654290, time 0.7 sec\n","log: epoch 1, iter 970, 20100.2 words/sec, avg_loss 11.212370, time 0.7 sec\n","log: epoch 1, iter 980, 20304.4 words/sec, avg_loss 10.546235, time 0.7 sec\n","log: epoch 1, iter 990, 19713.8 words/sec, avg_loss 10.772370, time 0.7 sec\n","log: epoch 1, iter 1000, 20247.9 words/sec, avg_loss 10.848633, time 0.7 sec\n","dev: epoch 1, iter 1000, 20208.1 words/sec, avg_loss 12.711999, time 17.8 sec\n","dev: epoch 1, iter 1000, dev_loss 3.991679, patience 0, decay_num 0\n","log: epoch 1, iter 1010, 21013.4 words/sec, avg_loss 12.090597, time 0.7 sec\n","log: epoch 1, iter 1020, 20064.5 words/sec, avg_loss 10.779678, time 0.7 sec\n","log: epoch 1, iter 1030, 20462.6 words/sec, avg_loss 11.283015, time 0.7 sec\n","log: epoch 1, iter 1040, 20093.2 words/sec, avg_loss 10.450441, time 0.7 sec\n","log: epoch 1, iter 1050, 19426.3 words/sec, avg_loss 9.834857, time 0.7 sec\n","log: epoch 1, iter 1060, 20395.9 words/sec, avg_loss 9.789522, time 0.7 sec\n","log: epoch 1, iter 1070, 20820.7 words/sec, avg_loss 10.804954, time 0.7 sec\n","log: epoch 1, iter 1080, 19837.7 words/sec, avg_loss 8.987224, time 0.7 sec\n","log: epoch 1, iter 1090, 20206.0 words/sec, avg_loss 9.752818, time 0.7 sec\n","log: epoch 1, iter 1100, 19664.1 words/sec, avg_loss 8.815481, time 0.7 sec\n","log: epoch 1, iter 1110, 20044.5 words/sec, avg_loss 9.618431, time 0.7 sec\n","log: epoch 1, iter 1120, 19740.0 words/sec, avg_loss 9.493677, time 0.7 sec\n","log: epoch 1, iter 1130, 19207.5 words/sec, avg_loss 9.244301, time 0.8 sec\n","log: epoch 1, iter 1140, 20210.1 words/sec, avg_loss 9.069850, time 0.7 sec\n","log: epoch 1, iter 1150, 20330.7 words/sec, avg_loss 9.240903, time 0.7 sec\n","log: epoch 1, iter 1160, 19089.5 words/sec, avg_loss 9.317807, time 0.7 sec\n","log: epoch 1, iter 1170, 21404.7 words/sec, avg_loss 8.594013, time 0.7 sec\n","log: epoch 1, iter 1180, 20644.2 words/sec, avg_loss 8.756043, time 0.7 sec\n","log: epoch 1, iter 1190, 20566.7 words/sec, avg_loss 9.116378, time 0.7 sec\n","log: epoch 1, iter 1200, 20820.7 words/sec, avg_loss 8.264434, time 0.7 sec\n","log: epoch 1, iter 1210, 21964.4 words/sec, avg_loss 8.435277, time 0.7 sec\n","log: epoch 1, iter 1220, 19460.2 words/sec, avg_loss 8.195716, time 0.7 sec\n","log: epoch 1, iter 1230, 21668.4 words/sec, avg_loss 7.766607, time 0.7 sec\n","log: epoch 1, iter 1240, 20815.4 words/sec, avg_loss 7.958789, time 0.7 sec\n","log: epoch 1, iter 1250, 19880.9 words/sec, avg_loss 7.628349, time 0.7 sec\n","dev: epoch 1, iter 1250, 20289.6 words/sec, avg_loss 9.331566, time 17.8 sec\n","dev: epoch 1, iter 1250, dev_loss 3.083753, patience 0, decay_num 0\n","log: epoch 1, iter 1260, 20202.1 words/sec, avg_loss 7.825122, time 0.7 sec\n","log: epoch 2, iter 1270, 19571.8 words/sec, avg_loss 8.541074, time 0.7 sec\n","log: epoch 2, iter 1280, 20088.5 words/sec, avg_loss 7.955910, time 0.7 sec\n","log: epoch 2, iter 1290, 20068.6 words/sec, avg_loss 7.726385, time 0.7 sec\n","log: epoch 2, iter 1300, 20292.0 words/sec, avg_loss 7.593098, time 0.7 sec\n","log: epoch 2, iter 1310, 21958.2 words/sec, avg_loss 7.867095, time 0.7 sec\n","log: epoch 2, iter 1320, 20287.7 words/sec, avg_loss 7.047929, time 0.7 sec\n","log: epoch 2, iter 1330, 19762.9 words/sec, avg_loss 7.487465, time 0.7 sec\n","log: epoch 2, iter 1340, 19998.8 words/sec, avg_loss 7.683783, time 0.7 sec\n","log: epoch 2, iter 1350, 21171.4 words/sec, avg_loss 7.181192, time 0.7 sec\n","log: epoch 2, iter 1360, 13327.1 words/sec, avg_loss 7.739493, time 1.1 sec\n","log: epoch 2, iter 1370, 20590.1 words/sec, avg_loss 7.545932, time 0.7 sec\n","log: epoch 2, iter 1380, 20241.8 words/sec, avg_loss 7.312493, time 0.7 sec\n","log: epoch 2, iter 1390, 19994.7 words/sec, avg_loss 6.671944, time 0.7 sec\n","log: epoch 2, iter 1400, 20084.8 words/sec, avg_loss 7.324361, time 0.7 sec\n","log: epoch 2, iter 1410, 20325.4 words/sec, avg_loss 6.997279, time 0.7 sec\n","log: epoch 2, iter 1420, 20596.9 words/sec, avg_loss 6.850797, time 0.7 sec\n","log: epoch 2, iter 1430, 21547.6 words/sec, avg_loss 7.379163, time 0.7 sec\n","log: epoch 2, iter 1440, 19866.6 words/sec, avg_loss 7.295870, time 0.7 sec\n","log: epoch 2, iter 1450, 20848.9 words/sec, avg_loss 7.521533, time 0.7 sec\n","log: epoch 2, iter 1460, 19424.9 words/sec, avg_loss 6.027490, time 0.7 sec\n","log: epoch 2, iter 1470, 20589.3 words/sec, avg_loss 6.858889, time 0.7 sec\n","log: epoch 2, iter 1480, 20081.1 words/sec, avg_loss 7.199183, time 0.7 sec\n","log: epoch 2, iter 1490, 20802.8 words/sec, avg_loss 7.146233, time 0.7 sec\n","log: epoch 2, iter 1500, 20196.1 words/sec, avg_loss 6.694011, time 0.7 sec\n","dev: epoch 2, iter 1500, 19921.8 words/sec, avg_loss 7.336238, time 18.0 sec\n","dev: epoch 2, iter 1500, dev_loss 2.594432, patience 0, decay_num 0\n","log: epoch 2, iter 1510, 18974.8 words/sec, avg_loss 6.842743, time 0.8 sec\n","log: epoch 2, iter 1520, 19668.4 words/sec, avg_loss 7.052471, time 0.7 sec\n","log: epoch 2, iter 1530, 20306.9 words/sec, avg_loss 6.629381, time 0.7 sec\n","log: epoch 2, iter 1540, 19282.7 words/sec, avg_loss 6.982816, time 0.8 sec\n","log: epoch 2, iter 1550, 20099.7 words/sec, avg_loss 6.540605, time 0.7 sec\n","log: epoch 2, iter 1560, 20413.0 words/sec, avg_loss 6.358022, time 0.7 sec\n","log: epoch 2, iter 1570, 20185.1 words/sec, avg_loss 7.051756, time 0.7 sec\n","log: epoch 2, iter 1580, 20856.9 words/sec, avg_loss 6.818071, time 0.7 sec\n","log: epoch 2, iter 1590, 19820.0 words/sec, avg_loss 6.591361, time 0.7 sec\n","log: epoch 2, iter 1600, 19846.1 words/sec, avg_loss 6.803443, time 0.7 sec\n","log: epoch 2, iter 1610, 19090.8 words/sec, avg_loss 6.696258, time 0.8 sec\n","log: epoch 2, iter 1620, 19166.7 words/sec, avg_loss 5.992507, time 0.7 sec\n","log: epoch 2, iter 1630, 19132.7 words/sec, avg_loss 6.156125, time 0.7 sec\n","log: epoch 2, iter 1640, 20370.7 words/sec, avg_loss 6.564086, time 0.7 sec\n","log: epoch 2, iter 1650, 20346.2 words/sec, avg_loss 6.344090, time 0.7 sec\n","log: epoch 2, iter 1660, 20355.0 words/sec, avg_loss 6.361930, time 0.7 sec\n","log: epoch 2, iter 1670, 20429.5 words/sec, avg_loss 6.541043, time 0.7 sec\n","log: epoch 2, iter 1680, 19593.3 words/sec, avg_loss 6.225817, time 0.7 sec\n","log: epoch 2, iter 1690, 21148.7 words/sec, avg_loss 5.965147, time 0.7 sec\n","log: epoch 2, iter 1700, 21827.5 words/sec, avg_loss 6.176853, time 0.6 sec\n","log: epoch 2, iter 1710, 20890.6 words/sec, avg_loss 6.475775, time 0.7 sec\n","log: epoch 2, iter 1720, 20298.6 words/sec, avg_loss 6.375592, time 0.7 sec\n","log: epoch 2, iter 1730, 20990.6 words/sec, avg_loss 6.131727, time 0.7 sec\n","log: epoch 2, iter 1740, 19394.5 words/sec, avg_loss 6.214503, time 0.7 sec\n","log: epoch 2, iter 1750, 19728.9 words/sec, avg_loss 5.928454, time 0.7 sec\n","dev: epoch 2, iter 1750, 20064.2 words/sec, avg_loss 6.472823, time 17.9 sec\n","dev: epoch 2, iter 1750, dev_loss 2.353258, patience 0, decay_num 0\n","log: epoch 2, iter 1760, 19972.0 words/sec, avg_loss 5.706938, time 0.7 sec\n","log: epoch 2, iter 1770, 22637.8 words/sec, avg_loss 6.517163, time 0.7 sec\n","log: epoch 2, iter 1780, 13554.8 words/sec, avg_loss 5.780149, time 1.0 sec\n","log: epoch 2, iter 1790, 12528.9 words/sec, avg_loss 6.045911, time 1.1 sec\n","log: epoch 2, iter 1800, 19827.8 words/sec, avg_loss 5.743306, time 0.7 sec\n","log: epoch 2, iter 1810, 19321.7 words/sec, avg_loss 6.347853, time 0.8 sec\n","log: epoch 2, iter 1820, 19783.1 words/sec, avg_loss 6.197105, time 0.7 sec\n","log: epoch 2, iter 1830, 20326.5 words/sec, avg_loss 5.717922, time 0.7 sec\n","log: epoch 2, iter 1840, 19326.5 words/sec, avg_loss 5.832257, time 0.7 sec\n","log: epoch 2, iter 1850, 18621.9 words/sec, avg_loss 5.425708, time 0.8 sec\n","log: epoch 2, iter 1860, 18581.0 words/sec, avg_loss 5.056044, time 0.7 sec\n","log: epoch 2, iter 1870, 19581.8 words/sec, avg_loss 6.058150, time 0.8 sec\n","log: epoch 2, iter 1880, 20168.0 words/sec, avg_loss 5.571790, time 0.7 sec\n","log: epoch 2, iter 1890, 20685.2 words/sec, avg_loss 5.191674, time 0.7 sec\n","log: epoch 2, iter 1900, 20770.7 words/sec, avg_loss 6.003004, time 0.7 sec\n","log: epoch 2, iter 1910, 20185.3 words/sec, avg_loss 5.532498, time 0.7 sec\n","log: epoch 2, iter 1920, 20286.4 words/sec, avg_loss 5.604621, time 0.7 sec\n","log: epoch 2, iter 1930, 20799.0 words/sec, avg_loss 5.606459, time 0.7 sec\n","log: epoch 2, iter 1940, 21508.7 words/sec, avg_loss 5.813575, time 0.7 sec\n","log: epoch 2, iter 1950, 19779.5 words/sec, avg_loss 5.154615, time 0.7 sec\n","log: epoch 2, iter 1960, 21450.1 words/sec, avg_loss 5.424623, time 0.7 sec\n","log: epoch 2, iter 1970, 21933.6 words/sec, avg_loss 5.842798, time 0.7 sec\n","log: epoch 2, iter 1980, 19397.1 words/sec, avg_loss 5.438862, time 0.7 sec\n","log: epoch 2, iter 1990, 20200.9 words/sec, avg_loss 5.457271, time 0.7 sec\n","log: epoch 2, iter 2000, 20913.1 words/sec, avg_loss 5.128402, time 0.7 sec\n","dev: epoch 2, iter 2000, 19382.4 words/sec, avg_loss 5.687948, time 18.7 sec\n","dev: epoch 2, iter 2000, dev_loss 2.180177, patience 0, decay_num 0\n","log: epoch 2, iter 2010, 20975.3 words/sec, avg_loss 5.259336, time 0.7 sec\n","log: epoch 2, iter 2020, 20708.2 words/sec, avg_loss 5.892531, time 0.7 sec\n","log: epoch 2, iter 2030, 20263.2 words/sec, avg_loss 5.431166, time 0.7 sec\n","log: epoch 2, iter 2040, 19940.4 words/sec, avg_loss 4.758668, time 0.7 sec\n","log: epoch 2, iter 2050, 20768.5 words/sec, avg_loss 5.918354, time 0.7 sec\n","log: epoch 2, iter 2060, 20181.9 words/sec, avg_loss 4.775624, time 0.7 sec\n","log: epoch 2, iter 2070, 20902.0 words/sec, avg_loss 5.327081, time 0.7 sec\n","log: epoch 2, iter 2080, 20882.1 words/sec, avg_loss 5.570461, time 0.7 sec\n","log: epoch 2, iter 2090, 19891.1 words/sec, avg_loss 5.516327, time 0.7 sec\n","log: epoch 2, iter 2100, 19953.0 words/sec, avg_loss 4.966507, time 0.7 sec\n","log: epoch 2, iter 2110, 20806.6 words/sec, avg_loss 4.670788, time 0.7 sec\n","log: epoch 2, iter 2120, 21807.6 words/sec, avg_loss 5.529924, time 0.7 sec\n","log: epoch 2, iter 2130, 19594.8 words/sec, avg_loss 5.261707, time 0.7 sec\n","log: epoch 2, iter 2140, 19470.7 words/sec, avg_loss 4.209550, time 0.7 sec\n","log: epoch 2, iter 2150, 20357.3 words/sec, avg_loss 4.590702, time 0.7 sec\n","log: epoch 2, iter 2160, 21082.3 words/sec, avg_loss 5.055266, time 0.7 sec\n","log: epoch 2, iter 2170, 19985.5 words/sec, avg_loss 5.133358, time 0.7 sec\n","log: epoch 2, iter 2180, 19827.0 words/sec, avg_loss 5.376917, time 0.7 sec\n","log: epoch 2, iter 2190, 20015.9 words/sec, avg_loss 4.591420, time 0.7 sec\n","log: epoch 2, iter 2200, 20063.5 words/sec, avg_loss 5.277088, time 0.7 sec\n","log: epoch 2, iter 2210, 19359.3 words/sec, avg_loss 5.093236, time 0.7 sec\n","log: epoch 2, iter 2220, 18790.7 words/sec, avg_loss 5.332239, time 0.8 sec\n","log: epoch 2, iter 2230, 19307.2 words/sec, avg_loss 4.935064, time 0.7 sec\n","log: epoch 2, iter 2240, 20776.1 words/sec, avg_loss 4.948271, time 0.7 sec\n","log: epoch 2, iter 2250, 19529.3 words/sec, avg_loss 5.750568, time 0.7 sec\n","dev: epoch 2, iter 2250, 20190.3 words/sec, avg_loss 5.166886, time 17.7 sec\n","dev: epoch 2, iter 2250, dev_loss 1.796921, patience 0, decay_num 0\n","log: epoch 2, iter 2260, 19722.1 words/sec, avg_loss 4.732266, time 0.7 sec\n","log: epoch 2, iter 2270, 19645.3 words/sec, avg_loss 4.910215, time 0.7 sec\n","log: epoch 2, iter 2280, 20095.5 words/sec, avg_loss 4.344231, time 0.7 sec\n","log: epoch 2, iter 2290, 20402.8 words/sec, avg_loss 5.525307, time 0.7 sec\n","log: epoch 2, iter 2300, 20472.0 words/sec, avg_loss 5.057817, time 0.7 sec\n","log: epoch 2, iter 2310, 20805.1 words/sec, avg_loss 5.078846, time 0.7 sec\n","log: epoch 2, iter 2320, 18794.7 words/sec, avg_loss 4.731481, time 0.7 sec\n","log: epoch 2, iter 2330, 20457.3 words/sec, avg_loss 4.859536, time 0.7 sec\n","log: epoch 2, iter 2340, 20249.4 words/sec, avg_loss 4.515856, time 0.7 sec\n","log: epoch 2, iter 2350, 19837.5 words/sec, avg_loss 4.915859, time 0.7 sec\n","log: epoch 2, iter 2360, 20118.8 words/sec, avg_loss 4.927415, time 0.7 sec\n","log: epoch 2, iter 2370, 20446.8 words/sec, avg_loss 4.967893, time 0.7 sec\n","log: epoch 2, iter 2380, 20173.6 words/sec, avg_loss 5.583923, time 0.7 sec\n","log: epoch 2, iter 2390, 19602.9 words/sec, avg_loss 4.595449, time 0.7 sec\n","log: epoch 2, iter 2400, 21617.0 words/sec, avg_loss 4.996153, time 0.7 sec\n","log: epoch 2, iter 2410, 20787.4 words/sec, avg_loss 4.827682, time 0.7 sec\n","log: epoch 2, iter 2420, 20429.2 words/sec, avg_loss 5.406529, time 0.7 sec\n","log: epoch 2, iter 2430, 20241.2 words/sec, avg_loss 4.813160, time 0.7 sec\n","log: epoch 2, iter 2440, 19780.2 words/sec, avg_loss 4.430437, time 0.7 sec\n","log: epoch 2, iter 2450, 20355.1 words/sec, avg_loss 4.694899, time 0.7 sec\n","log: epoch 2, iter 2460, 19645.3 words/sec, avg_loss 4.862263, time 0.7 sec\n","log: epoch 2, iter 2470, 19233.1 words/sec, avg_loss 4.796836, time 0.7 sec\n","log: epoch 2, iter 2480, 20715.8 words/sec, avg_loss 4.603380, time 0.7 sec\n","log: epoch 2, iter 2490, 18735.4 words/sec, avg_loss 4.243123, time 0.7 sec\n","log: epoch 2, iter 2500, 19359.5 words/sec, avg_loss 4.772588, time 0.7 sec\n","dev: epoch 2, iter 2500, 20054.2 words/sec, avg_loss 4.847726, time 17.9 sec\n","dev: epoch 2, iter 2500, dev_loss 1.816193, patience 1, decay_num 0\n","log: epoch 2, iter 2510, 19565.4 words/sec, avg_loss 4.432012, time 0.7 sec\n","log: epoch 2, iter 2520, 19753.8 words/sec, avg_loss 4.556746, time 0.7 sec\n","log: epoch 2, iter 2530, 20564.8 words/sec, avg_loss 4.235895, time 0.7 sec\n","log: epoch 3, iter 2540, 18808.1 words/sec, avg_loss 3.714678, time 0.7 sec\n","log: epoch 3, iter 2550, 20019.8 words/sec, avg_loss 3.897604, time 0.7 sec\n","log: epoch 3, iter 2560, 21621.3 words/sec, avg_loss 4.566743, time 0.7 sec\n","log: epoch 3, iter 2570, 21339.6 words/sec, avg_loss 4.466311, time 0.7 sec\n","log: epoch 3, iter 2580, 19036.6 words/sec, avg_loss 3.836189, time 0.7 sec\n","log: epoch 3, iter 2590, 20281.1 words/sec, avg_loss 4.488805, time 0.7 sec\n","log: epoch 3, iter 2600, 20752.1 words/sec, avg_loss 4.221141, time 0.7 sec\n","log: epoch 3, iter 2610, 19337.8 words/sec, avg_loss 4.337092, time 0.7 sec\n","log: epoch 3, iter 2620, 19830.2 words/sec, avg_loss 4.471351, time 0.7 sec\n","log: epoch 3, iter 2630, 18781.3 words/sec, avg_loss 4.154367, time 0.7 sec\n","log: epoch 3, iter 2640, 18984.1 words/sec, avg_loss 4.497757, time 0.8 sec\n","log: epoch 3, iter 2650, 18710.2 words/sec, avg_loss 4.626210, time 0.7 sec\n","log: epoch 3, iter 2660, 20235.5 words/sec, avg_loss 5.078863, time 0.7 sec\n","log: epoch 3, iter 2670, 21232.9 words/sec, avg_loss 4.389082, time 0.7 sec\n","log: epoch 3, iter 2680, 20944.8 words/sec, avg_loss 4.326689, time 0.7 sec\n","log: epoch 3, iter 2690, 20704.4 words/sec, avg_loss 4.208260, time 0.7 sec\n","log: epoch 3, iter 2700, 20739.2 words/sec, avg_loss 4.336653, time 0.7 sec\n","log: epoch 3, iter 2710, 19660.1 words/sec, avg_loss 4.889684, time 0.7 sec\n","log: epoch 3, iter 2720, 20410.1 words/sec, avg_loss 4.520082, time 0.7 sec\n","log: epoch 3, iter 2730, 20250.3 words/sec, avg_loss 3.950881, time 0.7 sec\n","log: epoch 3, iter 2740, 19940.9 words/sec, avg_loss 4.013268, time 0.8 sec\n","log: epoch 3, iter 2750, 20930.3 words/sec, avg_loss 4.132675, time 0.7 sec\n","dev: epoch 3, iter 2750, 20072.9 words/sec, avg_loss 4.335358, time 17.9 sec\n","dev: epoch 3, iter 2750, dev_loss 1.758921, patience 0, decay_num 0\n","log: epoch 3, iter 2760, 19722.4 words/sec, avg_loss 4.599326, time 0.7 sec\n","log: epoch 3, iter 2770, 19526.9 words/sec, avg_loss 3.948958, time 0.7 sec\n","log: epoch 3, iter 2780, 19035.2 words/sec, avg_loss 4.228029, time 0.8 sec\n","log: epoch 3, iter 2790, 20250.5 words/sec, avg_loss 4.657287, time 0.7 sec\n","log: epoch 3, iter 2800, 20331.2 words/sec, avg_loss 4.489657, time 0.7 sec\n","log: epoch 3, iter 2810, 18581.6 words/sec, avg_loss 4.495530, time 0.8 sec\n","log: epoch 3, iter 2820, 20667.9 words/sec, avg_loss 4.142926, time 0.7 sec\n","log: epoch 3, iter 2830, 20722.6 words/sec, avg_loss 4.057485, time 0.7 sec\n","log: epoch 3, iter 2840, 19985.1 words/sec, avg_loss 4.724829, time 0.7 sec\n","log: epoch 3, iter 2850, 21574.0 words/sec, avg_loss 4.114247, time 0.7 sec\n","log: epoch 3, iter 2860, 19976.9 words/sec, avg_loss 3.692626, time 0.7 sec\n","log: epoch 3, iter 2870, 21512.8 words/sec, avg_loss 4.259736, time 0.7 sec\n","log: epoch 3, iter 2880, 19129.2 words/sec, avg_loss 3.885069, time 0.7 sec\n","log: epoch 3, iter 2890, 18955.1 words/sec, avg_loss 3.937714, time 0.7 sec\n","log: epoch 3, iter 2900, 19648.6 words/sec, avg_loss 4.653129, time 0.7 sec\n","log: epoch 3, iter 2910, 19419.7 words/sec, avg_loss 4.060153, time 0.7 sec\n","log: epoch 3, iter 2920, 19990.2 words/sec, avg_loss 4.104442, time 0.7 sec\n","log: epoch 3, iter 2930, 19886.8 words/sec, avg_loss 4.279381, time 0.7 sec\n","log: epoch 3, iter 2940, 19898.5 words/sec, avg_loss 4.146613, time 0.7 sec\n","log: epoch 3, iter 2950, 19329.8 words/sec, avg_loss 3.835746, time 0.7 sec\n","log: epoch 3, iter 2960, 20049.3 words/sec, avg_loss 3.719880, time 0.7 sec\n","log: epoch 3, iter 2970, 20406.5 words/sec, avg_loss 3.946587, time 0.7 sec\n","log: epoch 3, iter 2980, 20345.7 words/sec, avg_loss 4.305339, time 0.7 sec\n","log: epoch 3, iter 2990, 20330.7 words/sec, avg_loss 3.667662, time 0.7 sec\n","log: epoch 3, iter 3000, 20029.9 words/sec, avg_loss 4.466114, time 0.7 sec\n","dev: epoch 3, iter 3000, 19956.1 words/sec, avg_loss 4.176739, time 18.1 sec\n","dev: epoch 3, iter 3000, dev_loss 1.573486, patience 0, decay_num 0\n","log: epoch 3, iter 3010, 21018.6 words/sec, avg_loss 4.495078, time 0.7 sec\n","log: epoch 3, iter 3020, 20019.1 words/sec, avg_loss 4.207198, time 0.7 sec\n","log: epoch 3, iter 3030, 20828.7 words/sec, avg_loss 4.324063, time 0.7 sec\n","log: epoch 3, iter 3040, 19377.7 words/sec, avg_loss 4.164286, time 0.8 sec\n","log: epoch 3, iter 3050, 20237.8 words/sec, avg_loss 4.110376, time 0.7 sec\n","log: epoch 3, iter 3060, 19784.2 words/sec, avg_loss 4.070425, time 0.7 sec\n","log: epoch 3, iter 3070, 20029.3 words/sec, avg_loss 3.770499, time 0.7 sec\n","log: epoch 3, iter 3080, 21001.1 words/sec, avg_loss 4.429950, time 0.7 sec\n","log: epoch 3, iter 3090, 21091.7 words/sec, avg_loss 4.290070, time 0.7 sec\n","log: epoch 3, iter 3100, 18976.2 words/sec, avg_loss 3.797807, time 0.7 sec\n","log: epoch 3, iter 3110, 20517.2 words/sec, avg_loss 3.670080, time 0.7 sec\n","log: epoch 3, iter 3120, 20788.7 words/sec, avg_loss 3.932280, time 0.7 sec\n","log: epoch 3, iter 3130, 20396.0 words/sec, avg_loss 4.092155, time 0.7 sec\n","log: epoch 3, iter 3140, 20155.0 words/sec, avg_loss 3.800668, time 0.7 sec\n","log: epoch 3, iter 3150, 20519.0 words/sec, avg_loss 3.662834, time 0.7 sec\n","log: epoch 3, iter 3160, 19748.4 words/sec, avg_loss 3.638010, time 0.7 sec\n","log: epoch 3, iter 3170, 19727.5 words/sec, avg_loss 3.443795, time 0.7 sec\n","log: epoch 3, iter 3180, 20499.7 words/sec, avg_loss 4.231720, time 0.7 sec\n","log: epoch 3, iter 3190, 18582.3 words/sec, avg_loss 4.122024, time 0.8 sec\n","log: epoch 3, iter 3200, 20411.1 words/sec, avg_loss 3.621628, time 0.7 sec\n","log: epoch 3, iter 3210, 20165.7 words/sec, avg_loss 4.349566, time 0.7 sec\n","log: epoch 3, iter 3220, 20809.3 words/sec, avg_loss 4.095257, time 0.7 sec\n","log: epoch 3, iter 3230, 21321.9 words/sec, avg_loss 4.272325, time 0.7 sec\n","log: epoch 3, iter 3240, 18712.7 words/sec, avg_loss 3.950283, time 0.8 sec\n","log: epoch 3, iter 3250, 19915.3 words/sec, avg_loss 4.080406, time 0.7 sec\n","dev: epoch 3, iter 3250, 20161.6 words/sec, avg_loss 4.024911, time 17.8 sec\n","dev: epoch 3, iter 3250, dev_loss 1.581563, patience 1, decay_num 0\n","log: epoch 3, iter 3260, 18953.0 words/sec, avg_loss 4.544607, time 0.8 sec\n","log: epoch 3, iter 3270, 20105.9 words/sec, avg_loss 4.261506, time 0.7 sec\n","log: epoch 3, iter 3280, 20443.7 words/sec, avg_loss 3.718698, time 0.7 sec\n","log: epoch 3, iter 3290, 21382.6 words/sec, avg_loss 3.286989, time 0.7 sec\n","log: epoch 3, iter 3300, 20133.6 words/sec, avg_loss 3.765682, time 0.8 sec\n","log: epoch 3, iter 3310, 19792.9 words/sec, avg_loss 4.265685, time 0.7 sec\n","log: epoch 3, iter 3320, 18617.2 words/sec, avg_loss 3.476934, time 0.8 sec\n","log: epoch 3, iter 3330, 18740.1 words/sec, avg_loss 3.758971, time 0.7 sec\n","log: epoch 3, iter 3340, 19807.8 words/sec, avg_loss 3.938411, time 0.7 sec\n","log: epoch 3, iter 3350, 21350.7 words/sec, avg_loss 3.918290, time 0.7 sec\n","log: epoch 3, iter 3360, 20547.8 words/sec, avg_loss 3.558524, time 0.7 sec\n","log: epoch 3, iter 3370, 21080.3 words/sec, avg_loss 4.340475, time 0.7 sec\n","log: epoch 3, iter 3380, 20461.8 words/sec, avg_loss 3.853717, time 0.7 sec\n","log: epoch 3, iter 3390, 20831.4 words/sec, avg_loss 3.946996, time 0.7 sec\n","log: epoch 3, iter 3400, 20624.9 words/sec, avg_loss 3.799109, time 0.7 sec\n","log: epoch 3, iter 3410, 21118.0 words/sec, avg_loss 3.663177, time 0.7 sec\n","log: epoch 3, iter 3420, 20958.3 words/sec, avg_loss 3.780498, time 0.7 sec\n","log: epoch 3, iter 3430, 21159.5 words/sec, avg_loss 4.004072, time 0.6 sec\n","log: epoch 3, iter 3440, 19866.6 words/sec, avg_loss 4.208677, time 0.7 sec\n","log: epoch 3, iter 3450, 18901.9 words/sec, avg_loss 3.905515, time 0.7 sec\n","log: epoch 3, iter 3460, 20492.5 words/sec, avg_loss 3.319630, time 0.7 sec\n","log: epoch 3, iter 3470, 19973.6 words/sec, avg_loss 3.475796, time 0.7 sec\n","log: epoch 3, iter 3480, 18943.1 words/sec, avg_loss 3.569726, time 0.7 sec\n","log: epoch 3, iter 3490, 20435.5 words/sec, avg_loss 3.958540, time 0.7 sec\n","log: epoch 3, iter 3500, 19255.3 words/sec, avg_loss 4.065461, time 0.8 sec\n","dev: epoch 3, iter 3500, 20131.6 words/sec, avg_loss 3.855427, time 18.0 sec\n","dev: epoch 3, iter 3500, dev_loss 1.454018, patience 0, decay_num 0\n","log: epoch 3, iter 3510, 19202.6 words/sec, avg_loss 3.974086, time 0.7 sec\n","log: epoch 3, iter 3520, 20550.9 words/sec, avg_loss 3.577983, time 0.7 sec\n","log: epoch 3, iter 3530, 19069.2 words/sec, avg_loss 3.737131, time 0.7 sec\n","log: epoch 3, iter 3540, 20278.3 words/sec, avg_loss 3.574504, time 0.7 sec\n","log: epoch 3, iter 3550, 20650.2 words/sec, avg_loss 3.515033, time 0.7 sec\n","log: epoch 3, iter 3560, 20040.5 words/sec, avg_loss 3.623439, time 0.7 sec\n","log: epoch 3, iter 3570, 19263.2 words/sec, avg_loss 3.345479, time 0.7 sec\n","log: epoch 3, iter 3580, 20955.7 words/sec, avg_loss 3.562396, time 0.7 sec\n","log: epoch 3, iter 3590, 19601.1 words/sec, avg_loss 3.562491, time 0.7 sec\n","log: epoch 3, iter 3600, 19283.4 words/sec, avg_loss 3.780690, time 0.8 sec\n","log: epoch 3, iter 3610, 20954.6 words/sec, avg_loss 3.876753, time 0.7 sec\n","log: epoch 3, iter 3620, 20281.3 words/sec, avg_loss 3.166732, time 0.7 sec\n","log: epoch 3, iter 3630, 20727.4 words/sec, avg_loss 3.841231, time 0.7 sec\n","log: epoch 3, iter 3640, 19656.6 words/sec, avg_loss 3.733113, time 0.7 sec\n","log: epoch 3, iter 3650, 20359.1 words/sec, avg_loss 3.660357, time 0.7 sec\n","log: epoch 3, iter 3660, 20938.1 words/sec, avg_loss 3.335691, time 0.7 sec\n","log: epoch 3, iter 3670, 20744.8 words/sec, avg_loss 3.883873, time 0.7 sec\n","log: epoch 3, iter 3680, 20288.4 words/sec, avg_loss 3.376209, time 0.7 sec\n","log: epoch 3, iter 3690, 19459.5 words/sec, avg_loss 3.553385, time 0.7 sec\n","log: epoch 3, iter 3700, 20740.8 words/sec, avg_loss 3.559444, time 0.7 sec\n","log: epoch 3, iter 3710, 20427.7 words/sec, avg_loss 3.636211, time 0.7 sec\n","log: epoch 3, iter 3720, 21191.2 words/sec, avg_loss 3.777044, time 0.7 sec\n","log: epoch 3, iter 3730, 20334.4 words/sec, avg_loss 3.479322, time 0.7 sec\n","log: epoch 3, iter 3740, 20547.2 words/sec, avg_loss 3.457106, time 0.7 sec\n","log: epoch 3, iter 3750, 20300.9 words/sec, avg_loss 3.283769, time 0.7 sec\n","dev: epoch 3, iter 3750, 20222.7 words/sec, avg_loss 3.594939, time 17.7 sec\n","dev: epoch 3, iter 3750, dev_loss 1.452037, patience 1, decay_num 0\n","log: epoch 3, iter 3760, 19838.8 words/sec, avg_loss 3.721072, time 0.7 sec\n","log: epoch 3, iter 3770, 19113.7 words/sec, avg_loss 3.491405, time 0.7 sec\n","log: epoch 3, iter 3780, 20214.4 words/sec, avg_loss 3.336484, time 0.7 sec\n","log: epoch 3, iter 3790, 21189.2 words/sec, avg_loss 3.240278, time 0.6 sec\n","log: epoch 3, iter 3800, 20436.6 words/sec, avg_loss 3.764195, time 0.7 sec\n","log: epoch 4, iter 3810, 19814.5 words/sec, avg_loss 3.715993, time 0.7 sec\n","log: epoch 4, iter 3820, 20656.0 words/sec, avg_loss 3.314088, time 0.7 sec\n","log: epoch 4, iter 3830, 20644.3 words/sec, avg_loss 3.415942, time 0.7 sec\n","log: epoch 4, iter 3840, 19764.3 words/sec, avg_loss 3.456203, time 0.7 sec\n","log: epoch 4, iter 3850, 20047.3 words/sec, avg_loss 3.343932, time 0.7 sec\n","log: epoch 4, iter 3860, 19632.1 words/sec, avg_loss 3.223375, time 0.7 sec\n","log: epoch 4, iter 3870, 20269.2 words/sec, avg_loss 3.022687, time 0.7 sec\n","log: epoch 4, iter 3880, 21053.2 words/sec, avg_loss 3.296041, time 0.7 sec\n","log: epoch 4, iter 3890, 20119.9 words/sec, avg_loss 2.897060, time 0.7 sec\n","log: epoch 4, iter 3900, 20126.4 words/sec, avg_loss 3.114374, time 0.7 sec\n","log: epoch 4, iter 3910, 21100.3 words/sec, avg_loss 2.915685, time 0.7 sec\n","log: epoch 4, iter 3920, 21330.1 words/sec, avg_loss 3.374341, time 0.7 sec\n","log: epoch 4, iter 3930, 19511.8 words/sec, avg_loss 3.224090, time 0.7 sec\n","log: epoch 4, iter 3940, 21157.6 words/sec, avg_loss 3.500923, time 0.7 sec\n","log: epoch 4, iter 3950, 18617.5 words/sec, avg_loss 3.048391, time 0.8 sec\n","log: epoch 4, iter 3960, 18751.1 words/sec, avg_loss 3.385357, time 0.8 sec\n","log: epoch 4, iter 3970, 19073.6 words/sec, avg_loss 3.361868, time 0.8 sec\n","log: epoch 4, iter 3980, 20260.3 words/sec, avg_loss 3.857373, time 0.7 sec\n","log: epoch 4, iter 3990, 20045.2 words/sec, avg_loss 3.585035, time 0.7 sec\n","log: epoch 4, iter 4000, 19900.4 words/sec, avg_loss 3.333878, time 0.7 sec\n","dev: epoch 4, iter 4000, 20080.3 words/sec, avg_loss 3.356795, time 17.8 sec\n","dev: epoch 4, iter 4000, dev_loss 1.416095, patience 0, decay_num 0\n","log: epoch 4, iter 4010, 18726.5 words/sec, avg_loss 2.696235, time 0.8 sec\n","log: epoch 4, iter 4020, 20210.3 words/sec, avg_loss 3.366345, time 0.7 sec\n","log: epoch 4, iter 4030, 19434.2 words/sec, avg_loss 3.625638, time 0.7 sec\n","log: epoch 4, iter 4040, 20232.2 words/sec, avg_loss 3.384578, time 0.7 sec\n","log: epoch 4, iter 4050, 21056.7 words/sec, avg_loss 3.218305, time 0.7 sec\n","log: epoch 4, iter 4060, 20923.6 words/sec, avg_loss 3.316334, time 0.7 sec\n","log: epoch 4, iter 4070, 20493.5 words/sec, avg_loss 3.214032, time 0.7 sec\n","log: epoch 4, iter 4080, 20730.5 words/sec, avg_loss 3.066847, time 0.7 sec\n","log: epoch 4, iter 4090, 19232.0 words/sec, avg_loss 2.755851, time 0.7 sec\n","log: epoch 4, iter 4100, 21348.0 words/sec, avg_loss 3.635201, time 0.7 sec\n","log: epoch 4, iter 4110, 19017.9 words/sec, avg_loss 3.374544, time 0.8 sec\n","log: epoch 4, iter 4120, 18967.3 words/sec, avg_loss 3.212686, time 0.8 sec\n","log: epoch 4, iter 4130, 19082.2 words/sec, avg_loss 3.278453, time 0.8 sec\n","log: epoch 4, iter 4140, 18726.6 words/sec, avg_loss 2.873465, time 0.8 sec\n","log: epoch 4, iter 4150, 20483.8 words/sec, avg_loss 3.457516, time 0.7 sec\n","log: epoch 4, iter 4160, 19656.8 words/sec, avg_loss 2.717106, time 0.7 sec\n","log: epoch 4, iter 4170, 20391.8 words/sec, avg_loss 3.184083, time 0.7 sec\n","log: epoch 4, iter 4180, 19899.7 words/sec, avg_loss 2.955504, time 0.7 sec\n","log: epoch 4, iter 4190, 20542.0 words/sec, avg_loss 3.226033, time 0.7 sec\n","log: epoch 4, iter 4200, 20454.0 words/sec, avg_loss 3.077725, time 0.7 sec\n","log: epoch 4, iter 4210, 20169.6 words/sec, avg_loss 2.961887, time 0.7 sec\n","log: epoch 4, iter 4220, 19166.4 words/sec, avg_loss 3.252096, time 0.7 sec\n","log: epoch 4, iter 4230, 20245.8 words/sec, avg_loss 3.730214, time 0.7 sec\n","log: epoch 4, iter 4240, 19848.9 words/sec, avg_loss 3.151483, time 0.7 sec\n","log: epoch 4, iter 4250, 19902.8 words/sec, avg_loss 3.390587, time 0.7 sec\n","dev: epoch 4, iter 4250, 19936.9 words/sec, avg_loss 3.204910, time 18.1 sec\n","dev: epoch 4, iter 4250, dev_loss 1.471620, patience 1, decay_num 0\n","log: epoch 4, iter 4260, 19125.6 words/sec, avg_loss 3.109527, time 0.8 sec\n","log: epoch 4, iter 4270, 21478.3 words/sec, avg_loss 3.341150, time 0.7 sec\n","log: epoch 4, iter 4280, 20203.8 words/sec, avg_loss 3.422535, time 0.7 sec\n","log: epoch 4, iter 4290, 19818.9 words/sec, avg_loss 3.137101, time 0.7 sec\n","log: epoch 4, iter 4300, 20184.8 words/sec, avg_loss 2.709229, time 0.7 sec\n","log: epoch 4, iter 4310, 19541.5 words/sec, avg_loss 3.050393, time 0.7 sec\n","log: epoch 4, iter 4320, 20207.9 words/sec, avg_loss 3.128979, time 0.7 sec\n","log: epoch 4, iter 4330, 20611.5 words/sec, avg_loss 3.123373, time 0.7 sec\n","log: epoch 4, iter 4340, 19578.6 words/sec, avg_loss 3.486175, time 0.7 sec\n","log: epoch 4, iter 4350, 20353.3 words/sec, avg_loss 3.126187, time 0.7 sec\n","log: epoch 4, iter 4360, 20330.7 words/sec, avg_loss 3.081216, time 0.7 sec\n","log: epoch 4, iter 4370, 20508.8 words/sec, avg_loss 3.109072, time 0.7 sec\n","log: epoch 4, iter 4380, 19858.5 words/sec, avg_loss 3.019512, time 0.7 sec\n","log: epoch 4, iter 4390, 20219.4 words/sec, avg_loss 3.171528, time 0.7 sec\n","log: epoch 4, iter 4400, 20291.7 words/sec, avg_loss 2.820607, time 0.7 sec\n","log: epoch 4, iter 4410, 19314.5 words/sec, avg_loss 3.093369, time 0.7 sec\n","log: epoch 4, iter 4420, 20035.9 words/sec, avg_loss 3.201538, time 0.7 sec\n","log: epoch 4, iter 4430, 19944.0 words/sec, avg_loss 3.262632, time 0.7 sec\n","log: epoch 4, iter 4440, 19356.4 words/sec, avg_loss 2.954996, time 0.7 sec\n","log: epoch 4, iter 4450, 21149.1 words/sec, avg_loss 2.992295, time 0.7 sec\n","log: epoch 4, iter 4460, 20876.6 words/sec, avg_loss 3.200026, time 0.7 sec\n","log: epoch 4, iter 4470, 20385.0 words/sec, avg_loss 3.325584, time 0.7 sec\n","log: epoch 4, iter 4480, 19468.6 words/sec, avg_loss 3.139944, time 0.7 sec\n","log: epoch 4, iter 4490, 19093.9 words/sec, avg_loss 3.186206, time 0.7 sec\n","log: epoch 4, iter 4500, 20023.3 words/sec, avg_loss 3.216838, time 0.7 sec\n","dev: epoch 4, iter 4500, 20063.2 words/sec, avg_loss 3.136401, time 17.9 sec\n","dev: epoch 4, iter 4500, dev_loss 1.437692, patience 2, decay_num 0\n","log: epoch 4, iter 4510, 19191.6 words/sec, avg_loss 3.140492, time 0.7 sec\n","log: epoch 4, iter 4520, 20292.4 words/sec, avg_loss 3.475106, time 0.7 sec\n","log: epoch 4, iter 4530, 20531.9 words/sec, avg_loss 2.953553, time 0.7 sec\n","log: epoch 4, iter 4540, 21114.6 words/sec, avg_loss 3.387130, time 0.7 sec\n","log: epoch 4, iter 4550, 19413.7 words/sec, avg_loss 3.432964, time 0.8 sec\n","log: epoch 4, iter 4560, 20575.5 words/sec, avg_loss 3.267576, time 0.7 sec\n","log: epoch 4, iter 4570, 19266.6 words/sec, avg_loss 3.240020, time 0.7 sec\n","log: epoch 4, iter 4580, 20323.2 words/sec, avg_loss 3.339364, time 0.7 sec\n","log: epoch 4, iter 4590, 20535.7 words/sec, avg_loss 2.828638, time 0.7 sec\n","log: epoch 4, iter 4600, 20893.8 words/sec, avg_loss 3.076315, time 0.7 sec\n","log: epoch 4, iter 4610, 19205.0 words/sec, avg_loss 3.019916, time 0.7 sec\n","log: epoch 4, iter 4620, 21601.4 words/sec, avg_loss 3.171891, time 0.7 sec\n","log: epoch 4, iter 4630, 21292.9 words/sec, avg_loss 3.545641, time 0.7 sec\n","log: epoch 4, iter 4640, 20586.3 words/sec, avg_loss 2.995139, time 0.7 sec\n","log: epoch 4, iter 4650, 19715.7 words/sec, avg_loss 3.307165, time 0.7 sec\n","log: epoch 4, iter 4660, 19613.5 words/sec, avg_loss 3.201237, time 0.8 sec\n","log: epoch 4, iter 4670, 18935.2 words/sec, avg_loss 2.973400, time 0.8 sec\n","log: epoch 4, iter 4680, 18298.1 words/sec, avg_loss 3.089221, time 0.8 sec\n","log: epoch 4, iter 4690, 19272.3 words/sec, avg_loss 3.223997, time 0.8 sec\n","log: epoch 4, iter 4700, 20823.6 words/sec, avg_loss 3.373580, time 0.7 sec\n","log: epoch 4, iter 4710, 19076.5 words/sec, avg_loss 2.879000, time 0.7 sec\n","log: epoch 4, iter 4720, 19651.3 words/sec, avg_loss 2.741473, time 0.7 sec\n","log: epoch 4, iter 4730, 20053.4 words/sec, avg_loss 3.081634, time 0.7 sec\n","log: epoch 4, iter 4740, 19711.5 words/sec, avg_loss 3.082421, time 0.7 sec\n","log: epoch 4, iter 4750, 19935.7 words/sec, avg_loss 2.766149, time 0.7 sec\n","dev: epoch 4, iter 4750, 19969.1 words/sec, avg_loss 3.143721, time 17.9 sec\n","dev: epoch 4, iter 4750, dev_loss 1.386763, patience 0, decay_num 0\n","log: epoch 4, iter 4760, 20069.6 words/sec, avg_loss 3.218644, time 0.7 sec\n","log: epoch 4, iter 4770, 20073.5 words/sec, avg_loss 2.926763, time 0.7 sec\n","log: epoch 4, iter 4780, 20457.5 words/sec, avg_loss 3.410209, time 0.7 sec\n","log: epoch 4, iter 4790, 19902.0 words/sec, avg_loss 2.947912, time 0.7 sec\n","log: epoch 4, iter 4800, 20782.4 words/sec, avg_loss 2.719023, time 0.7 sec\n","log: epoch 4, iter 4810, 20338.1 words/sec, avg_loss 3.248333, time 0.7 sec\n","log: epoch 4, iter 4820, 20869.2 words/sec, avg_loss 3.129322, time 0.7 sec\n","log: epoch 4, iter 4830, 18628.4 words/sec, avg_loss 3.115107, time 0.8 sec\n","log: epoch 4, iter 4840, 20719.2 words/sec, avg_loss 3.110253, time 0.7 sec\n","log: epoch 4, iter 4850, 21046.7 words/sec, avg_loss 3.340954, time 0.7 sec\n","log: epoch 4, iter 4860, 20502.8 words/sec, avg_loss 3.373420, time 0.7 sec\n","log: epoch 4, iter 4870, 21366.9 words/sec, avg_loss 2.883410, time 0.7 sec\n","log: epoch 4, iter 4880, 19074.4 words/sec, avg_loss 3.311453, time 0.7 sec\n","log: epoch 4, iter 4890, 21890.9 words/sec, avg_loss 3.139542, time 0.7 sec\n","log: epoch 4, iter 4900, 18679.4 words/sec, avg_loss 2.776442, time 0.7 sec\n","log: epoch 4, iter 4910, 20435.6 words/sec, avg_loss 3.119822, time 0.7 sec\n","log: epoch 4, iter 4920, 20168.0 words/sec, avg_loss 2.980176, time 0.7 sec\n","log: epoch 4, iter 4930, 19728.0 words/sec, avg_loss 3.115024, time 0.7 sec\n","log: epoch 4, iter 4940, 20787.1 words/sec, avg_loss 2.896053, time 0.7 sec\n","log: epoch 4, iter 4950, 19908.5 words/sec, avg_loss 2.820193, time 0.7 sec\n","log: epoch 4, iter 4960, 19443.2 words/sec, avg_loss 2.833501, time 0.7 sec\n","log: epoch 4, iter 4970, 20954.5 words/sec, avg_loss 3.376657, time 0.7 sec\n","log: epoch 4, iter 4980, 19398.8 words/sec, avg_loss 2.989416, time 0.7 sec\n","log: epoch 4, iter 4990, 20677.0 words/sec, avg_loss 3.558381, time 0.7 sec\n","log: epoch 4, iter 5000, 20352.7 words/sec, avg_loss 2.991808, time 0.7 sec\n","dev: epoch 4, iter 5000, 20234.1 words/sec, avg_loss 3.093273, time 17.8 sec\n","dev: epoch 4, iter 5000, dev_loss 1.299136, patience 0, decay_num 0\n","log: epoch 4, iter 5010, 20122.7 words/sec, avg_loss 2.595498, time 0.7 sec\n","log: epoch 4, iter 5020, 19996.3 words/sec, avg_loss 2.852309, time 0.7 sec\n","log: epoch 4, iter 5030, 19782.3 words/sec, avg_loss 2.756608, time 0.7 sec\n","log: epoch 4, iter 5040, 20490.5 words/sec, avg_loss 3.324999, time 0.7 sec\n","log: epoch 4, iter 5050, 20846.1 words/sec, avg_loss 2.771729, time 0.7 sec\n","log: epoch 4, iter 5060, 21396.4 words/sec, avg_loss 3.222584, time 0.7 sec\n","log: epoch 5, iter 5070, 17318.2 words/sec, avg_loss 3.196049, time 0.8 sec\n","log: epoch 5, iter 5080, 19268.9 words/sec, avg_loss 2.453882, time 0.7 sec\n","log: epoch 5, iter 5090, 20129.2 words/sec, avg_loss 2.611493, time 0.7 sec\n","log: epoch 5, iter 5100, 20020.7 words/sec, avg_loss 2.513628, time 0.7 sec\n","log: epoch 5, iter 5110, 19003.0 words/sec, avg_loss 2.692680, time 0.7 sec\n","log: epoch 5, iter 5120, 20408.7 words/sec, avg_loss 2.839810, time 0.7 sec\n","log: epoch 5, iter 5130, 19193.4 words/sec, avg_loss 2.762527, time 0.8 sec\n","log: epoch 5, iter 5140, 20388.7 words/sec, avg_loss 2.807181, time 0.7 sec\n","log: epoch 5, iter 5150, 20997.1 words/sec, avg_loss 2.746881, time 0.7 sec\n","log: epoch 5, iter 5160, 19543.0 words/sec, avg_loss 2.503933, time 0.8 sec\n","log: epoch 5, iter 5170, 20393.6 words/sec, avg_loss 2.487966, time 0.7 sec\n","log: epoch 5, iter 5180, 19361.4 words/sec, avg_loss 2.339496, time 0.7 sec\n","log: epoch 5, iter 5190, 20058.3 words/sec, avg_loss 2.659579, time 0.7 sec\n","log: epoch 5, iter 5200, 20546.1 words/sec, avg_loss 2.645623, time 0.7 sec\n","log: epoch 5, iter 5210, 19690.6 words/sec, avg_loss 2.867603, time 0.7 sec\n","log: epoch 5, iter 5220, 19414.8 words/sec, avg_loss 3.325923, time 0.7 sec\n","log: epoch 5, iter 5230, 19938.3 words/sec, avg_loss 2.813422, time 0.7 sec\n","log: epoch 5, iter 5240, 19609.5 words/sec, avg_loss 3.176833, time 0.7 sec\n","log: epoch 5, iter 5250, 19453.3 words/sec, avg_loss 2.638823, time 0.7 sec\n","dev: epoch 5, iter 5250, 19868.4 words/sec, avg_loss 2.783354, time 18.0 sec\n","dev: epoch 5, iter 5250, dev_loss 1.387326, patience 1, decay_num 0\n","log: epoch 5, iter 5260, 20874.9 words/sec, avg_loss 2.739954, time 0.7 sec\n","log: epoch 5, iter 5270, 19437.7 words/sec, avg_loss 2.059282, time 0.7 sec\n","log: epoch 5, iter 5280, 18754.3 words/sec, avg_loss 2.678387, time 0.8 sec\n","log: epoch 5, iter 5290, 19681.4 words/sec, avg_loss 2.967429, time 0.7 sec\n","log: epoch 5, iter 5300, 20065.3 words/sec, avg_loss 2.511130, time 0.7 sec\n","log: epoch 5, iter 5310, 19917.0 words/sec, avg_loss 2.768200, time 0.7 sec\n","log: epoch 5, iter 5320, 19755.4 words/sec, avg_loss 2.573236, time 0.7 sec\n","log: epoch 5, iter 5330, 20370.7 words/sec, avg_loss 2.649969, time 0.7 sec\n","log: epoch 5, iter 5340, 19913.7 words/sec, avg_loss 2.963196, time 0.7 sec\n","log: epoch 5, iter 5350, 19150.6 words/sec, avg_loss 2.980778, time 0.7 sec\n","log: epoch 5, iter 5360, 20419.4 words/sec, avg_loss 2.486069, time 0.7 sec\n","log: epoch 5, iter 5370, 20712.6 words/sec, avg_loss 2.713922, time 0.7 sec\n","log: epoch 5, iter 5380, 18931.6 words/sec, avg_loss 2.889121, time 0.8 sec\n","log: epoch 5, iter 5390, 20315.1 words/sec, avg_loss 2.853380, time 0.7 sec\n","log: epoch 5, iter 5400, 20489.4 words/sec, avg_loss 2.732785, time 0.7 sec\n","log: epoch 5, iter 5410, 20257.9 words/sec, avg_loss 2.560674, time 0.7 sec\n","log: epoch 5, iter 5420, 20265.9 words/sec, avg_loss 2.669729, time 0.7 sec\n","log: epoch 5, iter 5430, 21972.1 words/sec, avg_loss 2.542569, time 0.7 sec\n","log: epoch 5, iter 5440, 19628.5 words/sec, avg_loss 2.859628, time 0.7 sec\n","log: epoch 5, iter 5450, 20627.5 words/sec, avg_loss 2.696444, time 0.7 sec\n","log: epoch 5, iter 5460, 19640.3 words/sec, avg_loss 2.693836, time 0.7 sec\n","log: epoch 5, iter 5470, 20280.8 words/sec, avg_loss 2.614472, time 0.7 sec\n","log: epoch 5, iter 5480, 19855.5 words/sec, avg_loss 2.626355, time 0.7 sec\n","log: epoch 5, iter 5490, 20145.2 words/sec, avg_loss 2.800174, time 0.7 sec\n","log: epoch 5, iter 5500, 20223.5 words/sec, avg_loss 2.569793, time 0.7 sec\n","dev: epoch 5, iter 5500, 20052.4 words/sec, avg_loss 2.688020, time 18.0 sec\n","dev: epoch 5, iter 5500, dev_loss 1.358157, patience 2, decay_num 0\n","log: epoch 5, iter 5510, 20512.8 words/sec, avg_loss 2.497375, time 0.7 sec\n","log: epoch 5, iter 5520, 21319.9 words/sec, avg_loss 2.782234, time 0.7 sec\n","log: epoch 5, iter 5530, 20389.2 words/sec, avg_loss 2.615257, time 0.7 sec\n","log: epoch 5, iter 5540, 21498.4 words/sec, avg_loss 2.226942, time 0.6 sec\n","log: epoch 5, iter 5550, 20132.6 words/sec, avg_loss 2.626824, time 0.7 sec\n","log: epoch 5, iter 5560, 19538.3 words/sec, avg_loss 2.718399, time 0.7 sec\n","log: epoch 5, iter 5570, 21352.6 words/sec, avg_loss 2.733328, time 0.7 sec\n","log: epoch 5, iter 5580, 21144.0 words/sec, avg_loss 2.506307, time 0.7 sec\n","log: epoch 5, iter 5590, 21137.4 words/sec, avg_loss 3.204068, time 0.7 sec\n","log: epoch 5, iter 5600, 20900.5 words/sec, avg_loss 2.698983, time 0.7 sec\n","log: epoch 5, iter 5610, 20378.4 words/sec, avg_loss 2.558460, time 0.7 sec\n","log: epoch 5, iter 5620, 20569.5 words/sec, avg_loss 2.877966, time 0.7 sec\n","log: epoch 5, iter 5630, 20760.8 words/sec, avg_loss 2.531819, time 0.7 sec\n","log: epoch 5, iter 5640, 20922.0 words/sec, avg_loss 2.725882, time 0.7 sec\n","log: epoch 5, iter 5650, 21243.6 words/sec, avg_loss 2.594313, time 0.7 sec\n","log: epoch 5, iter 5660, 20511.4 words/sec, avg_loss 2.683591, time 0.7 sec\n","log: epoch 5, iter 5670, 21131.9 words/sec, avg_loss 2.831013, time 0.7 sec\n","log: epoch 5, iter 5680, 19740.1 words/sec, avg_loss 2.638106, time 0.7 sec\n","log: epoch 5, iter 5690, 20636.6 words/sec, avg_loss 2.546622, time 0.7 sec\n","log: epoch 5, iter 5700, 20316.9 words/sec, avg_loss 2.375105, time 0.7 sec\n","log: epoch 5, iter 5710, 19978.5 words/sec, avg_loss 2.518243, time 0.7 sec\n","log: epoch 5, iter 5720, 19753.8 words/sec, avg_loss 2.837538, time 0.7 sec\n","log: epoch 5, iter 5730, 20130.6 words/sec, avg_loss 2.753189, time 0.7 sec\n","log: epoch 5, iter 5740, 19125.7 words/sec, avg_loss 2.643467, time 0.7 sec\n","log: epoch 5, iter 5750, 19620.0 words/sec, avg_loss 2.422250, time 0.7 sec\n","dev: epoch 5, iter 5750, 20495.5 words/sec, avg_loss 2.645891, time 17.6 sec\n","dev: epoch 5, iter 5750, dev_loss 1.305856, patience 3, decay_num 0\n","log: epoch 5, iter 5760, 20793.6 words/sec, avg_loss 2.432934, time 0.7 sec\n","log: epoch 5, iter 5770, 19824.0 words/sec, avg_loss 2.761722, time 0.7 sec\n","log: epoch 5, iter 5780, 19510.2 words/sec, avg_loss 2.904206, time 0.7 sec\n","log: epoch 5, iter 5790, 20776.0 words/sec, avg_loss 3.098634, time 0.7 sec\n","log: epoch 5, iter 5800, 19214.8 words/sec, avg_loss 2.670724, time 0.7 sec\n","log: epoch 5, iter 5810, 19392.5 words/sec, avg_loss 2.650769, time 0.7 sec\n","log: epoch 5, iter 5820, 20511.7 words/sec, avg_loss 2.663450, time 0.7 sec\n","log: epoch 5, iter 5830, 20406.2 words/sec, avg_loss 2.779681, time 0.7 sec\n","log: epoch 5, iter 5840, 19501.8 words/sec, avg_loss 2.421220, time 0.7 sec\n","log: epoch 5, iter 5850, 19874.2 words/sec, avg_loss 2.860126, time 0.8 sec\n","log: epoch 5, iter 5860, 20079.5 words/sec, avg_loss 2.690476, time 0.7 sec\n","log: epoch 5, iter 5870, 19874.9 words/sec, avg_loss 2.640876, time 0.7 sec\n","log: epoch 5, iter 5880, 20731.4 words/sec, avg_loss 2.770579, time 0.7 sec\n","log: epoch 5, iter 5890, 19344.5 words/sec, avg_loss 2.605491, time 0.7 sec\n","log: epoch 5, iter 5900, 20721.6 words/sec, avg_loss 2.647287, time 0.7 sec\n","log: epoch 5, iter 5910, 21072.3 words/sec, avg_loss 2.708055, time 0.7 sec\n","log: epoch 5, iter 5920, 19815.6 words/sec, avg_loss 3.179428, time 0.7 sec\n","log: epoch 5, iter 5930, 20407.3 words/sec, avg_loss 2.504451, time 0.7 sec\n","log: epoch 5, iter 5940, 21692.5 words/sec, avg_loss 3.054731, time 0.7 sec\n","log: epoch 5, iter 5950, 19976.8 words/sec, avg_loss 2.691363, time 0.7 sec\n","log: epoch 5, iter 5960, 19935.8 words/sec, avg_loss 2.950743, time 0.7 sec\n","log: epoch 5, iter 5970, 19785.3 words/sec, avg_loss 2.530264, time 0.8 sec\n","log: epoch 5, iter 5980, 20334.7 words/sec, avg_loss 2.627839, time 0.7 sec\n","log: epoch 5, iter 5990, 20029.4 words/sec, avg_loss 2.740343, time 0.7 sec\n","log: epoch 5, iter 6000, 19917.4 words/sec, avg_loss 2.832259, time 0.7 sec\n","dev: epoch 5, iter 6000, 20130.7 words/sec, avg_loss 2.736706, time 17.9 sec\n","dev: epoch 5, iter 6000, dev_loss 1.286551, patience 0, decay_num 1\n","log: epoch 5, iter 6010, 19634.5 words/sec, avg_loss 2.601604, time 0.7 sec\n","log: epoch 5, iter 6020, 19846.1 words/sec, avg_loss 2.619425, time 0.7 sec\n","log: epoch 5, iter 6030, 19882.0 words/sec, avg_loss 2.498567, time 0.7 sec\n","log: epoch 5, iter 6040, 20143.1 words/sec, avg_loss 2.822743, time 0.7 sec\n","log: epoch 5, iter 6050, 21052.8 words/sec, avg_loss 2.875830, time 0.7 sec\n","log: epoch 5, iter 6060, 19745.3 words/sec, avg_loss 2.465538, time 0.7 sec\n","log: epoch 5, iter 6070, 18083.0 words/sec, avg_loss 2.818783, time 0.8 sec\n","log: epoch 5, iter 6080, 19685.0 words/sec, avg_loss 2.784209, time 0.7 sec\n","log: epoch 5, iter 6090, 19593.6 words/sec, avg_loss 2.686517, time 0.7 sec\n","log: epoch 5, iter 6100, 20911.7 words/sec, avg_loss 2.959823, time 0.7 sec\n","log: epoch 5, iter 6110, 19268.9 words/sec, avg_loss 2.601601, time 0.7 sec\n","log: epoch 5, iter 6120, 20338.0 words/sec, avg_loss 2.518548, time 0.7 sec\n","log: epoch 5, iter 6130, 20490.0 words/sec, avg_loss 2.671064, time 0.7 sec\n","log: epoch 5, iter 6140, 18972.3 words/sec, avg_loss 2.948351, time 0.8 sec\n","log: epoch 5, iter 6150, 20495.5 words/sec, avg_loss 2.545034, time 0.7 sec\n","log: epoch 5, iter 6160, 20948.7 words/sec, avg_loss 3.233195, time 0.7 sec\n","log: epoch 5, iter 6170, 20661.9 words/sec, avg_loss 2.657751, time 0.7 sec\n","log: epoch 5, iter 6180, 20505.6 words/sec, avg_loss 2.792835, time 0.7 sec\n","log: epoch 5, iter 6190, 22217.7 words/sec, avg_loss 2.657978, time 0.6 sec\n","log: epoch 5, iter 6200, 20609.9 words/sec, avg_loss 2.811296, time 0.7 sec\n","log: epoch 5, iter 6210, 21390.4 words/sec, avg_loss 2.971770, time 0.7 sec\n","log: epoch 5, iter 6220, 20695.8 words/sec, avg_loss 2.680876, time 0.7 sec\n","log: epoch 5, iter 6230, 20626.0 words/sec, avg_loss 2.702451, time 0.7 sec\n","log: epoch 5, iter 6240, 19117.3 words/sec, avg_loss 3.217348, time 0.8 sec\n","log: epoch 5, iter 6250, 21375.4 words/sec, avg_loss 2.910086, time 0.7 sec\n","dev: epoch 5, iter 6250, 20223.1 words/sec, avg_loss 2.762129, time 17.8 sec\n","dev: epoch 5, iter 6250, dev_loss 1.299136, patience 1, decay_num 1\n","log: epoch 5, iter 6260, 20805.0 words/sec, avg_loss 2.852526, time 0.7 sec\n","log: epoch 5, iter 6270, 20728.1 words/sec, avg_loss 2.693692, time 0.7 sec\n","log: epoch 5, iter 6280, 19911.4 words/sec, avg_loss 2.837903, time 0.7 sec\n","log: epoch 5, iter 6290, 19928.9 words/sec, avg_loss 2.676851, time 0.7 sec\n","log: epoch 5, iter 6300, 20661.5 words/sec, avg_loss 2.836344, time 0.7 sec\n","log: epoch 5, iter 6310, 19519.9 words/sec, avg_loss 2.853480, time 0.7 sec\n","log: epoch 5, iter 6320, 20952.3 words/sec, avg_loss 2.720195, time 0.7 sec\n","log: epoch 5, iter 6330, 22060.3 words/sec, avg_loss 2.694164, time 0.6 sec\n","log: epoch 6, iter 6340, 19015.6 words/sec, avg_loss 2.747781, time 0.7 sec\n","log: epoch 6, iter 6350, 20501.5 words/sec, avg_loss 2.655914, time 0.7 sec\n","log: epoch 6, iter 6360, 21540.9 words/sec, avg_loss 2.832080, time 0.7 sec\n","log: epoch 6, iter 6370, 19542.2 words/sec, avg_loss 3.062754, time 0.7 sec\n","log: epoch 6, iter 6380, 21370.3 words/sec, avg_loss 2.760602, time 0.7 sec\n","log: epoch 6, iter 6390, 19596.7 words/sec, avg_loss 2.951118, time 0.8 sec\n","log: epoch 6, iter 6400, 21001.2 words/sec, avg_loss 2.951024, time 0.7 sec\n","log: epoch 6, iter 6410, 19349.9 words/sec, avg_loss 2.654968, time 0.7 sec\n","log: epoch 6, iter 6420, 20352.0 words/sec, avg_loss 2.896188, time 0.7 sec\n","log: epoch 6, iter 6430, 20319.8 words/sec, avg_loss 2.763223, time 0.7 sec\n","log: epoch 6, iter 6440, 21177.3 words/sec, avg_loss 2.832543, time 0.7 sec\n","log: epoch 6, iter 6450, 19893.3 words/sec, avg_loss 2.620334, time 0.7 sec\n","log: epoch 6, iter 6460, 21263.5 words/sec, avg_loss 2.628851, time 0.7 sec\n","log: epoch 6, iter 6470, 21214.9 words/sec, avg_loss 2.988980, time 0.7 sec\n","log: epoch 6, iter 6480, 21198.7 words/sec, avg_loss 3.281763, time 0.7 sec\n","log: epoch 6, iter 6490, 20339.7 words/sec, avg_loss 3.043375, time 0.7 sec\n","log: epoch 6, iter 6500, 20428.5 words/sec, avg_loss 2.766869, time 0.7 sec\n","dev: epoch 6, iter 6500, 20483.6 words/sec, avg_loss 2.824313, time 17.6 sec\n","dev: epoch 6, iter 6500, dev_loss 1.299136, patience 2, decay_num 1\n","log: epoch 6, iter 6510, 19860.5 words/sec, avg_loss 2.851153, time 0.8 sec\n","log: epoch 6, iter 6520, 20538.7 words/sec, avg_loss 2.824057, time 0.7 sec\n","log: epoch 6, iter 6530, 19697.2 words/sec, avg_loss 2.930836, time 0.7 sec\n","log: epoch 6, iter 6540, 18474.3 words/sec, avg_loss 2.646983, time 0.8 sec\n","log: epoch 6, iter 6550, 19917.5 words/sec, avg_loss 2.933795, time 0.7 sec\n","log: epoch 6, iter 6560, 21206.9 words/sec, avg_loss 2.762405, time 0.7 sec\n","log: epoch 6, iter 6570, 20427.3 words/sec, avg_loss 3.105721, time 0.7 sec\n","log: epoch 6, iter 6580, 19257.5 words/sec, avg_loss 2.596768, time 0.7 sec\n","log: epoch 6, iter 6590, 20162.7 words/sec, avg_loss 2.912392, time 0.7 sec\n","log: epoch 6, iter 6600, 20906.3 words/sec, avg_loss 2.769904, time 0.7 sec\n","log: epoch 6, iter 6610, 21305.5 words/sec, avg_loss 2.824938, time 0.7 sec\n","log: epoch 6, iter 6620, 19807.5 words/sec, avg_loss 2.682937, time 0.7 sec\n","log: epoch 6, iter 6630, 19745.6 words/sec, avg_loss 2.537209, time 0.7 sec\n","log: epoch 6, iter 6640, 21353.0 words/sec, avg_loss 2.831429, time 0.7 sec\n","log: epoch 6, iter 6650, 20254.4 words/sec, avg_loss 2.909328, time 0.7 sec\n","log: epoch 6, iter 6660, 20017.7 words/sec, avg_loss 2.707191, time 0.7 sec\n","log: epoch 6, iter 6670, 21615.0 words/sec, avg_loss 2.971065, time 0.7 sec\n","log: epoch 6, iter 6680, 19994.1 words/sec, avg_loss 2.607363, time 0.7 sec\n","log: epoch 6, iter 6690, 20353.1 words/sec, avg_loss 2.884243, time 0.7 sec\n","log: epoch 6, iter 6700, 18575.5 words/sec, avg_loss 2.962403, time 0.8 sec\n","log: epoch 6, iter 6710, 19237.5 words/sec, avg_loss 2.998137, time 0.8 sec\n","log: epoch 6, iter 6720, 21469.6 words/sec, avg_loss 2.514859, time 0.7 sec\n","log: epoch 6, iter 6730, 20692.2 words/sec, avg_loss 2.747218, time 0.7 sec\n","log: epoch 6, iter 6740, 20737.9 words/sec, avg_loss 2.965913, time 0.7 sec\n","log: epoch 6, iter 6750, 20772.5 words/sec, avg_loss 3.103630, time 0.7 sec\n","dev: epoch 6, iter 6750, 20229.0 words/sec, avg_loss 2.823275, time 17.8 sec\n","dev: epoch 6, iter 6750, dev_loss 1.299136, patience 3, decay_num 1\n","log: epoch 6, iter 6760, 21004.6 words/sec, avg_loss 2.906898, time 0.7 sec\n","log: epoch 6, iter 6770, 19329.7 words/sec, avg_loss 2.973645, time 0.7 sec\n","log: epoch 6, iter 6780, 19366.1 words/sec, avg_loss 2.837890, time 0.7 sec\n","log: epoch 6, iter 6790, 20064.5 words/sec, avg_loss 2.568532, time 0.7 sec\n","log: epoch 6, iter 6800, 20090.6 words/sec, avg_loss 2.540139, time 0.7 sec\n","log: epoch 6, iter 6810, 20295.7 words/sec, avg_loss 2.652559, time 0.7 sec\n","log: epoch 6, iter 6820, 20098.9 words/sec, avg_loss 2.661833, time 0.7 sec\n","log: epoch 6, iter 6830, 19962.4 words/sec, avg_loss 2.758106, time 0.7 sec\n","log: epoch 6, iter 6840, 20072.6 words/sec, avg_loss 2.521905, time 0.7 sec\n","log: epoch 6, iter 6850, 20491.8 words/sec, avg_loss 2.783978, time 0.7 sec\n","log: epoch 6, iter 6860, 21759.9 words/sec, avg_loss 3.205422, time 0.7 sec\n","log: epoch 6, iter 6870, 20158.5 words/sec, avg_loss 2.821721, time 0.7 sec\n","log: epoch 6, iter 6880, 19436.9 words/sec, avg_loss 2.679579, time 0.7 sec\n","log: epoch 6, iter 6890, 20030.6 words/sec, avg_loss 2.675254, time 0.7 sec\n","log: epoch 6, iter 6900, 20773.6 words/sec, avg_loss 3.225387, time 0.7 sec\n","log: epoch 6, iter 6910, 19640.5 words/sec, avg_loss 3.048656, time 0.7 sec\n","log: epoch 6, iter 6920, 20027.5 words/sec, avg_loss 3.046746, time 0.7 sec\n","log: epoch 6, iter 6930, 21370.8 words/sec, avg_loss 2.779576, time 0.7 sec\n","log: epoch 6, iter 6940, 20115.1 words/sec, avg_loss 2.833322, time 0.7 sec\n","log: epoch 6, iter 6950, 20105.9 words/sec, avg_loss 2.835640, time 0.8 sec\n","log: epoch 6, iter 6960, 20162.0 words/sec, avg_loss 2.600549, time 0.7 sec\n","log: epoch 6, iter 6970, 20580.0 words/sec, avg_loss 2.706802, time 0.7 sec\n","log: epoch 6, iter 6980, 20939.2 words/sec, avg_loss 2.736579, time 0.7 sec\n","log: epoch 6, iter 6990, 21482.5 words/sec, avg_loss 2.859731, time 0.7 sec\n","log: epoch 6, iter 7000, 20237.2 words/sec, avg_loss 2.537784, time 0.7 sec\n","dev: epoch 6, iter 7000, 20286.6 words/sec, avg_loss 2.791929, time 17.8 sec\n","dev: epoch 6, iter 7000, dev_loss 1.299136, patience 0, decay_num 2\n","log: epoch 6, iter 7010, 20948.9 words/sec, avg_loss 2.957757, time 0.7 sec\n","log: epoch 6, iter 7020, 20616.4 words/sec, avg_loss 2.858302, time 0.7 sec\n","log: epoch 6, iter 7030, 19983.6 words/sec, avg_loss 2.795092, time 0.7 sec\n","log: epoch 6, iter 7040, 19251.1 words/sec, avg_loss 2.370749, time 0.8 sec\n","log: epoch 6, iter 7050, 19407.9 words/sec, avg_loss 2.543551, time 0.7 sec\n","log: epoch 6, iter 7060, 20423.3 words/sec, avg_loss 2.752727, time 0.7 sec\n","log: epoch 6, iter 7070, 19849.3 words/sec, avg_loss 2.783290, time 0.7 sec\n","log: epoch 6, iter 7080, 20256.2 words/sec, avg_loss 2.657696, time 0.7 sec\n","log: epoch 6, iter 7090, 19781.6 words/sec, avg_loss 2.761594, time 0.8 sec\n","log: epoch 6, iter 7100, 19753.5 words/sec, avg_loss 3.020754, time 0.7 sec\n","log: epoch 6, iter 7110, 20898.6 words/sec, avg_loss 2.516399, time 0.7 sec\n","log: epoch 6, iter 7120, 19141.0 words/sec, avg_loss 2.702453, time 0.7 sec\n","log: epoch 6, iter 7130, 20332.5 words/sec, avg_loss 2.910713, time 0.7 sec\n","log: epoch 6, iter 7140, 20021.3 words/sec, avg_loss 2.588800, time 0.7 sec\n","log: epoch 6, iter 7150, 19291.7 words/sec, avg_loss 2.480642, time 0.7 sec\n","log: epoch 6, iter 7160, 20622.7 words/sec, avg_loss 2.614727, time 0.7 sec\n","log: epoch 6, iter 7170, 20136.6 words/sec, avg_loss 2.696355, time 0.7 sec\n","log: epoch 6, iter 7180, 18440.4 words/sec, avg_loss 2.772745, time 0.8 sec\n","log: epoch 6, iter 7190, 19952.6 words/sec, avg_loss 2.557908, time 0.7 sec\n","log: epoch 6, iter 7200, 21445.4 words/sec, avg_loss 2.907807, time 0.6 sec\n","log: epoch 6, iter 7210, 19226.5 words/sec, avg_loss 3.073157, time 0.7 sec\n","log: epoch 6, iter 7220, 20359.3 words/sec, avg_loss 2.928202, time 0.7 sec\n","log: epoch 6, iter 7230, 20400.1 words/sec, avg_loss 2.975505, time 0.7 sec\n","log: epoch 6, iter 7240, 19013.1 words/sec, avg_loss 2.468590, time 0.7 sec\n","log: epoch 6, iter 7250, 20226.6 words/sec, avg_loss 2.495437, time 0.7 sec\n","dev: epoch 6, iter 7250, 19970.5 words/sec, avg_loss 2.727638, time 17.8 sec\n","dev: epoch 6, iter 7250, dev_loss 1.299136, patience 1, decay_num 2\n","log: epoch 6, iter 7260, 19455.4 words/sec, avg_loss 2.747109, time 0.7 sec\n","log: epoch 6, iter 7270, 19008.5 words/sec, avg_loss 2.306455, time 0.7 sec\n","log: epoch 6, iter 7280, 21465.2 words/sec, avg_loss 2.714240, time 0.7 sec\n","log: epoch 6, iter 7290, 20009.7 words/sec, avg_loss 3.037948, time 0.7 sec\n","log: epoch 6, iter 7300, 19759.1 words/sec, avg_loss 2.806496, time 0.7 sec\n","log: epoch 6, iter 7310, 20904.6 words/sec, avg_loss 2.742508, time 0.7 sec\n","log: epoch 6, iter 7320, 19509.2 words/sec, avg_loss 2.626227, time 0.7 sec\n","log: epoch 6, iter 7330, 21656.2 words/sec, avg_loss 2.886590, time 0.7 sec\n","log: epoch 6, iter 7340, 20835.0 words/sec, avg_loss 2.593586, time 0.7 sec\n","log: epoch 6, iter 7350, 20963.0 words/sec, avg_loss 2.646667, time 0.7 sec\n","log: epoch 6, iter 7360, 19947.5 words/sec, avg_loss 2.648725, time 0.7 sec\n","log: epoch 6, iter 7370, 20269.2 words/sec, avg_loss 3.075899, time 0.7 sec\n","log: epoch 6, iter 7380, 20650.1 words/sec, avg_loss 2.685894, time 0.7 sec\n","log: epoch 6, iter 7390, 20294.2 words/sec, avg_loss 2.898106, time 0.7 sec\n","log: epoch 6, iter 7400, 20551.8 words/sec, avg_loss 2.693567, time 0.7 sec\n","log: epoch 6, iter 7410, 19800.7 words/sec, avg_loss 2.525194, time 0.7 sec\n","log: epoch 6, iter 7420, 21022.8 words/sec, avg_loss 2.817356, time 0.7 sec\n","log: epoch 6, iter 7430, 20435.4 words/sec, avg_loss 2.674503, time 0.7 sec\n","log: epoch 6, iter 7440, 19867.8 words/sec, avg_loss 2.669768, time 0.7 sec\n","log: epoch 6, iter 7450, 18917.0 words/sec, avg_loss 2.787761, time 0.7 sec\n","log: epoch 6, iter 7460, 19680.5 words/sec, avg_loss 2.806508, time 0.7 sec\n","log: epoch 6, iter 7470, 20232.9 words/sec, avg_loss 2.761343, time 0.7 sec\n","log: epoch 6, iter 7480, 20193.5 words/sec, avg_loss 2.927678, time 0.7 sec\n","log: epoch 6, iter 7490, 20824.1 words/sec, avg_loss 2.844697, time 0.7 sec\n","log: epoch 6, iter 7500, 19397.3 words/sec, avg_loss 2.647245, time 0.7 sec\n","dev: epoch 6, iter 7500, 20211.1 words/sec, avg_loss 2.742883, time 17.8 sec\n","dev: epoch 6, iter 7500, dev_loss 1.299136, patience 2, decay_num 2\n","log: epoch 6, iter 7510, 20191.4 words/sec, avg_loss 2.592762, time 0.7 sec\n","log: epoch 6, iter 7520, 19497.2 words/sec, avg_loss 2.577892, time 0.7 sec\n","log: epoch 6, iter 7530, 18587.9 words/sec, avg_loss 2.536127, time 0.8 sec\n","log: epoch 6, iter 7540, 20334.7 words/sec, avg_loss 2.924046, time 0.7 sec\n","log: epoch 6, iter 7550, 20217.0 words/sec, avg_loss 2.898170, time 0.7 sec\n","log: epoch 6, iter 7560, 19391.3 words/sec, avg_loss 2.950086, time 0.7 sec\n","log: epoch 6, iter 7570, 20835.9 words/sec, avg_loss 2.677449, time 0.7 sec\n","log: epoch 6, iter 7580, 20502.0 words/sec, avg_loss 2.965277, time 0.7 sec\n","log: epoch 6, iter 7590, 19476.9 words/sec, avg_loss 2.865439, time 0.7 sec\n","log: epoch 6, iter 7600, 20892.5 words/sec, avg_loss 2.696868, time 0.7 sec\n","log: epoch 7, iter 7610, 18993.5 words/sec, avg_loss 2.759055, time 0.7 sec\n","log: epoch 7, iter 7620, 19107.0 words/sec, avg_loss 3.055809, time 0.7 sec\n","log: epoch 7, iter 7630, 18362.4 words/sec, avg_loss 3.087529, time 0.7 sec\n","log: epoch 7, iter 7640, 20331.6 words/sec, avg_loss 2.968350, time 0.7 sec\n","log: epoch 7, iter 7650, 20668.7 words/sec, avg_loss 2.576915, time 0.7 sec\n","log: epoch 7, iter 7660, 20060.5 words/sec, avg_loss 2.712565, time 0.7 sec\n","log: epoch 7, iter 7670, 18853.7 words/sec, avg_loss 3.087563, time 0.8 sec\n","log: epoch 7, iter 7680, 19359.4 words/sec, avg_loss 2.782436, time 0.7 sec\n","log: epoch 7, iter 7690, 21394.9 words/sec, avg_loss 2.685748, time 0.7 sec\n","log: epoch 7, iter 7700, 20931.7 words/sec, avg_loss 2.765021, time 0.7 sec\n","log: epoch 7, iter 7710, 20943.3 words/sec, avg_loss 2.460245, time 0.7 sec\n","log: epoch 7, iter 7720, 21267.9 words/sec, avg_loss 2.843084, time 0.7 sec\n","log: epoch 7, iter 7730, 19516.1 words/sec, avg_loss 2.640260, time 0.7 sec\n","log: epoch 7, iter 7740, 20305.4 words/sec, avg_loss 2.773419, time 0.7 sec\n","log: epoch 7, iter 7750, 20390.9 words/sec, avg_loss 2.761636, time 0.7 sec\n","dev: epoch 7, iter 7750, 19992.3 words/sec, avg_loss 2.785810, time 17.8 sec\n","dev: epoch 7, iter 7750, dev_loss 1.299136, patience 3, decay_num 2\n","log: epoch 7, iter 7760, 20259.2 words/sec, avg_loss 3.177527, time 0.7 sec\n","log: epoch 7, iter 7770, 21995.5 words/sec, avg_loss 2.700979, time 0.7 sec\n","log: epoch 7, iter 7780, 21638.9 words/sec, avg_loss 2.751885, time 0.7 sec\n","log: epoch 7, iter 7790, 19629.6 words/sec, avg_loss 2.887310, time 0.7 sec\n","log: epoch 7, iter 7800, 19365.6 words/sec, avg_loss 3.325395, time 0.8 sec\n","log: epoch 7, iter 7810, 20176.1 words/sec, avg_loss 2.771396, time 0.7 sec\n","log: epoch 7, iter 7820, 19980.9 words/sec, avg_loss 2.472008, time 0.7 sec\n","log: epoch 7, iter 7830, 20671.0 words/sec, avg_loss 2.801333, time 0.7 sec\n","log: epoch 7, iter 7840, 20182.1 words/sec, avg_loss 3.400610, time 0.7 sec\n","log: epoch 7, iter 7850, 20228.6 words/sec, avg_loss 2.941619, time 0.7 sec\n","log: epoch 7, iter 7860, 20900.1 words/sec, avg_loss 2.872520, time 0.7 sec\n","log: epoch 7, iter 7870, 21314.1 words/sec, avg_loss 2.569992, time 0.7 sec\n","log: epoch 7, iter 7880, 19855.7 words/sec, avg_loss 2.644766, time 0.7 sec\n","log: epoch 7, iter 7890, 20358.3 words/sec, avg_loss 2.620850, time 0.7 sec\n","log: epoch 7, iter 7900, 20756.6 words/sec, avg_loss 3.181564, time 0.7 sec\n","log: epoch 7, iter 7910, 19623.1 words/sec, avg_loss 2.920294, time 0.7 sec\n","log: epoch 7, iter 7920, 19480.0 words/sec, avg_loss 2.857887, time 0.7 sec\n","log: epoch 7, iter 7930, 19011.6 words/sec, avg_loss 2.835922, time 0.7 sec\n","log: epoch 7, iter 7940, 20739.2 words/sec, avg_loss 2.793891, time 0.7 sec\n","log: epoch 7, iter 7950, 20754.2 words/sec, avg_loss 2.612523, time 0.7 sec\n","log: epoch 7, iter 7960, 19367.5 words/sec, avg_loss 2.993522, time 0.8 sec\n","log: epoch 7, iter 7970, 19828.6 words/sec, avg_loss 2.641979, time 0.7 sec\n","log: epoch 7, iter 7980, 20137.7 words/sec, avg_loss 2.720225, time 0.7 sec\n","log: epoch 7, iter 7990, 20396.3 words/sec, avg_loss 2.419718, time 0.7 sec\n","log: epoch 7, iter 8000, 20003.7 words/sec, avg_loss 3.202993, time 0.7 sec\n","dev: epoch 7, iter 8000, 20247.1 words/sec, avg_loss 2.844748, time 17.7 sec\n","dev: epoch 7, iter 8000, dev_loss 1.299136, patience 0, decay_num 3\n","log: epoch 7, iter 8010, 22415.3 words/sec, avg_loss 3.004745, time 0.7 sec\n","log: epoch 7, iter 8020, 20822.9 words/sec, avg_loss 2.602145, time 0.7 sec\n","log: epoch 7, iter 8030, 19780.7 words/sec, avg_loss 2.593331, time 0.7 sec\n","log: epoch 7, iter 8040, 19466.1 words/sec, avg_loss 2.621252, time 0.7 sec\n","log: epoch 7, iter 8050, 20826.3 words/sec, avg_loss 2.848633, time 0.7 sec\n","log: epoch 7, iter 8060, 20670.6 words/sec, avg_loss 2.661945, time 0.7 sec\n","log: epoch 7, iter 8070, 19999.1 words/sec, avg_loss 2.954389, time 0.8 sec\n","log: epoch 7, iter 8080, 19949.6 words/sec, avg_loss 2.893381, time 0.7 sec\n","log: epoch 7, iter 8090, 21305.0 words/sec, avg_loss 2.557700, time 0.7 sec\n","log: epoch 7, iter 8100, 19428.9 words/sec, avg_loss 2.644737, time 0.7 sec\n","log: epoch 7, iter 8110, 19643.7 words/sec, avg_loss 2.552536, time 0.8 sec\n","log: epoch 7, iter 8120, 20608.0 words/sec, avg_loss 2.614956, time 0.7 sec\n","log: epoch 7, iter 8130, 20224.5 words/sec, avg_loss 2.704689, time 0.8 sec\n","log: epoch 7, iter 8140, 20723.4 words/sec, avg_loss 2.985091, time 0.7 sec\n","log: epoch 7, iter 8150, 20165.5 words/sec, avg_loss 2.697913, time 0.7 sec\n","log: epoch 7, iter 8160, 20134.9 words/sec, avg_loss 2.818504, time 0.7 sec\n","log: epoch 7, iter 8170, 19861.7 words/sec, avg_loss 2.816565, time 0.7 sec\n","log: epoch 7, iter 8180, 19741.0 words/sec, avg_loss 2.821201, time 0.7 sec\n","log: epoch 7, iter 8190, 21224.4 words/sec, avg_loss 3.346845, time 0.7 sec\n","log: epoch 7, iter 8200, 19852.5 words/sec, avg_loss 2.865080, time 0.7 sec\n","log: epoch 7, iter 8210, 20406.6 words/sec, avg_loss 3.021614, time 0.7 sec\n","log: epoch 7, iter 8220, 21121.2 words/sec, avg_loss 2.629781, time 0.7 sec\n","log: epoch 7, iter 8230, 20708.4 words/sec, avg_loss 2.810065, time 0.7 sec\n","log: epoch 7, iter 8240, 19913.7 words/sec, avg_loss 2.930261, time 0.7 sec\n","log: epoch 7, iter 8250, 19789.6 words/sec, avg_loss 2.385188, time 0.7 sec\n","dev: epoch 7, iter 8250, 20338.6 words/sec, avg_loss 2.775302, time 17.8 sec\n","dev: epoch 7, iter 8250, dev_loss 1.299136, patience 1, decay_num 3\n","log: epoch 7, iter 8260, 20670.1 words/sec, avg_loss 2.846999, time 0.7 sec\n","log: epoch 7, iter 8270, 20539.2 words/sec, avg_loss 2.651547, time 0.7 sec\n","log: epoch 7, iter 8280, 18823.2 words/sec, avg_loss 2.606575, time 0.7 sec\n","log: epoch 7, iter 8290, 18271.2 words/sec, avg_loss 2.679409, time 0.7 sec\n","log: epoch 7, iter 8300, 21387.3 words/sec, avg_loss 3.166146, time 0.7 sec\n","log: epoch 7, iter 8310, 20513.7 words/sec, avg_loss 3.071555, time 0.7 sec\n","log: epoch 7, iter 8320, 20132.5 words/sec, avg_loss 2.551990, time 0.7 sec\n","log: epoch 7, iter 8330, 19877.8 words/sec, avg_loss 3.052127, time 0.7 sec\n","log: epoch 7, iter 8340, 18967.1 words/sec, avg_loss 2.867840, time 0.7 sec\n","log: epoch 7, iter 8350, 20144.0 words/sec, avg_loss 3.016459, time 0.7 sec\n","log: epoch 7, iter 8360, 20337.0 words/sec, avg_loss 2.912186, time 0.7 sec\n","log: epoch 7, iter 8370, 19843.3 words/sec, avg_loss 2.840947, time 0.7 sec\n","log: epoch 7, iter 8380, 19822.7 words/sec, avg_loss 2.789289, time 0.7 sec\n","log: epoch 7, iter 8390, 19996.7 words/sec, avg_loss 3.046304, time 0.7 sec\n","log: epoch 7, iter 8400, 21763.6 words/sec, avg_loss 3.135738, time 0.7 sec\n","log: epoch 7, iter 8410, 19806.7 words/sec, avg_loss 2.539641, time 0.7 sec\n","log: epoch 7, iter 8420, 19739.8 words/sec, avg_loss 2.639674, time 0.7 sec\n","log: epoch 7, iter 8430, 18788.8 words/sec, avg_loss 2.812091, time 0.7 sec\n","log: epoch 7, iter 8440, 20837.5 words/sec, avg_loss 2.657328, time 0.7 sec\n","log: epoch 7, iter 8450, 19645.9 words/sec, avg_loss 2.967575, time 0.7 sec\n","log: epoch 7, iter 8460, 19686.1 words/sec, avg_loss 2.533831, time 0.7 sec\n","log: epoch 7, iter 8470, 20072.1 words/sec, avg_loss 3.281143, time 0.7 sec\n","log: epoch 7, iter 8480, 19360.8 words/sec, avg_loss 2.793256, time 0.7 sec\n","log: epoch 7, iter 8490, 19626.5 words/sec, avg_loss 2.827846, time 0.7 sec\n","log: epoch 7, iter 8500, 20402.5 words/sec, avg_loss 2.783765, time 0.7 sec\n","dev: epoch 7, iter 8500, 19945.3 words/sec, avg_loss 2.842850, time 17.9 sec\n","dev: epoch 7, iter 8500, dev_loss 1.299136, patience 2, decay_num 3\n","log: epoch 7, iter 8510, 20058.7 words/sec, avg_loss 2.826479, time 0.7 sec\n","log: epoch 7, iter 8520, 20614.7 words/sec, avg_loss 2.769600, time 0.7 sec\n","log: epoch 7, iter 8530, 19581.3 words/sec, avg_loss 2.343400, time 0.7 sec\n","log: epoch 7, iter 8540, 21207.1 words/sec, avg_loss 2.556602, time 0.7 sec\n","log: epoch 7, iter 8550, 20640.1 words/sec, avg_loss 2.609022, time 0.7 sec\n","log: epoch 7, iter 8560, 18507.4 words/sec, avg_loss 2.428208, time 0.7 sec\n","log: epoch 7, iter 8570, 19455.4 words/sec, avg_loss 2.328603, time 0.7 sec\n","log: epoch 7, iter 8580, 19342.4 words/sec, avg_loss 2.726054, time 0.7 sec\n","log: epoch 7, iter 8590, 20093.4 words/sec, avg_loss 2.949687, time 0.7 sec\n","log: epoch 7, iter 8600, 20414.7 words/sec, avg_loss 3.248727, time 0.7 sec\n","log: epoch 7, iter 8610, 19565.0 words/sec, avg_loss 2.508919, time 0.8 sec\n","log: epoch 7, iter 8620, 19494.7 words/sec, avg_loss 2.532271, time 0.8 sec\n","log: epoch 7, iter 8630, 19218.5 words/sec, avg_loss 2.940667, time 0.8 sec\n","log: epoch 7, iter 8640, 19317.4 words/sec, avg_loss 2.991831, time 0.7 sec\n","log: epoch 7, iter 8650, 21110.0 words/sec, avg_loss 2.851158, time 0.7 sec\n","log: epoch 7, iter 8660, 19750.9 words/sec, avg_loss 2.798911, time 0.7 sec\n","log: epoch 7, iter 8670, 20492.0 words/sec, avg_loss 3.221365, time 0.7 sec\n","log: epoch 7, iter 8680, 18868.6 words/sec, avg_loss 2.437639, time 0.7 sec\n","log: epoch 7, iter 8690, 19726.3 words/sec, avg_loss 2.618056, time 0.7 sec\n","log: epoch 7, iter 8700, 20325.3 words/sec, avg_loss 3.126456, time 0.7 sec\n","log: epoch 7, iter 8710, 20740.8 words/sec, avg_loss 2.930997, time 0.7 sec\n","log: epoch 7, iter 8720, 20304.6 words/sec, avg_loss 2.784908, time 0.7 sec\n","log: epoch 7, iter 8730, 20678.6 words/sec, avg_loss 2.856828, time 0.7 sec\n","log: epoch 7, iter 8740, 19687.1 words/sec, avg_loss 2.782057, time 0.8 sec\n","log: epoch 7, iter 8750, 19401.8 words/sec, avg_loss 2.976491, time 0.7 sec\n","dev: epoch 7, iter 8750, 19930.4 words/sec, avg_loss 2.765798, time 18.0 sec\n","dev: epoch 7, iter 8750, dev_loss 1.299136, patience 3, decay_num 3\n","log: epoch 7, iter 8760, 19537.8 words/sec, avg_loss 2.638581, time 0.7 sec\n","log: epoch 7, iter 8770, 20044.6 words/sec, avg_loss 2.853575, time 0.7 sec\n","log: epoch 7, iter 8780, 19991.4 words/sec, avg_loss 2.857335, time 0.7 sec\n","log: epoch 7, iter 8790, 19943.9 words/sec, avg_loss 2.838421, time 0.7 sec\n","log: epoch 7, iter 8800, 21118.5 words/sec, avg_loss 2.797081, time 0.7 sec\n","log: epoch 7, iter 8810, 20270.8 words/sec, avg_loss 2.702387, time 0.7 sec\n","log: epoch 7, iter 8820, 20208.5 words/sec, avg_loss 2.620909, time 0.7 sec\n","log: epoch 7, iter 8830, 21542.8 words/sec, avg_loss 2.965618, time 0.7 sec\n","log: epoch 7, iter 8840, 20935.8 words/sec, avg_loss 2.362762, time 0.7 sec\n","log: epoch 7, iter 8850, 21367.4 words/sec, avg_loss 2.603635, time 0.7 sec\n","log: epoch 7, iter 8860, 21102.4 words/sec, avg_loss 2.749478, time 0.7 sec\n","log: epoch 8, iter 8870, 18087.1 words/sec, avg_loss 2.752767, time 0.8 sec\n","log: epoch 8, iter 8880, 20895.5 words/sec, avg_loss 2.599206, time 0.7 sec\n","log: epoch 8, iter 8890, 20526.0 words/sec, avg_loss 2.847332, time 0.7 sec\n","log: epoch 8, iter 8900, 20143.5 words/sec, avg_loss 2.879462, time 0.7 sec\n","log: epoch 8, iter 8910, 20848.1 words/sec, avg_loss 2.732016, time 0.7 sec\n","log: epoch 8, iter 8920, 19742.3 words/sec, avg_loss 2.462427, time 0.7 sec\n","log: epoch 8, iter 8930, 22165.1 words/sec, avg_loss 2.754246, time 0.6 sec\n","log: epoch 8, iter 8940, 20068.2 words/sec, avg_loss 3.073874, time 0.7 sec\n","log: epoch 8, iter 8950, 19654.0 words/sec, avg_loss 3.163294, time 0.8 sec\n","log: epoch 8, iter 8960, 21588.5 words/sec, avg_loss 3.191172, time 0.7 sec\n","log: epoch 8, iter 8970, 20091.9 words/sec, avg_loss 2.820462, time 0.7 sec\n","log: epoch 8, iter 8980, 19125.8 words/sec, avg_loss 3.077431, time 0.7 sec\n","log: epoch 8, iter 8990, 19386.9 words/sec, avg_loss 2.930111, time 0.7 sec\n","log: epoch 8, iter 9000, 20102.7 words/sec, avg_loss 2.625117, time 0.7 sec\n","dev: epoch 8, iter 9000, 20307.2 words/sec, avg_loss 2.796045, time 17.7 sec\n","Early stop. Save result model to ./model/model.pth\n"]}]},{"cell_type":"code","source":["!sh run.sh test"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RvVNAgVm0SWn","executionInfo":{"status":"ok","timestamp":1639970690290,"user_tz":-420,"elapsed":10398,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"d716917c-e672-4022-c6e7-c14a90f8e362"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["num of test samples: 4631\n","start testing...\n","using device cuda\n","processed 172601 tokens with 6192 phrases; found: 5308 phrases; correct: 4532.\n","accuracy:  97.46%; precision:  85.38%; recall:  73.19%; FB1:  78.82\n","              LOC: precision:  86.14%; recall:  79.74%; FB1:  82.82  2663\n","              ORG: precision:  84.59%; recall:  65.59%; FB1:  73.89  1032\n","              PER: precision:  84.62%; recall:  68.80%; FB1:  75.90  1613\n"]}]}]}