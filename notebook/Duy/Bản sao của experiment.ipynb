{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Bản sao của experiment.ipynb","provenance":[],"collapsed_sections":["hTw8AYfHGszL","FLfiCckAHPWB","7k1WqTO59A5Z","CAnYaObi9HXf"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"sMicb1TP98ji","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640248825976,"user_tz":-420,"elapsed":18930,"user":{"displayName":"Quang Duy Trần","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvMaEX5IKcYvanQuv2HTQEdjP5mTCjGM4CkVJX=s64","userId":"16019943656023573816"}},"outputId":"5df6fdca-d0e0-434d-f078-285accbbfd0f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive._mount('/content/drive')"]},{"cell_type":"code","source":["!pip install transformers seqeval[gpu] -q\n","!pip install fairseq -q\n","!pip install fastBPE -q\n","!pip install pytorch-crf -q"],"metadata":{"id":"if8rbToF-IbX","executionInfo":{"status":"ok","timestamp":1640248861180,"user_tz":-420,"elapsed":35210,"user":{"displayName":"Quang Duy Trần","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvMaEX5IKcYvanQuv2HTQEdjP5mTCjGM4CkVJX=s64","userId":"16019943656023573816"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8af71902-b5a1-45b4-c5c2-fa6cbd002053"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 3.4 MB 5.3 MB/s \n","\u001b[K     |████████████████████████████████| 43 kB 1.9 MB/s \n","\u001b[K     |████████████████████████████████| 895 kB 46.9 MB/s \n","\u001b[K     |████████████████████████████████| 3.3 MB 50.0 MB/s \n","\u001b[K     |████████████████████████████████| 61 kB 427 kB/s \n","\u001b[K     |████████████████████████████████| 596 kB 52.0 MB/s \n","\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |████████████████████████████████| 1.7 MB 5.4 MB/s \n","\u001b[K     |████████████████████████████████| 90 kB 1.1 MB/s \n","\u001b[K     |████████████████████████████████| 145 kB 48.1 MB/s \n","\u001b[K     |████████████████████████████████| 74 kB 2.2 MB/s \n","\u001b[K     |████████████████████████████████| 112 kB 52.9 MB/s \n","\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for fastBPE (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/NLP/project_nlp"],"metadata":{"id":"cD7orqk4-HRv","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640248861181,"user_tz":-420,"elapsed":17,"user":{"displayName":"Quang Duy Trần","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvMaEX5IKcYvanQuv2HTQEdjP5mTCjGM4CkVJX=s64","userId":"16019943656023573816"}},"outputId":"402d17fd-8972-4583-e65e-583237bb8170"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/NLP/project_nlp\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","from torchcrf import CRF\n","\n","from transformers import RobertaConfig, RobertaPreTrainedModel, RobertaModel, RobertaForTokenClassification\n","from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule\n","from transformers.modeling_outputs import TokenClassifierOutput\n","\n","import seqeval\n","from seqeval.metrics import classification_report, f1_score\n","\n","import pandas as pd\n","import numpy as np\n","import argparse\n","import time\n","import tqdm\n","\n","from fairseq.data.encoders.fastbpe import fastBPE\n","from fairseq.data import Dictionary\n","\n","from sklearn.utils.class_weight import compute_class_weight\n","\n","from src.dataset import *"],"metadata":{"id":"vQKjvZWT-LUa","executionInfo":{"status":"ok","timestamp":1640248871573,"user_tz":-420,"elapsed":10402,"user":{"displayName":"Quang Duy Trần","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvMaEX5IKcYvanQuv2HTQEdjP5mTCjGM4CkVJX=s64","userId":"16019943656023573816"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"],"metadata":{"id":"usUDwbTOJ47L","executionInfo":{"status":"ok","timestamp":1640248871574,"user_tz":-420,"elapsed":10,"user":{"displayName":"Quang Duy Trần","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvMaEX5IKcYvanQuv2HTQEdjP5mTCjGM4CkVJX=s64","userId":"16019943656023573816"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","source":["## DATA - HERE"],"metadata":{"id":"c0w2mvo-fF_X"}},{"cell_type":"code","source":["data = pd.read_csv('./data/joint/data_joint.csv')\n","data"],"metadata":{"id":"_tudmS6OfM9c","colab":{"base_uri":"https://localhost:8080/","height":554},"executionInfo":{"status":"ok","timestamp":1640248872034,"user_tz":-420,"elapsed":468,"user":{"displayName":"Quang Duy Trần","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvMaEX5IKcYvanQuv2HTQEdjP5mTCjGM4CkVJX=s64","userId":"16019943656023573816"}},"outputId":"453e5417-9b24-45b2-c36d-3d67e8b6b566"},"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-6d9fa9e5-ff13-421e-a5fe-a26c04098a3a\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>sentence</th>\n","      <th>word_labels</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>combo 3 cái giao có 1 cái , thành_ra đặt 6 cái...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-DES,B-DE...</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>mình mua áo có cổ màu trắng lại ship tới cho m...</td>\n","      <td>O,O,B-DES,B-DES,I-DES,B-DES,I-DES,O,O,O,O,O,B-...</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>giao sai hàng . tôi muốn trả hàng . đặt be đậm...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,B-DES,I-DES,O,B-DES,I-DES</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>sản_xuất việt_nam nhưng thấy in chữ trung_quốc...</td>\n","      <td>O,O,O,O,B-DES,I-DES,O,O,O,O,O,O,O</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>mình đặt áo sơ_mi trắng dài tay mà shop giao c...</td>\n","      <td>O,O,B-DES,I-DES,B-DES,B-DES,I-DES,O,O,O,O,O,O,...</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3652</th>\n","      <td>3652</td>\n","      <td>chất_lượng sản_phẩm giống mô tả . giao hàng nh...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3653</th>\n","      <td>3653</td>\n","      <td>hài_lòng vô_cùng , giao nhanh , nhân_viên giao...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,O,B-DES,O,B-DES</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3654</th>\n","      <td>3654</td>\n","      <td>sản_phẩm ổn . giá phải_chăng . thật_sự là nhận...</td>\n","      <td>O,O,O,B-PRI,B-PRI,O,O,O,O,O,O,O,O,O,O,O,O,O</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3655</th>\n","      <td>3655</td>\n","      <td>hàng đẹp chuẩn chất_lượng . nếu áo có đai ngan...</td>\n","      <td>O,B-DES,O,O,O,O,B-DES,O,O,O,B-DES,O,B-DES,O,O,O</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3656</th>\n","      <td>3656</td>\n","      <td>vải mặc mát , dày_dặn . không có miếng mút ngự...</td>\n","      <td>B-DES,O,B-DES,O,B-DES,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3657 rows × 4 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-6d9fa9e5-ff13-421e-a5fe-a26c04098a3a')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-6d9fa9e5-ff13-421e-a5fe-a26c04098a3a button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-6d9fa9e5-ff13-421e-a5fe-a26c04098a3a');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["      Unnamed: 0  ... label\n","0              0  ...    -1\n","1              1  ...    -1\n","2              2  ...    -1\n","3              3  ...    -1\n","4              4  ...    -1\n","...          ...  ...   ...\n","3652        3652  ...     1\n","3653        3653  ...     1\n","3654        3654  ...     1\n","3655        3655  ...     1\n","3656        3656  ...     1\n","\n","[3657 rows x 4 columns]"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","source":["## Params"],"metadata":{"id":"38uoKuPKL0_4"}},{"cell_type":"code","source":["BATCHSIZE_TRAIN = 8\n","BATCHSIZE_VAL = 4\n","LEARNING_RATE = 5e-5\n","MAX_LEN = 128\n","NUM_EPOCH = 20\n","SEED = 42\n","NUM_CLASS = 5\n","MAX_GRAD_NORM = 1"],"metadata":{"id":"rgWrsDNLLz7N","executionInfo":{"status":"ok","timestamp":1640248872035,"user_tz":-420,"elapsed":5,"user":{"displayName":"Quang Duy Trần","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvMaEX5IKcYvanQuv2HTQEdjP5mTCjGM4CkVJX=s64","userId":"16019943656023573816"}}},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":["## Data"],"metadata":{"id":"5Vo_jQIvM8sX"}},{"cell_type":"code","source":["parser = argparse.ArgumentParser()\n","parser.add_argument('--bpe-codes', \n","    default=\"./PhoBERT_base_transformers/bpe.codes\",\n","    required=False,\n","    type=str,\n","    help='path to fastBPE BPE'\n",")\n","args, unknown = parser.parse_known_args()\n","bpe = fastBPE(args)\n","\n","# Load the dictionary\n","vocab = Dictionary()\n","vocab.add_from_file(\"./PhoBERT_base_transformers/dict.txt\")\n","\n","labels_to_ids = {'B-DES': 1, 'B-PRI': 3, 'I-DES': 2, 'I-PRI': 4, 'O': 0, 'X': -100}\n","ids_to_labels = {0: 'O', 1: 'B-DES', 2: 'I-DES', 3:'B-PRI', 4:'I-PRI'}\n","\n","X, Y_label, Y_mask = convert_lines(\n","    data.sentence.values, \n","    data.word_labels.values, \n","    vocab, \n","    bpe, \n","    labels_to_ids, \n","    max_sequence_length=MAX_LEN)\n","\n","print('X shape: ', X.shape)\n","print('Y label shape', Y_label.shape)\n","print('Y mask shape', Y_mask.shape)\n","\n","train_size = 0.8\n","def train_test_split(data, train_size):\n","    X_df = pd.DataFrame(data)\n","    X_train = X_df.sample(frac = train_size, random_state=200)\n","    X_test = X_df.drop(X_train.index).reset_index(drop=True)\n","    X_train = X_train.reset_index(drop=True)\n","    return X_train.values, X_test .values\n","\n","X_train, X_test = train_test_split(X, train_size)\n","Y_label_train, Y_label_test = train_test_split(Y_label, train_size)\n","Y_mask_train, Y_mask_test = train_test_split(Y_mask, train_size)\n","\n","class_weight = compute_class_weight(\n","    class_weight='balanced', \n","    classes = np.array([0,1,2,3,4]), \n","    y=Y_label_train.flatten()[Y_label_train.flatten()>=0])\n","\n","print(class_weight)"],"metadata":{"id":"ao1LEzAfB6-P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640248875122,"user_tz":-420,"elapsed":3091,"user":{"displayName":"Quang Duy Trần","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvMaEX5IKcYvanQuv2HTQEdjP5mTCjGM4CkVJX=s64","userId":"16019943656023573816"}},"outputId":"8cf9b7c6-75f8-40bb-d1bc-ccd50224bbfe"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 3657/3657 [00:01<00:00, 2449.40it/s]\n"]},{"output_type":"stream","name":"stdout","text":["X shape:  (3657, 128)\n","Y label shape (3657, 128)\n","Y mask shape (3657, 128)\n","[ 0.24328616  1.55779008  6.52840909 15.97682503 31.33636364]\n"]}]},{"cell_type":"code","source":["train_dataset = TensorDataset(\n","    torch.tensor(X_train,dtype=torch.long), \n","    torch.tensor(Y_label_train,dtype=torch.long)\n","    )\n","valid_dataset = TensorDataset(\n","    torch.tensor(X_test,dtype=torch.long), \n","    torch.tensor(Y_label_test,dtype=torch.long)\n","    )\n","\n","train_loader = torch.utils.data.DataLoader(\n","    train_dataset, \n","    batch_size=BATCHSIZE_TRAIN, \n","    shuffle=True\n","    )\n","valid_loader = torch.utils.data.DataLoader(\n","    valid_dataset, \n","    batch_size=BATCHSIZE_VAL, \n","    shuffle=False\n","    )"],"metadata":{"id":"w-jI8GwDMJPw","executionInfo":{"status":"ok","timestamp":1640248875123,"user_tz":-420,"elapsed":8,"user":{"displayName":"Quang Duy Trần","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvMaEX5IKcYvanQuv2HTQEdjP5mTCjGM4CkVJX=s64","userId":"16019943656023573816"}}},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":["## Config"],"metadata":{"id":"8ObdUxfbMLgI"}},{"cell_type":"code","source":["class argu():\n","    def __init__(self):\n","        self.dict_path = \"./PhoBERT_base_transformers/dict.txt\"\n","        self.config_path = \"./PhoBERT_base_transformers/config.json\"\n","        self.max_sequence_length = MAX_LEN\n","        self.accumulation_steps = 1\n","        self.epochs = NUM_EPOCH\n","        self.seed = SEED\n","        self.bpe_codes = \"./PhoBERT_base_transformers/bpe.codes\"\n","args = argu()\n","\n","config = RobertaConfig.from_pretrained(\n","    args.config_path,\n","    output_hidden_states=True,\n","    return_dict=True,\n","    num_labels=NUM_CLASS,\n","    pad_token_id = 1,\n","    bos_token_id = 0,\n","    eos_token_id = 2,\n","    attention_probs_dropout_prob = 0.1,\n","    classifier_dropout=0.5,\n","    gradient_checkpointing=False,\n","    hidden_act=\"gelu\",\n","    hidden_dropout_prob=0.1,\n","    hidden_size=768,\n","    initializer_range=0.02,\n","    intermediate_size=3072,\n","    layer_norm_eps=1e-05,\n","    max_position_embeddings=258,\n","    model_type=\"roberta\",\n","    num_attention_heads=12,\n","    num_hidden_layers=12,\n","    position_embedding_type=\"absolute\",\n","    tokenizer_class=\"PhobertTokenizer\",\n","    transformers_version=\"4.15.0\",\n","    type_vocab_size=1,\n","    use_cache=True,\n","    vocab_size=64001\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"klKRUc99MNYH","executionInfo":{"status":"ok","timestamp":1640248875123,"user_tz":-420,"elapsed":8,"user":{"displayName":"Quang Duy Trần","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvMaEX5IKcYvanQuv2HTQEdjP5mTCjGM4CkVJX=s64","userId":"16019943656023573816"}},"outputId":"005e0f9d-78c2-40cd-cb92-3827c37aaf0e"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["You are using a model of type bert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n"]}]},{"cell_type":"code","source":["def train(epoch, verbose = False):\n","    tr_loss, tr_accuracy = 0, 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","    tr_preds, tr_labels = [], []\n","    # put model in training mode\n","    model.train()\n","    \n","    for idx, batch in enumerate(train_loader):\n","        ids, labels = batch\n","        ids = ids.to(device)\n","        labels = labels.to(device)\n","        mask = ids!=1\n","\n","        outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n","        loss = outputs[0]\n","        \n","        tr_logits = outputs[1]\n","        tr_loss += loss.item()\n","\n","        nb_tr_steps += 1\n","        nb_tr_examples += labels.size(0)\n","        \n","        if idx % 100==0 and verbose:\n","            loss_step = tr_loss/nb_tr_steps\n","            print(f\"Training loss per 100 training steps: {loss_step}\")\n","           \n","        # compute training accuracy\n","        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","        \n","        # only compute accuracy at active labels\n","        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","        \n","        labels = torch.masked_select(flattened_targets, active_accuracy)\n","        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","        \n","        tr_labels.extend(labels)\n","        tr_preds.extend(predictions)\n","    \n","        # gradient clipping\n","        torch.nn.utils.clip_grad_norm_(\n","            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n","        )\n","        \n","        # backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        try: \n","            scheduler0.step()\n","        except:\n","            scheduler.step()\n","\n","    epoch_loss = tr_loss / nb_tr_steps\n","    tr_accuracy = tr_accuracy / nb_tr_steps\n","\n","    labels = [ids_to_labels[id.item()] for id in tr_labels]\n","    predictions = [ids_to_labels[id.item()] for id in tr_preds]\n","    f1 = seqeval.metrics.f1_score([labels], [predictions])\n","\n","    print(f\"Training loss epoch: {epoch_loss}\", f\"Training F1 epoch: {f1}\")\n","\n","def valid(model, test_loader, verbose=False):\n","    # put model in evaluation mode\n","    model.eval()\n","    \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_examples, nb_eval_steps = 0, 0\n","    eval_preds, eval_labels = [], []\n","    \n","    with torch.no_grad():\n","        for idx, batch in enumerate(test_loader):\n","            \n","            ids, labels = batch\n","            ids = ids.to(device)\n","            labels = labels.to(device)\n","            mask = ids!=1 \n","\n","            outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n","            loss = outputs[0]\n","            eval_logits = outputs[1]\n","            eval_loss += loss.item()\n","\n","            nb_eval_steps += 1\n","            nb_eval_examples += labels.size(0)\n","        \n","            if idx % 100==0 and verbose:\n","                loss_step = eval_loss/nb_eval_steps\n","                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n","              \n","            # compute evaluation accuracy\n","            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","            \n","            # only compute accuracy at active labels\n","            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","        \n","            labels = torch.masked_select(flattened_targets, active_accuracy)\n","            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","            \n","            eval_labels.extend(labels)\n","            eval_preds.extend(predictions)\n","            \n","\n","    labels = [ids_to_labels[id.item()] for id in eval_labels]\n","    predictions = [ids_to_labels[id.item()] for id in eval_preds]\n","    \n","    eval_loss = eval_loss / nb_eval_steps\n","    eval_accuracy = eval_accuracy / nb_eval_steps\n","    f1 = seqeval.metrics.f1_score([labels], [predictions])\n","    print(f\"Validation Loss: {eval_loss}\", f\"Validation F1: {f1}\")\n","\n","    return labels, predictions, f1\n","\n","def train_crf(epoch, verbose = False):\n","    tr_loss, tr_accuracy = 0, 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","    tr_preds, tr_labels = [], []\n","    model.train()\n","    \n","    for idx, batch in enumerate(train_loader):\n","        ids, labels = batch\n","        ids = ids.to(device)\n","        labels = labels.to(device)\n","        mask = ids!=1\n","        \n","        outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n","        loss = outputs[0]\n","                \n","        tr_loss += loss.item()\n","\n","        nb_tr_steps += 1\n","        nb_tr_examples += labels.size(0)\n","        \n","        if idx % 100==0 and verbose:\n","            loss_step = tr_loss/nb_tr_steps\n","            print(f\"Training loss per 100 training steps: {loss_step}\")\n","           \n","        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","        flattened_predictions = outputs[1].view(-1)\n","        \n","        # only compute accuracy at active labels\n","        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","        active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n","        \n","        labels = torch.masked_select(flattened_targets, active_accuracy)\n","        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","        \n","        tr_labels.extend(labels)\n","        tr_preds.extend(predictions)\n","    \n","        # gradient clipping\n","        torch.nn.utils.clip_grad_norm_(\n","            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n","        )\n","        \n","        # backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    epoch_loss = tr_loss / nb_tr_steps\n","    tr_accuracy = tr_accuracy / nb_tr_steps\n","\n","    labels = [ids_to_labels[id.item()] for id in tr_labels]\n","    predictions = [ids_to_labels[id.item()] for id in tr_preds]\n","    f1 = seqeval.metrics.f1_score([labels], [predictions])\n","\n","    print(f\"Training loss epoch: {epoch_loss}\", f\"Training F1 epoch: {f1}\")\n","\n","def valid_crf(model, test_loader, verbose=False):\n","    # put model in evaluation mode\n","    model.eval()\n","    \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_examples, nb_eval_steps = 0, 0\n","    eval_preds, eval_labels = [], []\n","    \n","    with torch.no_grad():\n","        for idx, batch in enumerate(test_loader):\n","            \n","            ids, labels = batch\n","            ids = ids.to(device)\n","            labels = labels.to(device)\n","            mask = ids!=1 \n","\n","            outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n","            loss = outputs[0]\n","            eval_loss += loss.item()\n","\n","            nb_eval_steps += 1\n","            nb_eval_examples += labels.size(0)\n","        \n","            if idx % 100==0 and verbose:\n","                loss_step = eval_loss/nb_eval_steps\n","                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n","              \n","            # compute evaluation accuracy\n","            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","            flattened_predictions = outputs[1].view(-1)\n","            \n","            # only compute accuracy at active labels\n","            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","            active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n","\n","            labels = torch.masked_select(flattened_targets, active_accuracy)\n","            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","            \n","            eval_labels.extend(labels)\n","            eval_preds.extend(predictions)\n","            \n","\n","    labels = [ids_to_labels[id.item()] for id in eval_labels]\n","    predictions = [ids_to_labels[id.item()] for id in eval_preds]\n","    \n","    eval_loss = eval_loss / nb_eval_steps\n","    # eval_accuracy = eval_accuracy / nb_eval_steps\n","    f1 = seqeval.metrics.f1_score([labels], [predictions])\n","    print(f\"Validation Loss: {eval_loss}\", f\"Validation F1: {f1}\")\n","\n","    return labels, predictions, f1"],"metadata":{"id":"J3Qxsp2gXq3P","executionInfo":{"status":"ok","timestamp":1640248888221,"user_tz":-420,"elapsed":896,"user":{"displayName":"Quang Duy Trần","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvMaEX5IKcYvanQuv2HTQEdjP5mTCjGM4CkVJX=s64","userId":"16019943656023573816"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["## Default Phobert + Linear layer + Tiki dataset"],"metadata":{"id":"hTw8AYfHGszL"}},{"cell_type":"code","source":["class RobertaForTokenClassification(RobertaPreTrainedModel):\n","    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n","    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n","\n","    def __init__(self, config, class_weight=None):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","        self.roberta = RobertaModel(config, add_pooling_layer=False)\n","        classifier_dropout = (\n","            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n","        )\n","        self.dropout = nn.Dropout(classifier_dropout)\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","        # Initialize weights and apply final processing\n","        self.post_init()\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        outputs = self.roberta(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        sequence_output = outputs[0]\n","\n","        sequence_output = self.dropout(sequence_output)\n","        logits = self.classifier(sequence_output)\n","\n","        loss = None\n","        if labels is not None:\n","            if class_weight is not None:\n","                loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(class_weight, dtype=torch.float).to(device))\n","            else: \n","                loss_fct = nn.CrossEntropyLoss()\n","            # Only keep active parts of the loss\n","            if attention_mask is not None:\n","                active_loss = attention_mask.view(-1) == 1\n","                active_logits = logits.view(-1, self.num_labels)\n","                active_labels = torch.where(\n","                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n","                )\n","                loss = loss_fct(active_logits, active_labels)\n","            else:\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","\n","        if not return_dict:\n","            output = (logits,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return TokenClassifierOutput(\n","            loss=loss,\n","            logits=logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )\n","\n","checkpoint_path = './checkpoints/RobertaForTokenClassification_best.pth'\n","model = RobertaForTokenClassification.from_pretrained(\"vinai/phobert-base\", config=config, class_weight=class_weight)\n","model.cuda()"],"metadata":{"id":"_g1hWIJDIezR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","num_train_optimization_steps = int(args.epochs*len(train_dataset)/BATCHSIZE_TRAIN/args.accumulation_steps)\n","optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=num_train_optimization_steps)  # PyTorch scheduler\n","# scheduler0 = get_constant_schedule(optimizer)  # PyTorch scheduler\n","\n","print(\"Learning rate: \", LEARNING_RATE)\n","print(\"num_train_optimization_steps:\", num_train_optimization_steps)\n","\n","tsfm = model.roberta\n","for child in tsfm.children():\n","    for param in child.parameters():\n","        if not param.requires_grad:\n","            print(\"whoopsies\")\n","        param.requires_grad = False\n","frozen = True"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qa1AGCV8QI32","executionInfo":{"status":"ok","timestamp":1640248598046,"user_tz":-420,"elapsed":21,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"fee2c2d4-dd8c-4612-a71e-d5d20714b68d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Learning rate:  5e-05\n","num_train_optimization_steps: 7315\n"]}]},{"cell_type":"code","source":["f1_best = 0\n","for epoch in range(NUM_EPOCH):\n","    if epoch > 0 and frozen:\n","        for child in tsfm.children():\n","            for param in child.parameters():\n","                param.requires_grad = True\n","        frozen = False\n","        try:\n","            del scheduler0\n","        except:\n","            pass\n","        torch.cuda.empty_cache()\n","    st = time.time()\n","    print(f\"Training epoch: {epoch + 1}\")\n","    train(epoch)\n","    labels, predictions, f1_val = valid(model, valid_loader)\n","    if f1_val > f1_best:\n","        f1_best = f1_val\n","        print(f'New best f1 {f1_best}')\n","        print(classification_report([labels], [predictions]))\n","    # save best model\n","    torch.save(model.state_dict(), checkpoint_path)\n","    for param_group in optimizer.param_groups:\n","        print('Current leanring rate: ',param_group['lr'])\n","    print('Time: ',time.time() - st)\n","    print('======================================================')"],"metadata":{"id":"jkqnn-a33hqI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Phobert + LSTM + Linear + Tiki dataset"],"metadata":{"id":"87NrC-3SG1pb"}},{"cell_type":"code","source":["class RobertaLSTM(RobertaPreTrainedModel):\n","    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n","    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n","\n","    def __init__(self, config, class_weight=None):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.roberta = RobertaModel(config, add_pooling_layer=False)\n","        classifier_dropout = (\n","            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n","        )\n","        self.dropout = nn.Dropout(classifier_dropout)\n","        self.bilstm = nn.LSTM(config.hidden_size, (config.hidden_size) // 2, dropout=0.5, batch_first=True, bidirectional=True)\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","\n","        # Initialize weights and apply final processing\n","        self.post_init()\n","\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","        r\"\"\"\n","        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n","            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n","            1]``.\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        outputs = self.roberta(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        sequence_output = outputs[0]\n","\n","        sequence_output = self.dropout(sequence_output)\n","        lstm_output, hc = self.bilstm(sequence_output)\n","        logits = self.classifier(lstm_output)\n","\n","        loss = None\n","        if labels is not None:\n","            if class_weight is not None:\n","                loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(class_weight, dtype=torch.float).to(device))\n","            else: \n","                loss_fct = nn.CrossEntropyLoss()\n","            # Only keep active parts of the loss\n","            if attention_mask is not None:\n","                active_loss = attention_mask.view(-1) == 1\n","                active_logits = logits.view(-1, self.num_labels)\n","                active_labels = torch.where(\n","                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n","                )\n","                loss = loss_fct(active_logits, active_labels)\n","            else:\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","\n","        if not return_dict:\n","            output = (logits,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return TokenClassifierOutput(\n","            loss=loss,\n","            logits=logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )\n","\n","checkpoint_path = './checkpoints/RobertaLSTM.pth'\n","model = RobertaLSTM.from_pretrained(\"vinai/phobert-base\", config=config, class_weight=class_weight)\n","model.to(device)"],"metadata":{"id":"a6YCktfxG8Ii"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","num_train_optimization_steps = int(args.epochs*len(train_dataset)/BATCHSIZE_TRAIN/args.accumulation_steps)\n","optimizer = AdamW(optimizer_grouped_parameters, lr=LEARNING_RATE, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=num_train_optimization_steps)  # PyTorch scheduler\n","# scheduler0 = get_constant_schedule(optimizer)  # PyTorch scheduler\n","\n","print(\"Learning rate: \", LEARNING_RATE)\n","print(\"num_train_optimization_steps:\", num_train_optimization_steps)\n","\n","tsfm = model.roberta\n","for child in tsfm.children():\n","    for param in child.parameters():\n","        if not param.requires_grad:\n","            print(\"whoopsies\")\n","        param.requires_grad = False\n","frozen = True"],"metadata":{"id":"O98Xwy0KQ-o1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640249036142,"user_tz":-420,"elapsed":12,"user":{"displayName":"Quang Duy Trần","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvMaEX5IKcYvanQuv2HTQEdjP5mTCjGM4CkVJX=s64","userId":"16019943656023573816"}},"outputId":"7f3c7dee-eef2-4265-eda8-e3ae5bf336fd"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Learning rate:  5e-05\n","num_train_optimization_steps: 7315\n"]}]},{"cell_type":"code","source":["f1_best = 0\n","for epoch in range(NUM_EPOCH):\n","    if epoch > 0 and frozen:\n","        for child in tsfm.children():\n","            for param in child.parameters():\n","                param.requires_grad = True\n","        frozen = False\n","        try:\n","            del scheduler0\n","        except:\n","            pass\n","        torch.cuda.empty_cache()\n","    st = time.time()\n","    print(f\"Training epoch: {epoch + 1}\")\n","    train(epoch)\n","    labels, predictions, f1_val = valid(model, valid_loader)\n","    if f1_val > f1_best:\n","        f1_best = f1_val\n","        print(f'New best f1 {f1_best}')\n","        print(classification_report([labels], [predictions]))\n","    # save best model\n","    torch.save(model.state_dict(), checkpoint_path)\n","    for param_group in optimizer.param_groups:\n","        print('Current leanring rate: ',param_group['lr'])\n","    print('Time: ',time.time() - st)\n","    print('======================================================')"],"metadata":{"id":"PoljO15xQ-o3","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640253022323,"user_tz":-420,"elapsed":3986184,"user":{"displayName":"Quang Duy Trần","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvMaEX5IKcYvanQuv2HTQEdjP5mTCjGM4CkVJX=s64","userId":"16019943656023573816"}},"outputId":"c436b42b-f06a-4799-f16c-8c05c3fab867"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stdout","text":["Training epoch: 1\n","Training loss epoch: 0.9673119274482049 Training F1 epoch: 0.3007355682858208\n","Validation Loss: 0.5241246931051295 Validation F1: 0.4832965914871969\n","New best f1 0.4832965914871969\n","              precision    recall  f1-score   support\n","\n","         DES       0.47      0.59      0.52      2274\n","         PRI       0.14      0.46      0.22       182\n","\n","   micro avg       0.41      0.58      0.48      2456\n","   macro avg       0.31      0.53      0.37      2456\n","weighted avg       0.45      0.58      0.50      2456\n","\n","Current leanring rate:  4.815661815661816e-05\n","Current leanring rate:  4.815661815661816e-05\n","Time:  92.77245831489563\n","======================================================\n","Training epoch: 2\n","Training loss epoch: 0.366832478877788 Training F1 epoch: 0.6377276167579361\n","Validation Loss: 0.24825116344651238 Validation F1: 0.7122978563369688\n","New best f1 0.7122978563369688\n","              precision    recall  f1-score   support\n","\n","         DES       0.70      0.80      0.74      2274\n","         PRI       0.31      0.47      0.37       182\n","\n","   micro avg       0.66      0.77      0.71      2456\n","   macro avg       0.50      0.63      0.56      2456\n","weighted avg       0.67      0.77      0.72      2456\n","\n","Current leanring rate:  4.562023562023562e-05\n","Current leanring rate:  4.562023562023562e-05\n","Time:  204.84626865386963\n","======================================================\n","Training epoch: 3\n","Training loss epoch: 0.15788454266743418 Training F1 epoch: 0.8190792688886717\n","Validation Loss: 0.26032649317013273 Validation F1: 0.8731446665347318\n","New best f1 0.8731446665347318\n","              precision    recall  f1-score   support\n","\n","         DES       0.87      0.90      0.89      2274\n","         PRI       0.65      0.83      0.73       182\n","\n","   micro avg       0.85      0.90      0.87      2456\n","   macro avg       0.76      0.87      0.81      2456\n","weighted avg       0.85      0.90      0.87      2456\n","\n","Current leanring rate:  4.3083853083853084e-05\n","Current leanring rate:  4.3083853083853084e-05\n","Time:  205.39341402053833\n","======================================================\n","Training epoch: 4\n","Training loss epoch: 0.11714201914960813 Training F1 epoch: 0.8686380016784321\n","Validation Loss: 0.2198037483698887 Validation F1: 0.8586614173228347\n","Current leanring rate:  4.054747054747055e-05\n","Current leanring rate:  4.054747054747055e-05\n","Time:  205.05718994140625\n","======================================================\n","Training epoch: 5\n","Training loss epoch: 0.08152697269697903 Training F1 epoch: 0.8978674524297058\n","Validation Loss: 0.2182173195824312 Validation F1: 0.8033504664001523\n","Current leanring rate:  3.8011088011088013e-05\n","Current leanring rate:  3.8011088011088013e-05\n","Time:  205.16658186912537\n","======================================================\n","Training epoch: 6\n","Training loss epoch: 0.06166008324489568 Training F1 epoch: 0.9225825886133145\n","Validation Loss: 0.21883687874444765 Validation F1: 0.8893726499109439\n","New best f1 0.8893726499109439\n","              precision    recall  f1-score   support\n","\n","         DES       0.87      0.92      0.89      2274\n","         PRI       0.79      0.90      0.84       182\n","\n","   micro avg       0.87      0.91      0.89      2456\n","   macro avg       0.83      0.91      0.87      2456\n","weighted avg       0.87      0.91      0.89      2456\n","\n","Current leanring rate:  3.5474705474705475e-05\n","Current leanring rate:  3.5474705474705475e-05\n","Time:  205.40224933624268\n","======================================================\n","Training epoch: 7\n","Training loss epoch: 0.045996681052957324 Training F1 epoch: 0.9366271741870431\n","Validation Loss: 0.22925119540626132 Validation F1: 0.9001389716100854\n","New best f1 0.9001389716100854\n","              precision    recall  f1-score   support\n","\n","         DES       0.89      0.93      0.91      2274\n","         PRI       0.79      0.90      0.84       182\n","\n","   micro avg       0.88      0.92      0.90      2456\n","   macro avg       0.84      0.91      0.87      2456\n","weighted avg       0.88      0.92      0.90      2456\n","\n","Current leanring rate:  3.293832293832294e-05\n","Current leanring rate:  3.293832293832294e-05\n","Time:  205.32032465934753\n","======================================================\n","Training epoch: 8\n","Training loss epoch: 0.03812623522028103 Training F1 epoch: 0.9482758620689655\n","Validation Loss: 0.22725812984119 Validation F1: 0.8935413786292711\n","Current leanring rate:  3.0401940401940404e-05\n","Current leanring rate:  3.0401940401940404e-05\n","Time:  205.07768416404724\n","======================================================\n","Training epoch: 9\n","Training loss epoch: 0.03408086180491017 Training F1 epoch: 0.9536176232821343\n","Validation Loss: 0.22702115492594982 Validation F1: 0.8929566255471548\n","Current leanring rate:  2.7865557865557868e-05\n","Current leanring rate:  2.7865557865557868e-05\n","Time:  205.30843377113342\n","======================================================\n","Training epoch: 10\n","Training loss epoch: 0.03409900675308916 Training F1 epoch: 0.9539546599496221\n","Validation Loss: 0.23734791144853346 Validation F1: 0.8796078431372549\n","Current leanring rate:  2.532917532917533e-05\n","Current leanring rate:  2.532917532917533e-05\n","Time:  205.05561566352844\n","======================================================\n","Training epoch: 11\n","Training loss epoch: 0.026437429429090435 Training F1 epoch: 0.9597695340139493\n","Validation Loss: 0.26308652225759677 Validation F1: 0.8990969768354927\n","Current leanring rate:  2.2792792792792794e-05\n","Current leanring rate:  2.2792792792792794e-05\n","Time:  204.94051718711853\n","======================================================\n","Training epoch: 12\n","Training loss epoch: 0.023925348687878826 Training F1 epoch: 0.9646142958244869\n","Validation Loss: 0.27301892574209863 Validation F1: 0.9074995026854984\n","New best f1 0.9074995026854984\n","              precision    recall  f1-score   support\n","\n","         DES       0.90      0.93      0.91      2274\n","         PRI       0.79      0.89      0.84       182\n","\n","   micro avg       0.89      0.93      0.91      2456\n","   macro avg       0.84      0.91      0.88      2456\n","weighted avg       0.89      0.93      0.91      2456\n","\n","Current leanring rate:  2.025641025641026e-05\n","Current leanring rate:  2.025641025641026e-05\n","Time:  205.30489492416382\n","======================================================\n","Training epoch: 13\n","Training loss epoch: 0.022596089834477143 Training F1 epoch: 0.9646844844136815\n","Validation Loss: 0.2822211435506161 Validation F1: 0.9128348660535786\n","New best f1 0.9128348660535786\n","              precision    recall  f1-score   support\n","\n","         DES       0.91      0.93      0.92      2274\n","         PRI       0.80      0.92      0.85       182\n","\n","   micro avg       0.90      0.93      0.91      2456\n","   macro avg       0.85      0.92      0.89      2456\n","weighted avg       0.90      0.93      0.91      2456\n","\n","Current leanring rate:  1.772002772002772e-05\n","Current leanring rate:  1.772002772002772e-05\n","Time:  205.69872188568115\n","======================================================\n","Training epoch: 14\n","Training loss epoch: 0.021245919810506244 Training F1 epoch: 0.961846681114863\n","Validation Loss: 0.278327967590828 Validation F1: 0.9154169166166767\n","New best f1 0.9154169166166767\n","              precision    recall  f1-score   support\n","\n","         DES       0.91      0.93      0.92      2274\n","         PRI       0.80      0.91      0.85       182\n","\n","   micro avg       0.90      0.93      0.92      2456\n","   macro avg       0.85      0.92      0.89      2456\n","weighted avg       0.90      0.93      0.92      2456\n","\n","Current leanring rate:  1.5183645183645184e-05\n","Current leanring rate:  1.5183645183645184e-05\n","Time:  205.11326098442078\n","======================================================\n","Training epoch: 15\n","Training loss epoch: 0.014552003705283739 Training F1 epoch: 0.9717181956411556\n","Validation Loss: 0.2855383529252426 Validation F1: 0.9197924980047885\n","New best f1 0.9197924980047885\n","              precision    recall  f1-score   support\n","\n","         DES       0.91      0.94      0.92      2274\n","         PRI       0.83      0.91      0.87       182\n","\n","   micro avg       0.90      0.94      0.92      2456\n","   macro avg       0.87      0.93      0.90      2456\n","weighted avg       0.90      0.94      0.92      2456\n","\n","Current leanring rate:  1.2647262647262647e-05\n","Current leanring rate:  1.2647262647262647e-05\n","Time:  205.4239799976349\n","======================================================\n","Training epoch: 16\n","Training loss epoch: 0.009958065861754649 Training F1 epoch: 0.9769683441558442\n","Validation Loss: 0.28692118803489586 Validation F1: 0.9169488149770961\n","Current leanring rate:  1.0110880110880111e-05\n","Current leanring rate:  1.0110880110880111e-05\n","Time:  204.90126943588257\n","======================================================\n","Training epoch: 17\n","Training loss epoch: 0.010035233700468556 Training F1 epoch: 0.9775161142973152\n","Validation Loss: 0.2980988321617841 Validation F1: 0.9176\n","Current leanring rate:  7.574497574497574e-06\n","Current leanring rate:  7.574497574497574e-06\n","Time:  205.30079698562622\n","======================================================\n","Training epoch: 18\n","Training loss epoch: 0.008241639074314732 Training F1 epoch: 0.9798800934864342\n","Validation Loss: 0.3061020686866873 Validation F1: 0.9209787404733253\n","New best f1 0.9209787404733253\n","              precision    recall  f1-score   support\n","\n","         DES       0.91      0.94      0.92      2274\n","         PRI       0.84      0.92      0.88       182\n","\n","   micro avg       0.91      0.93      0.92      2456\n","   macro avg       0.88      0.93      0.90      2456\n","weighted avg       0.91      0.93      0.92      2456\n","\n","Current leanring rate:  5.038115038115039e-06\n","Current leanring rate:  5.038115038115039e-06\n","Time:  203.45155930519104\n","======================================================\n","Training epoch: 19\n","Training loss epoch: 0.009864313409458897 Training F1 epoch: 0.9780459362165997\n","Validation Loss: 0.3060313267030567 Validation F1: 0.9180982820615262\n","Current leanring rate:  2.5017325017325016e-06\n","Current leanring rate:  2.5017325017325016e-06\n","Time:  203.1371624469757\n","======================================================\n","Training epoch: 20\n","Training loss epoch: 0.008054337283606576 Training F1 epoch: 0.980374211917836\n","Validation Loss: 0.305305505456254 Validation F1: 0.9199359487590072\n","Current leanring rate:  0.0\n","Current leanring rate:  0.0\n","Time:  203.13045287132263\n","======================================================\n"]}]},{"cell_type":"markdown","source":["## Phobert + CRF + Tiki dataset"],"metadata":{"id":"FLfiCckAHPWB"}},{"cell_type":"code","source":["class RobertaCRF(RobertaPreTrainedModel):\n","\n","    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n","    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.roberta = RobertaModel(config, add_pooling_layer=False)\n","        classifier_dropout = (\n","            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n","        )\n","        self.dropout = nn.Dropout(classifier_dropout)\n","        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n","\n","        self.crf = CRF(num_tags=self.num_labels, batch_first=True)\n","        # self.post_init()\n","        self.init_weights()\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","        r\"\"\"\n","        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n","            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n","            1]``.\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        outputs = self.roberta(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        sequence_output = outputs[0]\n","        sequence_output = self.dropout(sequence_output)\n","        logits = self.classifier(sequence_output)\n","\n","        loss = None\n","        if labels is not None:\n","            labels_copy = torch.clone(labels)\n","            labels_copy[labels==-100] = 0\n","            log_likelihood, tags = self.crf(logits, labels_copy), self.crf.decode(logits)\n","            loss = 0 - log_likelihood\n","        else:\n","            tags = self.crf.decode(logits)\n","        tags = torch.Tensor(tags)\n","        tags = tags.to(device)\n","\n","        if not return_dict:\n","            output = (tags,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return loss, tags"],"metadata":{"id":"HFmwsDfeHUJZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = RobertaCRF.from_pretrained('vinai/phobert-base', num_labels=5)\n","model.cuda()"],"metadata":{"id":"Os7ldVmd5P_c"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating optimizer and lr schedulers\n","param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","num_train_optimization_steps = int(args.epochs*len(train_dataset)/args.batch_size/args.accumulation_steps)\n","optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=num_train_optimization_steps)  # PyTorch scheduler\n","scheduler0 = get_constant_schedule(optimizer)  # PyTorch scheduler\n","\n","tsfm = model.roberta\n","for child in tsfm.children():\n","    for param in child.parameters():\n","        if not param.requires_grad:\n","            print(\"whoopsies\")\n","        param.requires_grad = False\n","frozen = True\n","\n","EPOCHS = 10\n","MAX_GRAD_NORM = 10\n","\n","import time\n","\n","for epoch in range(EPOCHS):\n","    if epoch > 0 and frozen:\n","        for child in tsfm.children():\n","            for param in child.parameters():\n","                param.requires_grad = True\n","        frozen = False\n","        del scheduler0\n","        torch.cuda.empty_cache()\n","    st = time.time()\n","    print(f\"Training epoch: {epoch + 1}\")\n","    train_modified(epoch)\n","    labels, predictions = valid_modified(model, valid_loader)\n","    \n","    print('Time: ',time.time() - st)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BhSDwHo98xua","executionInfo":{"status":"ok","timestamp":1640100663683,"user_tz":-420,"elapsed":4056234,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"e6badaa3-1382-481c-dbdb-781889be7a39"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training epoch: 1\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchcrf/__init__.py:249: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  ../aten/src/ATen/native/TensorCompare.cpp:328.)\n","  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"]},{"output_type":"stream","name":"stdout","text":["Training loss epoch: 141.1873681949136 Training F1 epoch: 0.010669715156711442\n","Validation Loss: 30.532176116776597 Validation F1: 0.0016253555465258025\n","Time:  341.531724691391\n","Training epoch: 2\n","Training loss epoch: 16.23040179476712 Training F1 epoch: 0.7982217573221758\n","Validation Loss: 4.768356740148993 Validation F1: 0.8821261919253398\n","Time:  412.92261958122253\n","Training epoch: 3\n","Training loss epoch: 6.580289830275572 Training F1 epoch: 0.9179917626818206\n","Validation Loss: 3.6424914083845628 Validation F1: 0.925264012997563\n","Time:  411.90776658058167\n","Training epoch: 4\n","Training loss epoch: 4.381463306197703 Training F1 epoch: 0.9429851990827601\n","Validation Loss: 3.985554846258111 Validation F1: 0.915184958103413\n","Time:  412.2711532115936\n","Training epoch: 5\n","Training loss epoch: 3.25973218907424 Training F1 epoch: 0.9550573514077164\n","Validation Loss: 3.832001232710041 Validation F1: 0.9256097560975609\n","Time:  412.1608827114105\n","Training epoch: 6\n","Training loss epoch: 3.1167723442035946 Training F1 epoch: 0.9574689878036068\n","Validation Loss: 4.960251938449881 Validation F1: 0.9129116993331987\n","Time:  412.02876687049866\n","Training epoch: 7\n","Training loss epoch: 2.1713440274931695 Training F1 epoch: 0.9666077187142412\n","Validation Loss: 4.823043072809939 Validation F1: 0.9199679871948779\n","Time:  412.3896486759186\n","Training epoch: 8\n","Training loss epoch: 2.553066566342213 Training F1 epoch: 0.962766234442535\n","Validation Loss: 4.004213593696636 Validation F1: 0.9243697478991597\n","Time:  412.5732305049896\n","Training epoch: 9\n","Training loss epoch: 1.9120571782680158 Training F1 epoch: 0.9735499323128189\n","Validation Loss: 4.414583299980789 Validation F1: 0.9320701471477526\n","Time:  414.12650537490845\n","Training epoch: 10\n","Training loss epoch: 1.3469604325424778 Training F1 epoch: 0.9777870259584872\n","Validation Loss: 4.554044462943989 Validation F1: 0.9375513557929335\n","Time:  414.1243920326233\n"]}]},{"cell_type":"markdown","source":["## Phobert + LSTM + CRF + Tiki dataset"],"metadata":{"id":"Y5i4fuQrHUne"}},{"cell_type":"markdown","source":["https://github.com/hemingkx/CLUENER2020/tree/main/BERT-Softmax"],"metadata":{"id":"-YkZxXqfF2vY"}},{"cell_type":"code","source":["class RobertaLSTMCRF(RobertaPreTrainedModel):\n","\n","    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n","    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.roberta = RobertaModel(config, add_pooling_layer=False)\n","        classifier_dropout = (\n","            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n","        )\n","        self.dropout = nn.Dropout(classifier_dropout)\n","        self.bilstm = nn.LSTM(config.hidden_size, (config.hidden_size) // 2, dropout=0.1, batch_first=True, bidirectional=True)\n","        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n","\n","        self.crf = CRF(num_tags=self.num_labels, batch_first=True)\n","        self.post_init()\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","        r\"\"\"\n","        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n","            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n","            1]``.\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        outputs = self.roberta(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        sequence_output = outputs[0]\n","        sequence_output = self.dropout(sequence_output)\n","        lstm_output, hc = self.bilstm(sequence_output)\n","        logits = self.classifier(lstm_output)\n","\n","        loss = None\n","        if labels is not None:\n","            labels[labels==-100] = 0\n","            log_likelihood, tags = self.crf(logits, labels), self.crf.decode(logits)\n","            loss = 0 - log_likelihood\n","        else:\n","            tags = self.crf.decode(logits)\n","        tags = torch.Tensor(tags)\n","        tags = tags.to(device)\n","\n","        if not return_dict:\n","            output = (tags,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return loss, tags"],"metadata":{"id":"U4AjvocHHY12"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = RobertaLSTMCRF.from_pretrained('vinai/phobert-base', num_labels=5)\n","model.cuda()"],"metadata":{"id":"moBnXhdFpb2M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating optimizer and lr schedulers\n","param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","num_train_optimization_steps = int(args.epochs*len(train_dataset)/args.batch_size/args.accumulation_steps)\n","optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=num_train_optimization_steps)  # PyTorch scheduler\n","scheduler0 = get_constant_schedule(optimizer)  # PyTorch scheduler\n","\n","tsfm = model.roberta\n","for child in tsfm.children():\n","    for param in child.parameters():\n","        if not param.requires_grad:\n","            print(\"whoopsies\")\n","        param.requires_grad = False\n","frozen = True\n","\n","EPOCHS = 10\n","MAX_GRAD_NORM = 1\n","\n","import time\n","\n","for epoch in range(EPOCHS):\n","    if epoch > 0 and frozen:\n","        for child in tsfm.children():\n","            for param in child.parameters():\n","                param.requires_grad = True\n","        frozen = False\n","        del scheduler0\n","        torch.cuda.empty_cache()\n","    st = time.time()\n","    print(f\"Training epoch: {epoch + 1}\")\n","    train_modified(epoch)\n","    labels, predictions = valid_modified(model, valid_loader)\n","    \n","    print('Time: ',time.time() - st)"],"metadata":{"id":"Ygafo_w0WzoM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1640106347996,"user_tz":-420,"elapsed":4311859,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"36240787-4c49-46ac-e9ae-17c06cff9fe4"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training loss epoch: 20.55725689663913 Training F1 epoch: 0.6859557171890042\n","Validation Loss: 8.222302045978484 Validation F1: 0.7467652495378928\n","Time:  397.2945246696472\n","Training epoch: 2\n","Training loss epoch: 15.062786915263192 Training F1 epoch: 0.8032530497341257\n","Validation Loss: 5.09008538918417 Validation F1: 0.8728949478748999\n","Time:  473.24821758270264\n","Training epoch: 3\n","Training loss epoch: 8.17585737718259 Training F1 epoch: 0.8943106312292359\n","Validation Loss: 4.090535315008111 Validation F1: 0.9047714918461849\n","Time:  472.9101982116699\n","Training epoch: 4\n","Training loss epoch: 5.621717192436177 Training F1 epoch: 0.9266417290108062\n","Validation Loss: 3.6883598285946038 Validation F1: 0.9090909090909091\n","Time:  472.8960266113281\n","Training epoch: 5\n","Training loss epoch: 4.464758409177019 Training F1 epoch: 0.9421178426686064\n","Validation Loss: 3.531988592095714 Validation F1: 0.915011263567479\n","Time:  471.5756878852844\n","Training epoch: 6\n","Training loss epoch: 3.35708157482043 Training F1 epoch: 0.9528365659612085\n","Validation Loss: 4.023460012967469 Validation F1: 0.9135702746365106\n","Time:  471.38697576522827\n","Training epoch: 7\n","Training loss epoch: 2.712613725922798 Training F1 epoch: 0.9606029106029107\n","Validation Loss: 4.4713851845329575 Validation F1: 0.9121054734318818\n","Time:  472.0966091156006\n","Training epoch: 8\n","Training loss epoch: 2.5252524301654002 Training F1 epoch: 0.9629513997294203\n","Validation Loss: 3.8572265124711835 Validation F1: 0.9115508885298869\n","Time:  471.18241119384766\n","Training epoch: 9\n","Training loss epoch: 2.021754098150248 Training F1 epoch: 0.9709586759654418\n","Validation Loss: 4.1755393241924015 Validation F1: 0.9247311827956989\n","Time:  471.5348255634308\n","Training epoch: 10\n","Training loss epoch: 1.8250824066408264 Training F1 epoch: 0.9730123238521138\n","Validation Loss: 3.7752516961814275 Validation F1: 0.9343598055105348\n","Time:  469.37305521965027\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"1tl_ObVfpWv4"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Temp\n"],"metadata":{"id":"7k1WqTO59A5Z"}},{"cell_type":"code","source":["from vncorenlp import VnCoreNLP\n","\n","# To perform word segmentation, POS tagging, NER and then dependency parsing\n","annotator = VnCoreNLP(\"/content/VnCoreNLP/VnCoreNLP-1.1.1.jar\", annotators=\"wseg,pos,ner,parse\", max_heap_size='-Xmx2g') \n"," \n","    \n","# Input \n","text = \"Ông Nguyễn Khắc Chúc  đang làm việc tại Đại học Quốc gia Hà Nội. Bà Lan, vợ ông Chúc, cũng làm việc tại đây.\"\n","\n","# To perform word segmentation, POS tagging, NER and then dependency parsing\n","annotated_text = annotator.annotate(text)\n","\n","# To perform word segmentation only\n","word_segmented_text = annotator.tokenize(text) \n"],"metadata":{"id":"mYzxKxlX9CYp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Input \n","text = \"thô lỗ\"\n","\n","# To perform word segmentation, POS tagging, NER and then dependency parsing\n","annotated_text = annotator.annotate(text)\n","\n","# To perform word segmentation only\n","word_segmented_text = annotator.tokenize(text) \n","word_segmented_text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"32MdsTte-B-R","executionInfo":{"status":"ok","timestamp":1640146896575,"user_tz":-420,"elapsed":405,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"}},"outputId":"56323918-7024-491d-b21d-a07ae9a78c5a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['thô_lỗ']]"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":["lines = ['mua được giá tốt lại được freeship mừng rơi nước_mắt đối_với tỉnh_lẻ thì tiền ship là 1 trở_ngại sản_phẩm quá ổn cảm_ơn shop cảm_ơn tiki this is english sentences cảm_ơn'] \n","tags = ['O,O,B-PRI,O,O,O,B-PRI,O,O,O,O,O,O,B-PRI,I-PRI,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O']\n","\n","ids, labels, masks = convert_lines(lines, tags, vocab, bpe, labels_to_ids)\n","for item in zip(ids[0], labels[0], masks[0]):\n","    print(f'{item[0]} \\t {item[1]} \\t {item[2]}')"],"metadata":{"id":"eSobWje-MLGt"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"neLtVN_fXfeM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Data - legacy\n","\n"],"metadata":{"id":"CAnYaObi9HXf"}},{"cell_type":"code","source":["# data_path = './data/seq_tag/tokens_labeled_final.xlsx'\n","data_path = './data/seq_tag/tokens_labeled_no_whitelist.csv'\n","data = prepare_dataset(data_path)\n","data.head(10)"],"metadata":{"id":"Lx7keeHx_A0F"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["path = data_path\n","if path.split('.')[-1]=='xlsx':\n","    with open(path, 'rb') as f:\n","        data = pd.read_excel(f)\n","else:\n","    data = pd.read_csv(path, encoding='utf-8')\n","data.drop(columns=['Unnamed: 0'], inplace=True)\n","\n","data.rename(columns={'sentence': 'Sentence #', 'tokens': 'Word', 'tag': 'Tag'}, inplace=True)\n","print(data.shape)\n","label = pd.read_csv('./data/seq_tag/tokens_labeled_sentiment.csv')\n","\n","print(label.shape)\n","# data['label'] =\n"],"metadata":{"id":"Sj2LlnvoQX_A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data['Sentence #'] = data['Sentence #'].apply(lambda x: f'Sentence: {int(x+1)}')\n","\n","print(\"Number of tags: {}\".format(len(data.Tag.unique())))\n","\n","frequencies = data.Tag.value_counts()\n","tags = {}\n","for tag, count in zip(frequencies.index, frequencies):\n","    if tag != \"O\":\n","        if tag[2:5] not in tags.keys():\n","            tags[tag[2:5]] = count\n","        else:\n","            tags[tag[2:5]] += count\n","    continue\n","\n","print(sorted(tags.items(), key=lambda x: x[1], reverse=True))\n","\n","labels_to_ids = {k: v for v, k in enumerate(data.Tag.unique())}\n","ids_to_labels = {v: k for v, k in enumerate(data.Tag.unique())}\n","print(labels_to_ids)\n","\n","data = data.fillna(method='ffill')\n","print(data)\n","if path.split('.')[-1]=='csv':\n","    data['sentence'] = data[['Sentence #','Word','Tag', 'label']].groupby(['Sentence #'])['Word'].transform(lambda x: ' '.join(x))\n","    data['word_labels'] = data[['Sentence #','Word','Tag', 'label']].groupby(['Sentence #'])['Tag'].transform(lambda x: ','.join(x))\n","if path.split('.')[-1]=='xlsx':\n","    data['sentence'] = data.groupby(['Sentence #'])['Word'].transform(lambda x: ' '.join(str(v) for v in x))\n","    data['word_labels'] = data.groupby(['Sentence #'])['Tag'].transform(lambda x: ','.join(str(v) for v in x))\n","    # data['label'] = data.groupby(['Sentence #'])['label'].mean()\n","data = data.drop_duplicates(subset='sentence').reset_index(drop=True)\n"],"metadata":{"id":"Jl6IRWjWZxDI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["label = []\n","for sent in data['Sentence #']:\n","    sent_id = int(sent.split(' ')[-1])\n","    label.append(df.label.iloc[sent_id-1])"],"metadata":{"id":"DfwcqFZBddmY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["df = pd.read_excel('./data/clean/final6.xlsx')\n","df.drop(columns='Unnamed: 0', inplace=True)\n","df"],"metadata":{"id":"gMgiFxq-dUT4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data['label'] = label"],"metadata":{"id":"5SXWwbpieH2e"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data.drop(columns=['Sentence #','Word','Tag'], inplace=True)\n","data.drop_duplicates(inplace=True)\n","data"],"metadata":{"id":"A0whzbX0eLJO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.to_csv('./data/joint/data_joint.csv')"],"metadata":{"id":"r4o7Ymn2enJ_"},"execution_count":null,"outputs":[]}]}