{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20040,"status":"ok","timestamp":1640027181395,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"},"user_tz":-420},"id":"j4yH5oIECEv7","outputId":"2c254b89-da65-4cf6-c2b5-3bbc4729f22f"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive._mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":525,"status":"ok","timestamp":1640027181917,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"},"user_tz":-420},"id":"nA2kNk4ULadb","outputId":"867f9e96-b74c-49ec-ddbe-679d43aa78b5"},"outputs":[{"name":"stdout","output_type":"stream","text":["/content/drive/MyDrive/NLP/project_nlp\n"]}],"source":["%cd /content/drive/MyDrive/NLP/project_nlp"]},{"cell_type":"markdown","metadata":{"id":"tW-3xdHzjf7v"},"source":["### Environment"]},{"cell_type":"markdown","metadata":{"id":"6rAvHW6ti7-g"},"source":["#### Install dependency"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28433,"status":"ok","timestamp":1640027210348,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"},"user_tz":-420},"id":"j-sgUbzBXPZK","outputId":"d75cb2a6-f9ab-44dc-82a3-b4a881f272ab"},"outputs":[{"name":"stdout","output_type":"stream","text":["\u001b[K     |████████████████████████████████| 3.4 MB 10.0 MB/s \n","\u001b[K     |████████████████████████████████| 43 kB 1.4 MB/s \n","\u001b[K     |████████████████████████████████| 61 kB 451 kB/s \n","\u001b[K     |████████████████████████████████| 596 kB 51.1 MB/s \n","\u001b[K     |████████████████████████████████| 895 kB 50.2 MB/s \n","\u001b[K     |████████████████████████████████| 3.3 MB 26.7 MB/s \n","\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |████████████████████████████████| 1.7 MB 8.4 MB/s \n","\u001b[K     |████████████████████████████████| 145 kB 50.9 MB/s \n","\u001b[K     |████████████████████████████████| 90 kB 8.7 MB/s \n","\u001b[K     |████████████████████████████████| 74 kB 2.8 MB/s \n","\u001b[K     |████████████████████████████████| 112 kB 52.8 MB/s \n","\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for fastBPE (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}],"source":["!pip install transformers seqeval[gpu] -q\n","!pip install fairseq -q\n","!pip install fastBPE -q"]},{"cell_type":"markdown","metadata":{"id":"wXy6Wq5rjFul"},"source":["#### Import libs and check environment"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IEnlUbgm8z3B"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.metrics import accuracy_score\n","import re\n","\n","import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader\n","\n","from transformers import BertTokenizerFast, BertConfig, BertForTokenClassification\n","from transformers import RobertaModel, RobertaConfig, BertPreTrainedModel, RobertaForTokenClassification\n","from transformers.modeling_outputs  import TokenClassifierOutput\n","\n","from torch.utils.data import TensorDataset\n","\n","import seqeval\n","from seqeval.metrics import classification_report, f1_score"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1640027219140,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"},"user_tz":-420},"id":"Sm1krxJtKxpx","outputId":"b290ef9b-5780-4b5b-9992-5329844537c3"},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]}],"source":["from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","print(device)"]},{"cell_type":"markdown","metadata":{"id":"LIXPEAKxjjq9"},"source":["### Dataset"]},{"cell_type":"markdown","metadata":{"id":"XH9amch_jNsJ"},"source":["#### Read dataframe"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":1550,"status":"ok","timestamp":1640027220686,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"},"user_tz":-420},"id":"VySdvWqlbYuR","outputId":"428b2e1d-7d88-4a7c-d0df-4078a09741c0"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-ca552c1f-aa3c-4fc6-810d-5b59e32cfeae\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence #</th>\n","      <th>Word</th>\n","      <th>Tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Sentence: 1</td>\n","      <td>combo</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Sentence: 1</td>\n","      <td>3</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Sentence: 1</td>\n","      <td>cái</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Sentence: 1</td>\n","      <td>giao</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Sentence: 1</td>\n","      <td>có</td>\n","      <td>O</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ca552c1f-aa3c-4fc6-810d-5b59e32cfeae')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-ca552c1f-aa3c-4fc6-810d-5b59e32cfeae button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-ca552c1f-aa3c-4fc6-810d-5b59e32cfeae');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["    Sentence #   Word Tag\n","0  Sentence: 1  combo   O\n","1  Sentence: 1      3   O\n","2  Sentence: 1    cái   O\n","3  Sentence: 1   giao   O\n","4  Sentence: 1     có   O"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["data = pd.read_csv(\"./data/seq_tag/tokens_labeled_no_whitelist_col_price.csv\", encoding='utf-8')\n","data.drop(columns=['Unnamed: 0'], inplace=True)\n","data.rename(columns={'sentence': 'Sentence #', 'tokens': 'Word', 'tag': 'Tag'}, inplace=True)\n","data['Sentence #'] = data['Sentence #'].apply(lambda x: f'Sentence: {int(x+1)}')\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":39,"status":"ok","timestamp":1640027220687,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"},"user_tz":-420},"id":"qfU7IHjfEOlV","outputId":"24828bf7-81f6-431e-b806-e29b648710af"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'pandas.core.frame.DataFrame'>\n","RangeIndex: 89860 entries, 0 to 89859\n","Data columns (total 3 columns):\n"," #   Column      Non-Null Count  Dtype \n","---  ------      --------------  ----- \n"," 0   Sentence #  89860 non-null  object\n"," 1   Word        89860 non-null  object\n"," 2   Tag         89860 non-null  object\n","dtypes: object(3)\n","memory usage: 2.1+ MB\n"]}],"source":["data.info()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":33,"status":"ok","timestamp":1640027220688,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"},"user_tz":-420},"id":"6gMibEJXTKDw","outputId":"be74f99d-5b54-498c-cea1-fb1e9727e194"},"outputs":[{"data":{"text/plain":["Sentence #    89860\n","Word          89860\n","Tag           89860\n","dtype: int64"]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["data.count()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":30,"status":"ok","timestamp":1640027220689,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"},"user_tz":-420},"id":"76iAeu2_XPZb","outputId":"94f3d7eb-1580-4fc0-a821-70d35aacc6f3"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of tags: 5\n"]},{"data":{"text/plain":["O        86762\n","B-COL     1145\n","B-PRI      985\n","I-PRI      496\n","I-COL      472\n","Name: Tag, dtype: int64"]},"execution_count":9,"metadata":{},"output_type":"execute_result"}],"source":["print(\"Number of tags: {}\".format(len(data.Tag.unique())))\n","frequencies = data.Tag.value_counts()\n","frequencies"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":26,"status":"ok","timestamp":1640027220690,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"},"user_tz":-420},"id":"LTFTa17UXPZf","outputId":"d6f8bb79-4e64-40c4-bcd0-981cfb84f47b"},"outputs":[{"name":"stdout","output_type":"stream","text":["[('COL', 1617), ('PRI', 1481)]\n"]}],"source":["tags = {}\n","for tag, count in zip(frequencies.index, frequencies):\n","    if tag != \"O\":\n","        if tag[2:5] not in tags.keys():\n","            tags[tag[2:5]] = count\n","        else:\n","            tags[tag[2:5]] += count\n","    continue\n","\n","print(sorted(tags.items(), key=lambda x: x[1], reverse=True))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23,"status":"ok","timestamp":1640027220691,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"},"user_tz":-420},"id":"CFRDM8WsQXvL","outputId":"d2d804e7-eff3-4328-c7fb-932dcd8bca94"},"outputs":[{"data":{"text/plain":["{'B-COL': 1, 'B-PRI': 3, 'I-COL': 2, 'I-PRI': 4, 'O': 0}"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["labels_to_ids = {k: v for v, k in enumerate(data.Tag.unique())}\n","ids_to_labels = {v: k for v, k in enumerate(data.Tag.unique())}\n","labels_to_ids"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":677},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1640027220692,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"},"user_tz":-420},"id":"zkW2vNcO-uMH","outputId":"60a0b76d-6961-4ba3-9f14-7d3d2eda7aa8"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-2a2b2394-70df-4766-bde3-35a11e1530e1\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence #</th>\n","      <th>Word</th>\n","      <th>Tag</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Sentence: 1</td>\n","      <td>combo</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Sentence: 1</td>\n","      <td>3</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Sentence: 1</td>\n","      <td>cái</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Sentence: 1</td>\n","      <td>giao</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Sentence: 1</td>\n","      <td>có</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>Sentence: 1</td>\n","      <td>1</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>Sentence: 1</td>\n","      <td>cái</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>Sentence: 1</td>\n","      <td>,</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>Sentence: 1</td>\n","      <td>thành_ra</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>Sentence: 1</td>\n","      <td>đặt</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>Sentence: 1</td>\n","      <td>6</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>Sentence: 1</td>\n","      <td>cái</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>Sentence: 1</td>\n","      <td>nhận</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>Sentence: 1</td>\n","      <td>được</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>14</th>\n","      <td>Sentence: 1</td>\n","      <td>4</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>Sentence: 1</td>\n","      <td>,</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>Sentence: 1</td>\n","      <td>hàng</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>Sentence: 1</td>\n","      <td>thì</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>Sentence: 1</td>\n","      <td>vải</td>\n","      <td>O</td>\n","    </tr>\n","    <tr>\n","      <th>19</th>\n","      <td>Sentence: 1</td>\n","      <td>xấu</td>\n","      <td>O</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2a2b2394-70df-4766-bde3-35a11e1530e1')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2a2b2394-70df-4766-bde3-35a11e1530e1 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2a2b2394-70df-4766-bde3-35a11e1530e1');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["     Sentence #      Word Tag\n","0   Sentence: 1     combo   O\n","1   Sentence: 1         3   O\n","2   Sentence: 1       cái   O\n","3   Sentence: 1      giao   O\n","4   Sentence: 1        có   O\n","5   Sentence: 1         1   O\n","6   Sentence: 1       cái   O\n","7   Sentence: 1         ,   O\n","8   Sentence: 1  thành_ra   O\n","9   Sentence: 1       đặt   O\n","10  Sentence: 1         6   O\n","11  Sentence: 1       cái   O\n","12  Sentence: 1      nhận   O\n","13  Sentence: 1      được   O\n","14  Sentence: 1         4   O\n","15  Sentence: 1         ,   O\n","16  Sentence: 1      hàng   O\n","17  Sentence: 1       thì   O\n","18  Sentence: 1       vải   O\n","19  Sentence: 1       xấu   O"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# pandas has a very handy \"forward fill\" function to fill missing values based on the last upper non-nan value\n","data = data.fillna(method='ffill')\n","data.head(20)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":354},"executionInfo":{"elapsed":3368,"status":"ok","timestamp":1640027224043,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"},"user_tz":-420},"id":"Hmd-ow389k6Y","outputId":"2cc03f20-4270-453a-a8ff-ad025d8dd5c6"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-f2490bfd-5f15-43ce-8837-89b30649e33c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Sentence #</th>\n","      <th>Word</th>\n","      <th>Tag</th>\n","      <th>sentence</th>\n","      <th>word_labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Sentence: 1</td>\n","      <td>combo</td>\n","      <td>O</td>\n","      <td>combo 3 cái giao có 1 cái , thành_ra đặt 6 cái...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Sentence: 1</td>\n","      <td>3</td>\n","      <td>O</td>\n","      <td>combo 3 cái giao có 1 cái , thành_ra đặt 6 cái...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>Sentence: 1</td>\n","      <td>cái</td>\n","      <td>O</td>\n","      <td>combo 3 cái giao có 1 cái , thành_ra đặt 6 cái...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Sentence: 1</td>\n","      <td>giao</td>\n","      <td>O</td>\n","      <td>combo 3 cái giao có 1 cái , thành_ra đặt 6 cái...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>Sentence: 1</td>\n","      <td>có</td>\n","      <td>O</td>\n","      <td>combo 3 cái giao có 1 cái , thành_ra đặt 6 cái...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f2490bfd-5f15-43ce-8837-89b30649e33c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-f2490bfd-5f15-43ce-8837-89b30649e33c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-f2490bfd-5f15-43ce-8837-89b30649e33c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["    Sentence #  ...                                        word_labels\n","0  Sentence: 1  ...  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n","1  Sentence: 1  ...  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n","2  Sentence: 1  ...  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n","3  Sentence: 1  ...  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n","4  Sentence: 1  ...  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n","\n","[5 rows x 5 columns]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# let's create a new column called \"sentence\" which groups the words by sentence \n","data['sentence'] = data[['Sentence #','Word','Tag']].groupby(['Sentence #'])['Word'].transform(lambda x: ' '.join(x))\n","# let's also create a new column called \"word_labels\" which groups the tags by sentence \n","data['word_labels'] = data[['Sentence #','Word','Tag']].groupby(['Sentence #'])['Tag'].transform(lambda x: ','.join(x))\n","data.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":468},"executionInfo":{"elapsed":34,"status":"ok","timestamp":1640027224045,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"},"user_tz":-420},"id":"SrEgd4PZUgmF","outputId":"78d5e5e3-3a87-4c47-d213-9fb8f7b16494"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-c2b5bd15-5167-493c-92e4-5b5f4f724cb5\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","      <th>word_labels</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>combo 3 cái giao có 1 cái , thành_ra đặt 6 cái...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>mình mua áo có cổ màu trắng lại ship tới cho m...</td>\n","      <td>O,O,O,O,O,B-COL,I-COL,O,O,O,O,O,O,O,O,B-COL,I-...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>giao sai hàng . tôi muốn trả hàng . đặt be đậm...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,B-COL,I-COL,O,B-COL,I-COL</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>sản_xuất việt_nam nhưng thấy in chữ trung_quốc...</td>\n","      <td>O,O,O,O,B-COL,I-COL,O,O,O,O,O,O,O</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>mình đặt áo sơ_mi trắng dài tay mà shop giao c...</td>\n","      <td>O,O,O,O,B-COL,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3652</th>\n","      <td>chất_lượng sản_phẩm giống mô tả.giao hàng nhan...</td>\n","      <td>O,O,O,O,O,O,O,O</td>\n","    </tr>\n","    <tr>\n","      <th>3653</th>\n","      <td>hài_lòng vô_cùng , giao nhanh , nhân_viên giao...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O</td>\n","    </tr>\n","    <tr>\n","      <th>3654</th>\n","      <td>sản_phẩm ổn . giá phải_chăng . thật_sự là nhận...</td>\n","      <td>O,O,O,B-PRI,B-PRI,O,O,O,O,O,O,O,O,O,O,O,O,O</td>\n","    </tr>\n","    <tr>\n","      <th>3655</th>\n","      <td>hàng đẹp chuẩn chất_lượng . nếu áo có đai ngan...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O</td>\n","    </tr>\n","    <tr>\n","      <th>3656</th>\n","      <td>vải mặc mát , dày_dặn . không có miếng mút ngự...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3657 rows × 2 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c2b5bd15-5167-493c-92e4-5b5f4f724cb5')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c2b5bd15-5167-493c-92e4-5b5f4f724cb5 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c2b5bd15-5167-493c-92e4-5b5f4f724cb5');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["                                               sentence                                        word_labels\n","0     combo 3 cái giao có 1 cái , thành_ra đặt 6 cái...  O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n","1     mình mua áo có cổ màu trắng lại ship tới cho m...  O,O,O,O,O,B-COL,I-COL,O,O,O,O,O,O,O,O,B-COL,I-...\n","2     giao sai hàng . tôi muốn trả hàng . đặt be đậm...      O,O,O,O,O,O,O,O,O,O,B-COL,I-COL,O,B-COL,I-COL\n","3     sản_xuất việt_nam nhưng thấy in chữ trung_quốc...                  O,O,O,O,B-COL,I-COL,O,O,O,O,O,O,O\n","4     mình đặt áo sơ_mi trắng dài tay mà shop giao c...  O,O,O,O,B-COL,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,...\n","...                                                 ...                                                ...\n","3652  chất_lượng sản_phẩm giống mô tả.giao hàng nhan...                                    O,O,O,O,O,O,O,O\n","3653  hài_lòng vô_cùng , giao nhanh , nhân_viên giao...                        O,O,O,O,O,O,O,O,O,O,O,O,O,O\n","3654  sản_phẩm ổn . giá phải_chăng . thật_sự là nhận...        O,O,O,B-PRI,B-PRI,O,O,O,O,O,O,O,O,O,O,O,O,O\n","3655  hàng đẹp chuẩn chất_lượng . nếu áo có đai ngan...                    O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O\n","3656  vải mặc mát , dày_dặn . không có miếng mút ngự...      O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O\n","\n","[3657 rows x 2 columns]"]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["data = data[[\"sentence\", \"word_labels\"]].drop_duplicates().reset_index(drop=True)\n","data"]},{"cell_type":"markdown","metadata":{"id":"xg3Fz7_NjsD7"},"source":["#### Expand the labels with subword based tokenizer"]},{"cell_type":"markdown","metadata":{"id":"OE6nV8ydL_Zy"},"source":["*PhoBERT sử dụng RDRSegmenter của VnCoreNLP (đã thực hiện trong dataframe) để tách từ cho dữ liệu đầu vào trước khi qua BPE encoder.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a5p-6_eHLuqW"},"outputs":[],"source":["from fairseq.data.encoders.fastbpe import fastBPE\n","from fairseq.data import Dictionary\n","import argparse\n","\n","parser = argparse.ArgumentParser()\n","parser.add_argument('--bpe-codes', \n","    default=\"./PhoBERT_base_transformers/bpe.codes\",\n","    required=False,\n","    type=str,\n","    help='path to fastBPE BPE'\n",")\n","args, unknown = parser.parse_known_args()\n","bpe = fastBPE(args)\n","\n","# Load the dictionary\n","vocab = Dictionary()\n","vocab.add_from_file(\"./PhoBERT_base_transformers/dict.txt\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0_RR_SSV5cqa"},"outputs":[],"source":["labels_to_ids['X'] = -100"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1640027225153,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"},"user_tz":-420},"id":"hpmdI2IP7kmY","outputId":"b891995e-d541-47ab-ddb4-bcaee1de1313"},"outputs":[{"data":{"text/plain":["{'B-COL': 1, 'B-PRI': 3, 'I-COL': 2, 'I-PRI': 4, 'O': 0, 'X': -100}"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["labels_to_ids"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":502,"status":"ok","timestamp":1640027225649,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"},"user_tz":-420},"id":"09vbPI7Yj57U","outputId":"35fd72f5-45ed-48c9-ac78-e172fe72566c"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 1/1 [00:00<00:00, 15.26it/s]"]},{"name":"stdout","output_type":"stream","text":["input_ids tensor encode: [[    0   188    11   133   167    44    11 18288  2438 56679  2766   891\n","   2396   190 27159    54   123 16132     8    99  5769   265   204  4752\n","   2321  9405  2321  2081  5418 22304  2573 15601  2455 14641  1302  6502\n","  26442     2     1     1     1     1     1     1     1     1     1     1\n","      1     1     1     1     1     1     1     1     1     1     1     1\n","      1     1     1     1     1     1     1     1     1     1     1     1\n","      1     1     1     1     1     1     1     1     1     1     1     1\n","      1     1     1     1     1     1     1     1     1     1     1     1\n","      1     1     1     1     1     1     1     1     1     1     1     1\n","      1     1     1     1     1     1     1     1     1     1     1     1\n","      1     1     1     1     1     1     1     1     1     1     1     1\n","      1     1     1     1     1     1     1     1     1     1     1     1\n","      1     1     1     1     1     1     1     1     1     1     1     1\n","      1     1     1     1     1     1     1     1     1     1     1     1\n","      1     1     1     1     1     1     1     1     1     1     1     1\n","      1     1     1     1     1     1     1     1     1     1     1     1\n","      1     1     1     1     1     1     1     1     1     1     1     1\n","      1     1     1     1     1     1     1     1     1     1     1     1\n","      1     1     1     1     1     1     1     1     1     1     1     1\n","      1     1     1     1     1     1     1     1     1     1     1     1\n","      1     1     1     1     1     1     1     1     1     1     1     1\n","      1     1     1     1]]\n",", shape: 256\n","\n","label_ids tensor encode: [[-100    0    0    3    0    0    0    3 -100 -100    0    0    0    0\n","     0    0    3    4    0    0    0    0    0    0    0    0    0    0\n","  -100    0    0    0 -100 -100    0 -100 -100    0 -100 -100 -100 -100\n","  -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n","  -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n","  -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n","  -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n","  -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n","  -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n","  -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n","  -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n","  -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n","  -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n","  -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n","  -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n","  -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n","  -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n","  -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100 -100\n","  -100 -100 -100 -100]]\n",", shape: 256\n","\n","masks tensor encode: [[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n","  1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n","  0 0 0 0]]\n",", shape: 256\n","\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["from tqdm import tqdm\n","def convert_lines(lines, tags, vocab, bpe, max_sequence_length=256):\n","    \"\"\"\n","    lines: list các văn bản input\n","    tags: list các chuỗi tag\n","    vocab: từ điển dùng để encoding subwords\n","    bpe: \n","    \"\"\"\n","    # Index của các token cls (đầu câu), eos (cuối câu), padding (padding token)\n","    outputs = np.zeros((len(lines), max_sequence_length), dtype=np.int32) # --> shape (number_lines, max_seq_len)\n","    outputs_labels = np.zeros((len(lines), max_sequence_length), dtype=np.int32)\n","    outputs_attention_mask = np.zeros((len(lines), max_sequence_length), dtype=np.int32)\n","    # Index của các token cls (đầu câu), eos (cuối câu), padding (padding token)\n","    cls_id = 0\n","    eos_id = 2\n","    pad_id = 1\n","    \n","    for idx, row in tqdm(enumerate(lines), total=len(lines)): \n","        # Mã hóa subwords theo byte pair encoding(bpe)\n","        subwords = bpe.encode(row)\n","        subwords = '<s> '+ subwords +' </s>'\n","        input_ids = vocab.encode_line(subwords, append_eos=False, add_if_not_exist=False).long().tolist()\n","        \n","        tag_list = ['O'] + tags[idx].split(',') + ['O']\n","        subword_idx = [subwords.split().index(word) for word in subwords.split() if '@@' in word]\n","        for i, orig_idx in enumerate(subword_idx):\n","            tag_list.insert(orig_idx+1, 'X')\n","        # print(tag_list)\n","        labels = [labels_to_ids[label] for label in tag_list] \n","\n","        # Truncate input nếu độ dài vượt quá max_seq_len\n","        if len(input_ids) > max_sequence_length: \n","            input_ids = input_ids[:max_sequence_length]\n","            input_ids[-1] = eos_id\n","            labels = labels[:max_sequence_length]\n","            labels[-1] = -100\n","        else:\n","        # Padding nếu độ dài câu chưa bằng max_seq_len\n","            input_ids = input_ids + [pad_id, ]*(max_sequence_length - len(input_ids))\n","            labels = labels + [-100, ]*(max_sequence_length - len(labels))\n","        \n","        labels[0] = -100\n","        outputs[idx,:] = np.array(input_ids)\n","        outputs_labels[idx,:] = np.array(labels)\n","        outputs_attention_mask[idx, np.array(input_ids)!=pad_id] = 1\n","\n","    return outputs, outputs_labels, outputs_attention_mask\n","\n","lines = ['mua được giá tốt lại được freeship mừng rơi nước_mắt đối_với tỉnh_lẻ thì tiền ship là 1 trở_ngại sản_phẩm quá ổn cảm_ơn shop cảm_ơn tiki this is english sentences'] \n","tags = ['O,O,B-PRI,O,O,O,B-PRI,O,O,O,O,O,O,B-PRI,I-PRI,O,O,O,O,O,O,O,O,O,O,O,O,O,O']\n","\n","ids, labels, masks = convert_lines(lines, tags, vocab, bpe)\n","print('input_ids tensor encode: {}\\n, shape: {}\\n'.format(ids[:10], ids.size))\n","print('label_ids tensor encode: {}\\n, shape: {}\\n'.format(labels[:10], labels.size))\n","print('masks tensor encode: {}\\n, shape: {}\\n'.format(masks[:10], masks.size))\n","# print('x1 tensor decode: ', phoBERT_cls.decode(torch.tensor(x1))[:103])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lgNSM8Xz79Mg"},"outputs":[],"source":["MAX_LEN = 256\n","TRAIN_BATCH_SIZE = 4\n","VALID_BATCH_SIZE = 2\n","EPOCHS = 1\n","LEARNING_RATE = 1e-05\n","MAX_GRAD_NORM = 10"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1150,"status":"ok","timestamp":1640027226783,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"},"user_tz":-420},"id":"a1qgmZJX9t-x","outputId":"5fa598d0-a84e-496b-e2fd-d51c10fd6661"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 3657/3657 [00:01<00:00, 2593.87it/s]"]},{"name":"stdout","output_type":"stream","text":["X shape:  (3657, 256)\n","Y label shape (3657, 256)\n","Y mask shape (3657, 256)\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["X, Y_label, Y_mask = convert_lines(data.sentence.values, data.word_labels.values, vocab, bpe, max_sequence_length=MAX_LEN)\n","print('X shape: ', X.shape)\n","print('Y label shape', Y_label.shape)\n","print('Y mask shape', Y_mask.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cDL0RTsU-fzG"},"outputs":[],"source":["import pickle\n","\n","def _save_pkl(path, obj):\n","  with open(path, 'wb') as f:\n","    pickle.dump(obj, f)\n","\n","def _load_pkl(path):\n","  with open(path, 'rb') as f:\n","    obj = pickle.load(f)\n","  return obj\n","\n","_save_pkl('./data/processed/X_col.pkl', X)\n","_save_pkl('./data/processed/Y_label_col.pkl', Y_label)\n","_save_pkl('./data/processed/Y_mask_col.pkl', Y_mask)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1640027228193,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"},"user_tz":-420},"id":"K7noI0nS-hwg","outputId":"246db35b-8017-48fd-e93a-993e13c6fb0e"},"outputs":[{"name":"stdout","output_type":"stream","text":["length of X:  3657\n","length of y:  3657\n","length of y:  3657\n"]}],"source":["X = _load_pkl('./data/processed/X_col.pkl')\n","Y_label = _load_pkl('./data/processed/Y_label_col.pkl')\n","Y_mask = _load_pkl('./data/processed/Y_mask_col.pkl')\n","\n","print('length of X: ', len(X))\n","print('length of y: ', len(Y_label))\n","print('length of y: ', len(Y_mask))"]},{"cell_type":"markdown","metadata":{"id":"Ptv5AT_iTb7W"},"source":["Let's have a look at the first training example:"]},{"cell_type":"markdown","metadata":{"id":"VvU4nzL2W2Xo"},"source":["Let's verify that the input ids and corresponding targets are correct:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4JyI2WGbpIO1"},"outputs":[],"source":["def decode(tokens: torch.LongTensor, labels: torch.LongTensor):\n","    print(tokens.dim())\n","    assert tokens.dim() == 1\n","    assert labels.dim() == 1\n","    tokens = tokens.numpy()\n","    labels - labels.numpy()\n","    if tokens[0] == vocab.bos():\n","        tokens = tokens[1:]  # remove <s>\n","    eos_mask = tokens == vocab.eos()\n","    doc_mask = eos_mask[1:] & eos_mask[:-1]\n","    sentences = np.split(tokens,  doc_mask.nonzero()[0] + 1)\n","    labels = np.split(labels, doc_mask.nonzero()[0] + 1)\n","    sentences = [\n","        bpe.decode(vocab.string(s)) for s in sentences\n","    ]\n","    labels = [np.delete(l, np.where(l == -100))[:-1] for l in labels]\n","    if len(sentences) == 1:\n","        return sentences[0], labels[0]\n","    return sentences, labels"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1640027228194,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"},"user_tz":-420},"id":"DWgnNJrYW2GP","outputId":"167399c2-b508-4b96-8259-3bd780196c8d"},"outputs":[{"name":"stdout","output_type":"stream","text":["1\n","tôi                  0 O         \n","đặt                  0 O         \n","áo                   0 O         \n","bra                  0 O         \n","màu                  1 B-COL     \n","trắng                2 I-COL     \n","mà                   0 O         \n","shop                 0 O         \n","gửi                  0 O         \n","màu                  1 B-COL     \n","đen                  2 I-COL     \n",",                    0 O         \n","và                   0 O         \n","chất_lượng           0 O         \n","vải                  0 O         \n","và                   0 O         \n","mút                  0 O         \n","đệm                  0 O         \n","ngực                 0 O         \n","thì                  0 O         \n","quá                  0 O         \n","tệ                   0 O         \n","so                   0 O         \n","với                  0 O         \n","giá                  3 B-PRI     \n","tiền                 4 I-PRI     \n","của                  0 O         \n","sản_phẩm             0 O         \n","!                    0 O         \n"]}],"source":["test_idx = 54\n","sentences, labels = decode(torch.tensor(X[test_idx]), torch.tensor(Y_label[test_idx]))\n","for token, label in zip(sentences.split(), labels):\n","  print('{0:10}  {1:10} {2:10}'.format(token, label, ids_to_labels[int(label)]))"]},{"cell_type":"markdown","metadata":{"id":"9d7dOlWeJ8z8"},"source":["### Training"]},{"cell_type":"markdown","metadata":{"id":"73OzU7oXRxR8"},"source":["#### **Defining the model**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pEWFugm23Vcg"},"outputs":[],"source":["class argu():\n","    def __init__(self):\n","        # self.train_path = './data/train.csv'\n","        self.dict_path = \"./PhoBERT_base_transformers/dict.txt\"\n","        self.config_path = \"./PhoBERT_base_transformers/config.json\"\n","        # self.rdrsegmenter_path = '/content/vncorenlp/VnCoreNLP-1.1.1.jar'\n","        self.pretrained_path = './PhoBERT_base_transformers/model.bin'\n","        self.max_sequence_length = 256\n","        self.batch_size = 8\n","        self.accumulation_steps = 1\n","        self.epochs = 10\n","        self.seed = 69\n","        self.fold = 0\n","        self.lr= 1e-5\n","        self.ckpt_path = './checkpoints'\n","        self.bpe_codes = \"./PhoBERT_base_transformers/bpe.codes\"\n","args = argu()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":32,"status":"ok","timestamp":1640027228730,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"},"user_tz":-420},"id":"xVJlt2kL3OjH","outputId":"06ef8bce-9e8c-433d-c7e5-b4c10a38d18e"},"outputs":[{"name":"stderr","output_type":"stream","text":["You are using a model of type bert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n"]}],"source":["config = RobertaConfig.from_pretrained(\n","    args.config_path,\n","    output_hidden_states=True,\n","    return_dict=True,\n","    num_labels=5,\n","    classifier_dropout = True\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"EL8MbWWd79U_"},"outputs":[],"source":["train_size = 0.8\n","def train_test_split(data, train_size):\n","    X_df = pd.DataFrame(data)\n","    X_train = X_df.sample(frac = train_size, random_state=200)\n","    X_test = X_df.drop(X_train.index).reset_index(drop=True)\n","    X_train = X_train.reset_index(drop=True)\n","    return X_train.values, X_test .values\n","\n","X_train, X_test = train_test_split(X, train_size)\n","Y_label_train, Y_label_test = train_test_split(Y_label, train_size)\n","Y_mask_train, Y_mask_test = train_test_split(Y_mask, train_size)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lG9-xqY8_0xT"},"outputs":[],"source":["train_dataset = TensorDataset(torch.tensor(X_train,dtype=torch.long), \n","                              torch.tensor(Y_label_train,dtype=torch.long))\n","\n","valid_dataset = TensorDataset(torch.tensor(X_test,dtype=torch.long), \n","                              torch.tensor(Y_label_test,dtype=torch.long))\n","\n","train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n","valid_loader = torch.utils.data.DataLoader(valid_dataset, batch_size=VALID_BATCH_SIZE, shuffle=False)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":31,"status":"ok","timestamp":1640027228734,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"},"user_tz":-420},"id":"TNvBArkVgBx0","outputId":"3831888d-2a8b-432a-894c-903c9f3a8ce4"},"outputs":[{"data":{"text/plain":["(749056,)"]},"execution_count":29,"metadata":{},"output_type":"execute_result"}],"source":["Y_label_train.flatten().shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lv4vnxZifVPk"},"outputs":[],"source":["from sklearn.utils.class_weight import compute_class_weight\n","y_org = Y_label_train\n","class_weight = compute_class_weight(class_weight='balanced', classes = np.array([0,1,2,3,4]), y=y_org.flatten()[y_org.flatten()>=0])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":25,"status":"ok","timestamp":1640027228736,"user":{"displayName":"Quang Duy Tran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Ghu2xyMtK_2iyS9Voe4qtTbv8PTTDqItODbJlD0Uw=s64","userId":"16933642894382662697"},"user_tz":-420},"id":"ifs-50S6gaIX","outputId":"267dabd8-46a8-42fe-af68-8872c2741382"},"outputs":[{"data":{"text/plain":["array([ 0.20681367, 16.49712974, 41.89212828, 18.54064516, 38.01322751])"]},"execution_count":31,"metadata":{},"output_type":"execute_result"}],"source":["class_weight"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qa8o3Sn03Ae0"},"outputs":[],"source":["from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","class Roberta_SeqTag(BertPreTrainedModel):\n","    config_class = RobertaConfig\n","    base_model_prefix = \"roberta\"\n","    def __init__(self, config):\n","        super(Roberta_SeqTag, self).__init__(config)\n","        self.num_labels = config.num_labels\n","        self.roberta = RobertaModel(config, add_pooling_layer=False)\n","\n","        classifier_dropout = (\n","                config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n","            )\n","        self.dropout = nn.Dropout(classifier_dropout)\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","        # self.lstm = nn.LSTM(config.hidden_size, 256, num_layers=1, bidirectional=True)\n","        # self.classifier = nn.Linear(256*2, config.num_labels)\n","        self.post_init()\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","\n","        outputs = self.roberta(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        sequence_output = outputs[0]\n","\n","        sequence_output = self.dropout(sequence_output)\n","        # x, _ = self.lstm(sequence_output)\n","        # sequence_output = torch.tanh(x)\n","        logits = self.classifier(sequence_output)\n","        # print(logits.shape)\n","        loss = None\n","        if labels is not None:\n","            loss_fct = nn.CrossEntropyLoss(weight=torch.tensor(class_weight, dtype=torch.float).to(device))\n","            # Only keep active parts of the loss\n","            if attention_mask is not None:\n","                active_loss = attention_mask.view(-1) == 1\n","                active_logits = logits.view(-1, self.num_labels)\n","                active_labels = torch.where(\n","                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n","                )\n","                # print(\n","                #     'active loss', active_loss, active_loss.shape,\n","                #     'active_logits', active_logits, active_logits.shape,\n","                #     'active_labels', active_labels, active_labels.shape)\n","                loss = loss_fct(active_logits, active_labels)\n","                # quit()\n","            else:\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","\n","        if not return_dict:\n","            output = (logits,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return TokenClassifierOutput(\n","            loss=loss,\n","            logits=logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CdnBtsTP2XiN"},"outputs":[],"source":["model = Roberta_SeqTag.from_pretrained(args.pretrained_path, config=config)\n","model.cuda()\n","\n","# model = RobertaForTokenClassification.from_pretrained(args.pretrained_path, config=config)\n","# model.cuda()\n","\n","from transformers import AutoModel, AutoTokenizer, AutoModelForTokenClassification\n","\n","model = AutoModelForTokenClassification.from_pretrained(\"vinai/phobert-base\", num_labels=9)\n","model.cuda()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GLFivpkwW1HY"},"outputs":[],"source":["# Defining the training function on the 80% of the dataset for tuning the bert model\n","def train(epoch, verbose = False):\n","    tr_loss, tr_accuracy = 0, 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","    tr_preds, tr_labels = [], []\n","    # put model in training mode\n","    model.train()\n","    \n","    for idx, batch in enumerate(train_loader):\n","        ids, labels = batch\n","        ids = ids.to(device)\n","        labels = labels.to(device)\n","        mask = ids!=1\n","        \n","        # check(ids, mask, labels)\n","        # break\n","\n","        outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n","        loss = outputs[0]\n","        tr_logits = outputs[1]\n","        tr_loss += loss.item()\n","\n","        nb_tr_steps += 1\n","        nb_tr_examples += labels.size(0)\n","        \n","        if idx % 100==0 and verbose:\n","            loss_step = tr_loss/nb_tr_steps\n","            print(f\"Training loss per 100 training steps: {loss_step}\")\n","           \n","        # compute training accuracy\n","        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","        # print(active_logits)\n","        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","        # print(flattened_predictions)\n","        \n","        # only compute accuracy at active labels\n","        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","        #active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n","        \n","        labels = torch.masked_select(flattened_targets, active_accuracy)\n","        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","        \n","        tr_labels.extend(labels)\n","        tr_preds.extend(predictions)\n","\n","        tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n","        tr_accuracy += tmp_tr_accuracy\n","    \n","        # gradient clipping\n","        torch.nn.utils.clip_grad_norm_(\n","            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n","        )\n","        \n","        # backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    epoch_loss = tr_loss / nb_tr_steps\n","    tr_accuracy = tr_accuracy / nb_tr_steps\n","\n","    labels = [ids_to_labels[id.item()] for id in tr_labels]\n","    predictions = [ids_to_labels[id.item()] for id in tr_preds]\n","    f1 = seqeval.metrics.f1_score([labels], [predictions])\n","\n","    print(f\"Training loss epoch: {epoch_loss}\", f\"Training F1 epoch: {f1}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RIVVfFHi7Aw7"},"outputs":[],"source":["def valid(model, test_loader, verbose=False):\n","    # put model in evaluation mode\n","    model.eval()\n","    \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_examples, nb_eval_steps = 0, 0\n","    eval_preds, eval_labels = [], []\n","    \n","    with torch.no_grad():\n","        for idx, batch in enumerate(test_loader):\n","            \n","            ids, labels = batch\n","            ids = ids.to(device)\n","            labels = labels.to(device)\n","            mask = ids!=1 \n","\n","            outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n","            loss = outputs[0]\n","            eval_logits = outputs[1]\n","            eval_loss += loss.item()\n","\n","            nb_eval_steps += 1\n","            nb_eval_examples += labels.size(0)\n","        \n","            if idx % 100==0 and verbose:\n","                loss_step = eval_loss/nb_eval_steps\n","                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n","              \n","            # compute evaluation accuracy\n","            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","            \n","            # only compute accuracy at active labels\n","            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","        \n","            labels = torch.masked_select(flattened_targets, active_accuracy)\n","            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","            \n","            eval_labels.extend(labels)\n","            eval_preds.extend(predictions)\n","            \n","            tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n","            eval_accuracy += tmp_eval_accuracy\n","\n","    labels = [ids_to_labels[id.item()] for id in eval_labels]\n","    predictions = [ids_to_labels[id.item()] for id in eval_preds]\n","    \n","    eval_loss = eval_loss / nb_eval_steps\n","    eval_accuracy = eval_accuracy / nb_eval_steps\n","    f1 = seqeval.metrics.f1_score([labels], [predictions])\n","    print(f\"Validation Loss: {eval_loss}\", f\"Validation F1: {f1}\")\n","\n","    return labels, predictions"]},{"cell_type":"markdown","metadata":{"id":"k2dsCyP7dcF3"},"source":["And let's train the model!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Wo5FZT6qVP8G"},"outputs":[],"source":["from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hA4_1KRAaA63"},"outputs":[],"source":["# Creating optimizer and lr schedulers\n","param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","num_train_optimization_steps = int(args.epochs*len(train_dataset)/args.batch_size/args.accumulation_steps)\n","optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=num_train_optimization_steps)  # PyTorch scheduler\n","scheduler0 = get_constant_schedule(optimizer)  # PyTorch scheduler\n","\n","tsfm = model.roberta\n","for child in tsfm.children():\n","    for param in child.parameters():\n","        if not param.requires_grad:\n","            print(\"whoopsies\")\n","        param.requires_grad = False\n","frozen = True"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/"},"id":"y07Ybw8rZeZ7","outputId":"20f2dc0d-7118-4907-a312-e5c20a87a427"},"outputs":[{"name":"stdout","output_type":"stream","text":["Training epoch: 1\n","Training loss epoch: 1.601916303074425 Training F1 epoch: 0.0\n","Validation Loss: 1.5978773343758506 Validation F1: 0.013726581339269153\n","Time:  42.21763038635254\n","Training epoch: 2\n","Training loss epoch: 1.5942021004163502 Training F1 epoch: 0.0\n","Validation Loss: 1.5868488765153728 Validation F1: 0.014876978828914741\n","Time:  118.25689506530762\n","Training epoch: 3\n","Training loss epoch: 1.5893093720485605 Training F1 epoch: 0.0\n","Validation Loss: 1.5777369985163539 Validation F1: 0.014016385351890238\n","Time:  132.99086046218872\n","Training epoch: 4\n","Training loss epoch: 1.5851695277000386 Training F1 epoch: 0.0\n","Validation Loss: 1.5694774444637403 Validation F1: 0.013484523444682806\n","Time:  118.61079287528992\n","Training epoch: 5\n","Training loss epoch: 1.5793420068227528 Training F1 epoch: 0.0\n","Validation Loss: 1.561345940079194 Validation F1: 0.012985630654603512\n","Time:  117.08675813674927\n","Training epoch: 6\n","Training loss epoch: 1.5778584133406155 Training F1 epoch: 0.0\n","Validation Loss: 1.5541035572036368 Validation F1: 0.012189716312056738\n","Time:  116.8448224067688\n","Training epoch: 7\n","Training loss epoch: 1.5718799695942571 Training F1 epoch: 0.0\n","Validation Loss: 1.5465078243140966 Validation F1: 0.01144993574015656\n","Time:  116.4931104183197\n","Training epoch: 8\n","Training loss epoch: 1.5676491343584218 Training F1 epoch: 0.0\n","Validation Loss: 1.5394122646154602 Validation F1: 0.012297097884899163\n","Time:  116.589679479599\n","Training epoch: 9\n","Training loss epoch: 1.5627479795875445 Training F1 epoch: 0.0\n","Validation Loss: 1.5321183074367501 Validation F1: 0.012765403152273024\n","Time:  117.00439143180847\n","Training epoch: 10\n","Training loss epoch: 1.5602176997179542 Training F1 epoch: 0.0\n","Validation Loss: 1.525548142162177 Validation F1: 0.012670431070100538\n","Time:  116.70111513137817\n","Training epoch: 11\n","Training loss epoch: 1.5549323097929928 Training F1 epoch: 0.0\n","Validation Loss: 1.518817949490469 Validation F1: 0.012402185146906837\n","Time:  116.99968576431274\n","Training epoch: 12\n","Training loss epoch: 1.5536787179324145 Training F1 epoch: 0.0\n","Validation Loss: 1.5127316296426325 Validation F1: 0.012987012987012986\n","Time:  117.04869794845581\n","Training epoch: 13\n","Training loss epoch: 1.5512217485188136 Training F1 epoch: 0.0\n","Validation Loss: 1.5068221525416348 Validation F1: 0.01230138390568939\n","Time:  116.77720308303833\n","Training epoch: 14\n","Training loss epoch: 1.5479244637033327 Training F1 epoch: 0.0\n","Validation Loss: 1.5010905666429488 Validation F1: 0.012087912087912088\n","Time:  116.53747606277466\n","Training epoch: 15\n","Training loss epoch: 1.547559350240426 Training F1 epoch: 0.0\n","Validation Loss: 1.4957508513184845 Validation F1: 0.011460185734044654\n","Time:  116.84626007080078\n","Training epoch: 16\n","Training loss epoch: 1.5458337281896768 Training F1 epoch: 0.0\n","Validation Loss: 1.4906946241529913 Validation F1: 0.011418904631000211\n","Time:  116.62231063842773\n","Training epoch: 17\n","Training loss epoch: 1.541216206355173 Training F1 epoch: 0.0\n","Validation Loss: 1.485611000673367 Validation F1: 0.009664058904739993\n","Time:  116.36505389213562\n","Training epoch: 18\n","Training loss epoch: 1.54021062421017 Training F1 epoch: 0.0\n","Validation Loss: 1.4807942398910314 Validation F1: 0.007928642220019821\n","Time:  116.4425277709961\n","Training epoch: 19\n","Training loss epoch: 1.5354612091851365 Training F1 epoch: 0.0\n","Validation Loss: 1.4759966138282108 Validation F1: 0.007028926736955934\n","Time:  116.23345732688904\n","Training epoch: 20\n","Training loss epoch: 1.5393765936783754 Training F1 epoch: 0.0\n","Validation Loss: 1.4719502688105641 Validation F1: 0.006932409012131715\n","Time:  116.6742684841156\n","Training epoch: 21\n","Training loss epoch: 1.5371379948378912 Training F1 epoch: 0.0\n","Validation Loss: 1.4677751800401615 Validation F1: 0.006269592476489029\n","Time:  116.19117879867554\n","Training epoch: 22\n","Training loss epoch: 1.5296218009268652 Training F1 epoch: 0.0\n","Validation Loss: 1.4633527010516392 Validation F1: 0.0061643835616438354\n","Time:  116.53905320167542\n","Training epoch: 23\n","Training loss epoch: 1.5337897791562836 Training F1 epoch: 0.0\n","Validation Loss: 1.4596826157283262 Validation F1: 0.0051470588235294126\n","Time:  116.29931306838989\n","Training epoch: 24\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-38-181a4d416318>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mst\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Training epoch: {epoch + 1}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m     \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredictions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-34-23102e102873>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(epoch, verbose)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0mepoch_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtr_loss\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnb_tr_steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/lr_scheduler.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     63\u001b[0m                 \u001b[0minstance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_step_count\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m                 \u001b[0mwrapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__get__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minstance\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcls\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m             \u001b[0;31m# Note that the returned function here is no longer a bound method,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/transformers/optimization.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    357\u001b[0m                     \u001b[0mstep_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstep_size\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 359\u001b[0;31m                 \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddcdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexp_avg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdenom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mstep_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    360\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    361\u001b[0m                 \u001b[0;31m# Just adding the square of the weights to the loss function is *not*\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["EPOCHS = 50\n","import time\n","\n","for epoch in range(EPOCHS):\n","    if epoch > 0 and frozen:\n","        for child in tsfm.children():\n","            for param in child.parameters():\n","                param.requires_grad = True\n","        frozen = False\n","        del scheduler0\n","        torch.cuda.empty_cache()\n","    st = time.time()\n","    print(f\"Training epoch: {epoch + 1}\")\n","    train(epoch)\n","    labels, predictions = valid(model, valid_loader)\n","    \n","    print('Time: ',time.time() - st)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_iNo8-R_Lq2a"},"outputs":[],"source":["for item in zip(labels, predictions):\n","    print(item)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0jDNXrjr-6BW"},"outputs":[],"source":["from seqeval.metrics import classification_report\n","print(classification_report([labels], [labels]))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KieJ2rH0S3XV"},"outputs":[],"source":["print(\"Number of tags: {}\".format(len(data.tags.unique())))\n","frequencies = data.Tag.value_counts()\n","frequencies"]},{"cell_type":"markdown","metadata":{"id":"4Gz-wHAw3xMk"},"source":["#### **Inference**\n","\n","The fun part is when we can quickly test the model on new, unseen sentences. \n","Here, we use the prediction of the **first word piece of every word** (which is how the model was trained). \n","\n","*In other words, the code below does not take into account when predictions of different word pieces that belong to the same word do not match.*"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gGe0F93pOuZa"},"outputs":[],"source":["ids_to_labels[-100] = 'X'"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zPDla1mmZiax"},"outputs":[],"source":["sentence = [\"Adam is a company based in New York, but is also has employees working in Paris\"]\n","ids, labels, masks = convert_lines(lines, tags, vocab, bpe)\n","\n","# move to gpu\n","ids = torch.tensor(ids).to(device)\n","mask = torch.tensor(masks).to(device)\n","# forward pass\n","outputs = model(ids, attention_mask=mask)\n","logits = outputs[0]\n","\n","active_logits = logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size*seq_len,) - predictions at the token level\n","\n","sentences, predictions = decode(ids.squeeze(0).cpu(), flattened_predictions.cpu())\n","_, labels = decode(ids.squeeze(0).cpu(), torch.tensor(labels).squeeze())\n","for token, pred, label in zip(sentences.split(), predictions, labels.squeeze()):\n","  print('{0:10}  {1:10} {2:10} {3:10}'.format(token, pred, ids_to_labels[int(pred)], ids_to_labels[int(label)]))\n","\n","\n","# tokens = tokenizer.convert_ids_to_tokens(ids.squeeze().tolist())\n","# token_predictions = [ids_to_labels[i] for i in flattened_predictions.cpu().numpy()]\n","# wp_preds = list(zip(tokens, token_predictions)) # list of tuples. Each tuple = (wordpiece, prediction)\n","\n","# prediction = []\n","# for token_pred, mapping in zip(wp_preds, inputs[\"offset_mapping\"].squeeze().tolist()):\n","#   #only predictions on first word pieces are important\n","#   if mapping[0] == 0 and mapping[1] != 0:\n","#     prediction.append(token_pred[1])\n","#   else:\n","#     continue\n","\n","# print(sentence.split())\n","# print(prediction)"]},{"cell_type":"markdown","metadata":{"id":"sqDklprSqB5d"},"source":["#### **Saving the model for future use**"]},{"cell_type":"markdown","metadata":{"id":"VuUdX_fImswO"},"source":["Finally, let's save the vocabulary (.txt) file, model weights (.bin) and the model's configuration (.json) to a directory, so that both the tokenizer and model can be re-loaded using the `from_pretrained()` class method.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sDZtSsKKntuI"},"outputs":[],"source":["import os\n","\n","directory = \"./model\"\n","\n","if not os.path.exists(directory):\n","    os.makedirs(directory)\n","\n","# save vocabulary of the tokenizer\n","tokenizer.save_vocabulary(directory)\n","# save the model weights and its configuration file\n","model.save_pretrained(directory)\n","print('All files saved')\n","print('This tutorial is completed')"]},{"cell_type":"markdown","metadata":{"id":"yQwQSVCAXPaX"},"source":["## Legacy"]},{"cell_type":"markdown","metadata":{"id":"QWslPozfnnPt"},"source":["The following code blocks were used during the development of this notebook, but are not included anymore."]},{"cell_type":"markdown","metadata":{"id":"zZmdgtpumBfH"},"source":["label for subword "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gzu7uvhCmAS4"},"outputs":[],"source":["# text = '<s> '+'Hôm_nay trời nóng quá nên tôi ở nhà viết Viblo!' +' </s>'\n","text = 'nên xem_lại tư_duy bán hàng , bán thua hàng_chợ thì cần xem_lại , đặt giá gốc cao xong hạ saleoff lừa à !' \n","subwords = bpe.encode(text)\n","subwords = '<s> '+subwords +' </s>'\n","input_ids = vocab.encode_line(subwords, append_eos=False, add_if_not_exist=False).long().tolist()\n","print(subwords)\n","print(input_ids)\n","subword_idx = [subwords.split().index(word) for word in subwords.split() if '@@' in word]\n","print(subword_idx)\n","tag_list = 'O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-PRI,I-PRI,I-PRI,O,B-PRI,B-PRI,O,O,O,O'.split(',')\n","for i, idx in enumerate(subword_idx):\n","    orig_idx = idx - i\n","    tag_list.insert(orig_idx+1, tag_list[orig_idx])\n","for pair in zip(subwords.split(), tag_list):\n","    print(pair)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"k3okbiKoXPaY"},"outputs":[],"source":["def prepare_sentence(sentence, tokenizer, maxlen):    \n","      # step 1: tokenize the sentence\n","      tokenized_sentence = tokenizer.tokenize(sentence)\n","      \n","      # step 2: add special tokens \n","      tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] \n","\n","      # step 3: truncating/padding\n","      if (len(tokenized_sentence) > maxlen):\n","        # truncate\n","        tokenized_sentence = tokenized_sentence[:maxlen]\n","      else:\n","        # pad\n","        tokenized_sentence = tokenized_sentence + ['[PAD]'for _ in range(maxlen - len(tokenized_sentence))]\n","\n","      # step 4: obtain the attention mask\n","      attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n","      \n","      # step 5: convert tokens to input ids\n","      ids = tokenizer.convert_tokens_to_ids(tokenized_sentence)\n","      \n","      return {\n","            'ids': torch.tensor(ids, dtype=torch.long),\n","            'mask': torch.tensor(attn_mask, dtype=torch.long),\n","            #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n","      }"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dyAL9FIRRcTg"},"outputs":[],"source":["def tokenize_and_preserve_labels(sentence, text_labels, tokenizer):\n","    \"\"\"\n","    Word piece tokenization makes it difficult to match word labels\n","    back up with individual word pieces. This function tokenizes each\n","    word one at a time so that it is easier to preserve the correct\n","    label for each subword. It is, of course, a bit slower in processing\n","    time, but it will help our model achieve higher accuracy.\n","    \"\"\"\n","\n","    tokenized_sentence = []\n","    labels = []\n","\n","    sentence = sentence.strip()\n","\n","    for word, label in zip(sentence.split(), text_labels.split(\",\")):\n","\n","        # Tokenize the word and count # of subwords the word is broken into\n","        tokenized_word = tokenizer.tokenize(word)\n","        n_subwords = len(tokenized_word)\n","\n","        # Add the tokenized word to the final tokenized word list\n","        tokenized_sentence.extend(tokenized_word)\n","\n","        # Add the same label to the new list of labels `n_subwords` times\n","        labels.extend([label] * n_subwords)\n","\n","    return tokenized_sentence, labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xWSn_glJRksn"},"outputs":[],"source":["class dataset(Dataset):\n","    def __init__(self, dataframe, tokenizer, max_len):\n","        self.len = len(dataframe)\n","        self.data = dataframe\n","        self.tokenizer = tokenizer\n","        self.max_len = max_len\n","        \n","    def __getitem__(self, index):\n","        # step 1: tokenize (and adapt corresponding labels)\n","        sentence = self.data.sentence[index]  \n","        word_labels = self.data.word_labels[index]  \n","        tokenized_sentence, labels = tokenize_and_preserve_labels(sentence, word_labels, self.tokenizer)\n","        \n","        # step 2: add special tokens (and corresponding labels)\n","        tokenized_sentence = [\"[CLS]\"] + tokenized_sentence + [\"[SEP]\"] # add special tokens\n","        labels.insert(0, \"O\") # add outside label for [CLS] token\n","        labels.insert(-1, \"O\") # add outside label for [SEP] token\n","\n","        # step 3: truncating/padding\n","        maxlen = self.max_len\n","\n","        if (len(tokenized_sentence) > maxlen):\n","          # truncate\n","          tokenized_sentence = tokenized_sentence[:maxlen]\n","          labels = labels[:maxlen]\n","        else:\n","          # pad\n","          tokenized_sentence = tokenized_sentence + ['[PAD]'for _ in range(maxlen - len(tokenized_sentence))]\n","          labels = labels + [\"O\" for _ in range(maxlen - len(labels))]\n","\n","        # step 4: obtain the attention mask\n","        attn_mask = [1 if tok != '[PAD]' else 0 for tok in tokenized_sentence]\n","        \n","        # step 5: convert tokens to input ids\n","        ids = self.tokenizer.convert_tokens_to_ids(tokenized_sentence)\n","\n","        label_ids = [labels_to_ids[label] for label in labels]\n","        # the following line is deprecated\n","        #label_ids = [label if label != 0 else -100 for label in label_ids]\n","        \n","        return {\n","              'ids': torch.tensor(ids, dtype=torch.long),\n","              'mask': torch.tensor(attn_mask, dtype=torch.long),\n","              #'token_type_ids': torch.tensor(token_ids, dtype=torch.long),\n","              'targets': torch.tensor(label_ids, dtype=torch.long)\n","        } \n","    \n","    def __len__(self):\n","        return self.len"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a9-BqpvYjkzN"},"outputs":[],"source":["sentence = \"this is a test @huggingface\".strip().split()\n","\n","inputs = tokenizer(sentence, is_pretokenized=True, return_offsets_mapping=True, padding='max_length', truncation=True)\n","tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])\n","token_offsets = inputs[\"offset_mapping\"]\n","print(tokens)\n","print(token_offsets)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pw4AfDxJlU_Z"},"outputs":[],"source":["word = \"@huggingface\"\n","\n","inputs = tokenizer(word, return_offsets_mapping=True, padding='max_length', truncation=True)\n","tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"])\n","token_offsets = inputs[\"offset_mapping\"]\n","print(tokens)\n","print(token_offsets)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W9LHi_V5WZ9z"},"outputs":[],"source":["# now, use mask to determine where we should compare predictions with targets (includes [CLS] and [SEP] token predictions)\n","        active_accuracy = mask.view(-1) == 1 # active accuracy is also of shape (batch_size * seq_len,)\n","        targets = torch.masked_select(flattened_targets, active_accuracy)\n","        predictions = torch.masked_select(flattened_predictions, active_accuracy)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qfLFlRHXH-gN"},"outputs":[],"source":["# check for initial loss\n","input_ids, labels = next(iter(train_loader))\n","input_ids.shape, labels.shape\n","\n","input_ids = input_ids.to(device)\n","labels = labels.to(device)\n","\n","outputs = model(input_ids, attention_mask=input_ids!=1, labels=labels)\n","initial_loss = outputs[0]\n","initial_loss"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["4Gz-wHAw3xMk","yQwQSVCAXPaX"],"name":"seq labeling own data 02.ipynb","provenance":[{"file_id":"1bGb3oakV_VcEj8rORlff6ZAZTjPm1ZnM","timestamp":1639822270628},{"file_id":"https://github.com/NielsRogge/Transformers-Tutorials/blob/master/BERT/Custom_Named_Entity_Recognition_with_BERT_only_first_wordpiece.ipynb","timestamp":1639584213203}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.4"}},"nbformat":4,"nbformat_minor":0}