{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CNN experiment.ipynb","provenance":[],"collapsed_sections":["7k1WqTO59A5Z"],"machine_shape":"hm"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"sMicb1TP98ji","colab":{"base_uri":"https://localhost:8080/"},"outputId":"1423808a-1701-4dff-f6bd-0375c23d6e5c","executionInfo":{"status":"ok","timestamp":1640252231669,"user_tz":-420,"elapsed":19778,"user":{"displayName":"Duy Trần Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij1youI44C-pViNsGwdbcY5YEbgeVxxf70O_ZNIA=s64","userId":"02731206869052649620"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}],"source":["from google.colab import drive\n","drive._mount('/content/drive')"]},{"cell_type":"code","source":["!pip install transformers seqeval[gpu] -q\n","!pip install fairseq -q\n","!pip install fastBPE -q\n","!pip install pytorch-crf -q"],"metadata":{"id":"if8rbToF-IbX","colab":{"base_uri":"https://localhost:8080/"},"outputId":"8f13647c-e96b-4a0c-da02-e02573f94114","executionInfo":{"status":"ok","timestamp":1640252274187,"user_tz":-420,"elapsed":42522,"user":{"displayName":"Duy Trần Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij1youI44C-pViNsGwdbcY5YEbgeVxxf70O_ZNIA=s64","userId":"02731206869052649620"}}},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 3.4 MB 5.5 MB/s \n","\u001b[K     |████████████████████████████████| 43 kB 1.4 MB/s \n","\u001b[K     |████████████████████████████████| 3.3 MB 36.7 MB/s \n","\u001b[K     |████████████████████████████████| 895 kB 46.0 MB/s \n","\u001b[K     |████████████████████████████████| 596 kB 45.8 MB/s \n","\u001b[K     |████████████████████████████████| 61 kB 380 kB/s \n","\u001b[?25h  Building wheel for seqeval (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[K     |████████████████████████████████| 1.7 MB 5.3 MB/s \n","\u001b[K     |████████████████████████████████| 145 kB 48.4 MB/s \n","\u001b[K     |████████████████████████████████| 90 kB 8.0 MB/s \n","\u001b[K     |████████████████████████████████| 112 kB 53.6 MB/s \n","\u001b[K     |████████████████████████████████| 74 kB 2.4 MB/s \n","\u001b[?25h  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for fastBPE (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/NLP/project_nlp"],"metadata":{"id":"cD7orqk4-HRv","colab":{"base_uri":"https://localhost:8080/"},"outputId":"f225d4c3-d693-42a6-e9dd-2fb2b223a3d0","executionInfo":{"status":"ok","timestamp":1640252274188,"user_tz":-420,"elapsed":30,"user":{"displayName":"Duy Trần Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij1youI44C-pViNsGwdbcY5YEbgeVxxf70O_ZNIA=s64","userId":"02731206869052649620"}}},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/NLP/project_nlp\n"]}]},{"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","from torch.utils.data import Dataset, DataLoader, TensorDataset\n","from torchcrf import CRF\n","\n","from transformers import RobertaConfig, RobertaPreTrainedModel, RobertaModel, RobertaForTokenClassification, RobertaForSequenceClassification\n","from transformers import AdamW, get_linear_schedule_with_warmup, get_constant_schedule\n","from transformers.modeling_outputs import TokenClassifierOutput, SequenceClassifierOutput\n","\n","import seqeval\n","from seqeval.metrics import classification_report, f1_score\n","\n","import pandas as pd\n","import numpy as np\n","import argparse\n","import time\n","import tqdm\n","\n","from fairseq.data.encoders.fastbpe import fastBPE\n","from fairseq.data import Dictionary\n","\n","from sklearn.utils.class_weight import compute_class_weight\n","\n","from src.dataset import *"],"metadata":{"id":"vQKjvZWT-LUa","executionInfo":{"status":"ok","timestamp":1640252726216,"user_tz":-420,"elapsed":361,"user":{"displayName":"Duy Trần Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij1youI44C-pViNsGwdbcY5YEbgeVxxf70O_ZNIA=s64","userId":"02731206869052649620"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["device = 'cuda' if torch.cuda.is_available() else 'cpu'"],"metadata":{"id":"05q1dePoobEx","executionInfo":{"status":"ok","timestamp":1640252284304,"user_tz":-420,"elapsed":10,"user":{"displayName":"Duy Trần Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij1youI44C-pViNsGwdbcY5YEbgeVxxf70O_ZNIA=s64","userId":"02731206869052649620"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["BATCHSIZE_TRAIN = 8\n","BATCHSIZE_VAL = 4\n","LEARNING_RATE = 5e-5\n","MAX_LEN = 128\n","NUM_EPOCH = 20\n","SEED = 42\n","NUM_CLASS = 5"],"metadata":{"id":"nzGjLqblox8g","executionInfo":{"status":"ok","timestamp":1640252284304,"user_tz":-420,"elapsed":7,"user":{"displayName":"Duy Trần Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij1youI44C-pViNsGwdbcY5YEbgeVxxf70O_ZNIA=s64","userId":"02731206869052649620"}}},"execution_count":6,"outputs":[]},{"cell_type":"markdown","source":["## Data\n"],"metadata":{"id":"CAnYaObi9HXf"}},{"cell_type":"code","source":["data = pd.read_csv('./data/joint/data_joint.csv')\n","data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"GX-F2k_lowjM","executionInfo":{"status":"ok","timestamp":1640252285005,"user_tz":-420,"elapsed":708,"user":{"displayName":"Duy Trần Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij1youI44C-pViNsGwdbcY5YEbgeVxxf70O_ZNIA=s64","userId":"02731206869052649620"}},"outputId":"576e2671-d8a7-4519-82c5-15c88a6e3327"},"execution_count":7,"outputs":[{"output_type":"execute_result","data":{"text/html":["\n","  <div id=\"df-a96c268a-daee-4bde-b82b-012e63b86f3c\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>sentence</th>\n","      <th>word_labels</th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>combo 3 cái giao có 1 cái , thành_ra đặt 6 cái...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,O,B-DES,B-DE...</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>mình mua áo có cổ màu trắng lại ship tới cho m...</td>\n","      <td>O,O,B-DES,B-DES,I-DES,B-DES,I-DES,O,O,O,O,O,B-...</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>giao sai hàng . tôi muốn trả hàng . đặt be đậm...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,B-DES,I-DES,O,B-DES,I-DES</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>sản_xuất việt_nam nhưng thấy in chữ trung_quốc...</td>\n","      <td>O,O,O,O,B-DES,I-DES,O,O,O,O,O,O,O</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>4</td>\n","      <td>mình đặt áo sơ_mi trắng dài tay mà shop giao c...</td>\n","      <td>O,O,B-DES,I-DES,B-DES,B-DES,I-DES,O,O,O,O,O,O,...</td>\n","      <td>-1</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>3652</th>\n","      <td>3652</td>\n","      <td>chất_lượng sản_phẩm giống mô tả . giao hàng nh...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3653</th>\n","      <td>3653</td>\n","      <td>hài_lòng vô_cùng , giao nhanh , nhân_viên giao...</td>\n","      <td>O,O,O,O,O,O,O,O,O,O,O,B-DES,O,B-DES</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3654</th>\n","      <td>3654</td>\n","      <td>sản_phẩm ổn . giá phải_chăng . thật_sự là nhận...</td>\n","      <td>O,O,O,B-PRI,B-PRI,O,O,O,O,O,O,O,O,O,O,O,O,O</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3655</th>\n","      <td>3655</td>\n","      <td>hàng đẹp chuẩn chất_lượng . nếu áo có đai ngan...</td>\n","      <td>O,B-DES,O,O,O,O,B-DES,O,O,O,B-DES,O,B-DES,O,O,O</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>3656</th>\n","      <td>3656</td>\n","      <td>vải mặc mát , dày_dặn . không có miếng mút ngự...</td>\n","      <td>B-DES,O,B-DES,O,B-DES,O,O,O,O,O,O,O,O,O,O,O,O,...</td>\n","      <td>1</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3657 rows × 4 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a96c268a-daee-4bde-b82b-012e63b86f3c')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a96c268a-daee-4bde-b82b-012e63b86f3c button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a96c268a-daee-4bde-b82b-012e63b86f3c');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["      Unnamed: 0  ... label\n","0              0  ...    -1\n","1              1  ...    -1\n","2              2  ...    -1\n","3              3  ...    -1\n","4              4  ...    -1\n","...          ...  ...   ...\n","3652        3652  ...     1\n","3653        3653  ...     1\n","3654        3654  ...     1\n","3655        3655  ...     1\n","3656        3656  ...     1\n","\n","[3657 rows x 4 columns]"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["parser = argparse.ArgumentParser()\n","parser.add_argument('--bpe-codes', \n","    default=\"./PhoBERT_base_transformers/bpe.codes\",\n","    required=False,\n","    type=str,\n","    help='path to fastBPE BPE'\n",")\n","args, unknown = parser.parse_known_args()\n","bpe = fastBPE(args)\n","\n","# Load the dictionary\n","vocab = Dictionary()\n","vocab.add_from_file(\"./PhoBERT_base_transformers/dict.txt\")\n","\n","labels_to_ids = {'B-DES': 1, 'B-PRI': 3, 'I-DES': 2, 'I-PRI': 4, 'O': 0, 'X': -100}\n","ids_to_labels = {0: 'O', 1: 'B-DES', 2: 'I-DES', 3:'B-PRI', 4:'I-PRI'}\n","\n","X, Y_label, Y_mask = convert_lines(\n","    data.sentence.values, \n","    data.word_labels.values, \n","    vocab, \n","    bpe, \n","    labels_to_ids, \n","    max_sequence_length=MAX_LEN)\n","\n","print('X shape: ', X.shape)\n","print('Y label shape', Y_label.shape)\n","print('Y mask shape', Y_mask.shape)\n","\n","train_size = 0.8\n","def train_test_split(data, train_size):\n","    X_df = pd.DataFrame(data)\n","    X_train = X_df.sample(frac = train_size, random_state=200)\n","    X_test = X_df.drop(X_train.index).reset_index(drop=True)\n","    X_train = X_train.reset_index(drop=True)\n","    return X_train.values, X_test .values\n","\n","X_train, X_test = train_test_split(X, train_size)\n","Y_label_train, Y_label_test = train_test_split(Y_label, train_size)\n","Y_mask_train, Y_mask_test = train_test_split(Y_mask, train_size)\n","\n","class_weight = compute_class_weight(\n","    class_weight='balanced', \n","    classes = np.array([0,1,2,3,4]), \n","    y=Y_label_train.flatten()[Y_label_train.flatten()>=0])\n","\n","print(class_weight)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fcQ3MwMgo1aH","executionInfo":{"status":"ok","timestamp":1640252287098,"user_tz":-420,"elapsed":2096,"user":{"displayName":"Duy Trần Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij1youI44C-pViNsGwdbcY5YEbgeVxxf70O_ZNIA=s64","userId":"02731206869052649620"}},"outputId":"5dc094a5-2a3e-4c04-c78d-e88a3cba5251"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 3657/3657 [00:01<00:00, 2483.53it/s]\n"]},{"output_type":"stream","name":"stdout","text":["X shape:  (3657, 128)\n","Y label shape (3657, 128)\n","Y mask shape (3657, 128)\n","[ 0.24328616  1.55779008  6.52840909 15.97682503 31.33636364]\n"]}]},{"cell_type":"code","source":["train_dataset = TensorDataset(\n","    torch.tensor(X_train,dtype=torch.long), \n","    torch.tensor(Y_label_train,dtype=torch.long)\n","    )\n","valid_dataset = TensorDataset(\n","    torch.tensor(X_test,dtype=torch.long), \n","    torch.tensor(Y_label_test,dtype=torch.long)\n","    )\n","\n","train_loader = torch.utils.data.DataLoader(\n","    train_dataset, \n","    batch_size=BATCHSIZE_TRAIN, \n","    shuffle=True\n","    )\n","valid_loader = torch.utils.data.DataLoader(\n","    valid_dataset, \n","    batch_size=BATCHSIZE_VAL, \n","    shuffle=False\n","    )"],"metadata":{"id":"DViXKXtGo2oz","executionInfo":{"status":"ok","timestamp":1640252287098,"user_tz":-420,"elapsed":4,"user":{"displayName":"Duy Trần Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij1youI44C-pViNsGwdbcY5YEbgeVxxf70O_ZNIA=s64","userId":"02731206869052649620"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","source":["class argu():\n","    def __init__(self):\n","        self.dict_path = \"./PhoBERT_base_transformers/dict.txt\"\n","        self.config_path = \"./PhoBERT_base_transformers/config.json\"\n","        self.max_sequence_length = MAX_LEN\n","        self.accumulation_steps = 1\n","        self.epochs = NUM_EPOCH\n","        self.seed = SEED\n","        self.bpe_codes = \"./PhoBERT_base_transformers/bpe.codes\"\n","args = argu()\n","\n","config = RobertaConfig.from_pretrained(\n","    args.config_path,\n","    output_hidden_states=True,\n","    return_dict=True,\n","    num_labels=NUM_CLASS,\n","    pad_token_id = 1,\n","    bos_token_id = 0,\n","    eos_token_id = 2,\n","    attention_probs_dropout_prob = 0.1,\n","    classifier_dropout=0.5,\n","    gradient_checkpointing=False,\n","    hidden_act=\"gelu\",\n","    hidden_dropout_prob=0.1,\n","    hidden_size=768,\n","    initializer_range=0.02,\n","    intermediate_size=3072,\n","    layer_norm_eps=1e-05,\n","    max_position_embeddings=258,\n","    model_type=\"roberta\",\n","    num_attention_heads=12,\n","    num_hidden_layers=12,\n","    position_embedding_type=\"absolute\",\n","    tokenizer_class=\"PhobertTokenizer\",\n","    transformers_version=\"4.15.0\",\n","    type_vocab_size=1,\n","    use_cache=True,\n","    vocab_size=64001\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sr0S-M0Ko3qo","executionInfo":{"status":"ok","timestamp":1640252287440,"user_tz":-420,"elapsed":346,"user":{"displayName":"Duy Trần Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij1youI44C-pViNsGwdbcY5YEbgeVxxf70O_ZNIA=s64","userId":"02731206869052649620"}},"outputId":"e4367b82-af6d-436a-9ae7-fddea8aea2ca"},"execution_count":10,"outputs":[{"output_type":"stream","name":"stderr","text":["You are using a model of type bert to instantiate a model of type roberta. This is not supported for all configurations of models and can yield errors.\n"]}]},{"cell_type":"code","source":["def train(epoch, verbose = False):\n","    tr_loss, tr_accuracy = 0, 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","    tr_preds, tr_labels = [], []\n","    # put model in training mode\n","    model.train()\n","    \n","    for idx, batch in enumerate(train_loader):\n","        ids, labels = batch\n","        ids = ids.to(device)\n","        labels = labels.to(device)\n","        mask = ids!=1\n","\n","        outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n","        loss = outputs[0]\n","        \n","        tr_logits = outputs[1]\n","        tr_loss += loss.item()\n","\n","        nb_tr_steps += 1\n","        nb_tr_examples += labels.size(0)\n","        \n","        if idx % 100==0 and verbose:\n","            loss_step = tr_loss/nb_tr_steps\n","            print(f\"Training loss per 100 training steps: {loss_step}\")\n","           \n","        # compute training accuracy\n","        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","        active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","        flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","        \n","        # only compute accuracy at active labels\n","        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","        \n","        labels = torch.masked_select(flattened_targets, active_accuracy)\n","        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","        \n","        tr_labels.extend(labels)\n","        tr_preds.extend(predictions)\n","    \n","        # gradient clipping\n","        torch.nn.utils.clip_grad_norm_(\n","            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n","        )\n","        \n","        # backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        try: \n","            scheduler0.step()\n","        except:\n","            scheduler.step()\n","\n","    epoch_loss = tr_loss / nb_tr_steps\n","    tr_accuracy = tr_accuracy / nb_tr_steps\n","\n","    labels = [ids_to_labels[id.item()] for id in tr_labels]\n","    predictions = [ids_to_labels[id.item()] for id in tr_preds]\n","    f1 = seqeval.metrics.f1_score([labels], [predictions])\n","\n","    print(f\"Training loss epoch: {epoch_loss}\", f\"Training F1 epoch: {f1}\")\n","\n","def valid(model, test_loader, verbose=False):\n","    # put model in evaluation mode\n","    model.eval()\n","    \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_examples, nb_eval_steps = 0, 0\n","    eval_preds, eval_labels = [], []\n","    \n","    with torch.no_grad():\n","        for idx, batch in enumerate(test_loader):\n","            \n","            ids, labels = batch\n","            ids = ids.to(device)\n","            labels = labels.to(device)\n","            mask = ids!=1 \n","\n","            outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n","            loss = outputs[0]\n","            eval_logits = outputs[1]\n","            eval_loss += loss.item()\n","\n","            nb_eval_steps += 1\n","            nb_eval_examples += labels.size(0)\n","        \n","            if idx % 100==0 and verbose:\n","                loss_step = eval_loss/nb_eval_steps\n","                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n","              \n","            # compute evaluation accuracy\n","            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","            active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","            flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","            \n","            # only compute accuracy at active labels\n","            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","        \n","            labels = torch.masked_select(flattened_targets, active_accuracy)\n","            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","            \n","            eval_labels.extend(labels)\n","            eval_preds.extend(predictions)\n","            \n","\n","    labels = [ids_to_labels[id.item()] for id in eval_labels]\n","    predictions = [ids_to_labels[id.item()] for id in eval_preds]\n","    \n","    eval_loss = eval_loss / nb_eval_steps\n","    eval_accuracy = eval_accuracy / nb_eval_steps\n","    f1 = seqeval.metrics.f1_score([labels], [predictions])\n","    print(f\"Validation Loss: {eval_loss}\", f\"Validation F1: {f1}\")\n","\n","    return labels, predictions, f1\n","\n","def train_crf(epoch, verbose = False):\n","    tr_loss, tr_accuracy = 0, 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","    tr_preds, tr_labels = [], []\n","    model.train()\n","    \n","    for idx, batch in enumerate(train_loader):\n","        ids, labels = batch\n","        ids = ids.to(device)\n","        labels = labels.to(device)\n","        mask = ids!=1\n","        \n","        outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n","        loss = outputs[0]\n","                \n","        tr_loss += loss.item()\n","\n","        nb_tr_steps += 1\n","        nb_tr_examples += labels.size(0)\n","        \n","        if idx % 100==0 and verbose:\n","            loss_step = tr_loss/nb_tr_steps\n","            print(f\"Training loss per 100 training steps: {loss_step}\")\n","           \n","        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","        flattened_predictions = outputs[1].view(-1)\n","        \n","        # only compute accuracy at active labels\n","        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","        active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n","        \n","        labels = torch.masked_select(flattened_targets, active_accuracy)\n","        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","        \n","        tr_labels.extend(labels)\n","        tr_preds.extend(predictions)\n","    \n","        # gradient clipping\n","        torch.nn.utils.clip_grad_norm_(\n","            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n","        )\n","        \n","        # backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        try: \n","            scheduler0.step()\n","        except:\n","            scheduler.step()\n","            \n","    epoch_loss = tr_loss / nb_tr_steps\n","    tr_accuracy = tr_accuracy / nb_tr_steps\n","\n","    labels = [ids_to_labels[id.item()] for id in tr_labels]\n","    predictions = [ids_to_labels[id.item()] for id in tr_preds]\n","    f1 = seqeval.metrics.f1_score([labels], [predictions])\n","\n","    print(f\"Training loss epoch: {epoch_loss}\", f\"Training F1 epoch: {f1}\")\n","\n","def valid_crf(model, test_loader, verbose=False):\n","    # put model in evaluation mode\n","    model.eval()\n","    \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_examples, nb_eval_steps = 0, 0\n","    eval_preds, eval_labels = [], []\n","    \n","    with torch.no_grad():\n","        for idx, batch in enumerate(test_loader):\n","            \n","            ids, labels = batch\n","            ids = ids.to(device)\n","            labels = labels.to(device)\n","            mask = ids!=1 \n","\n","            outputs = model(input_ids=ids, attention_mask=mask, labels=labels)\n","            loss = outputs[0]\n","            eval_loss += loss.item()\n","\n","            nb_eval_steps += 1\n","            nb_eval_examples += labels.size(0)\n","        \n","            if idx % 100==0 and verbose:\n","                loss_step = eval_loss/nb_eval_steps\n","                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n","              \n","            # compute evaluation accuracy\n","            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","            flattened_predictions = outputs[1].view(-1)\n","            \n","            # only compute accuracy at active labels\n","            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","            active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n","\n","            labels = torch.masked_select(flattened_targets, active_accuracy)\n","            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","            \n","            eval_labels.extend(labels)\n","            eval_preds.extend(predictions)\n","            \n","\n","    labels = [ids_to_labels[id.item()] for id in eval_labels]\n","    predictions = [ids_to_labels[id.item()] for id in eval_preds]\n","    \n","    eval_loss = eval_loss / nb_eval_steps\n","    # eval_accuracy = eval_accuracy / nb_eval_steps\n","    f1 = seqeval.metrics.f1_score([labels], [predictions])\n","    print(f\"Validation Loss: {eval_loss}\", f\"Validation F1: {f1}\")\n","\n","    return labels, predictions, f1"],"metadata":{"id":"K_cqVfLHo443"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## Default Roberta sequence classification\n"],"metadata":{"id":"iph-R_cppWha"}},{"cell_type":"code","source":["class RobertaClassificationHead(nn.Module):\n","    \"\"\"Head for sentence-level classification tasks.\"\"\"\n","    def __init__(self, config):\n","        super().__init__()\n","        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n","        self.out_proj = nn.Linear(config.hidden_size, config.num_labels)\n","\n","    def forward(self, features, **kwargs):\n","        x = features[:, 0, :]  # take <s> token (equiv. to [CLS])\n","        x = self.dropout(x)\n","        x = self.dense(x)\n","        x = torch.tanh(x)\n","        x = self.dropout(x)\n","        x = self.out_proj(x)\n","        return x\n","\n","class RobertaForSequenceClassification(RobertaPreTrainedModel):\n","    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","        self.config = config\n","\n","        self.roberta = RobertaModel(config, add_pooling_layer=False)\n","        self.classifier = RobertaClassificationHead(config)\n","\n","        # Initialize weights and apply final processing\n","        self.post_init()\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","        r\"\"\"\n","        labels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\n","            Labels for computing the sequence classification/regression loss. Indices should be in `[0, ..., config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss),\n","            If `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        outputs = self.roberta(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","        sequence_output = outputs[0]\n","        logits = self.classifier(sequence_output)\n","\n","        loss = None\n","        if labels is not None:\n","            if self.config.problem_type is None:\n","                if self.num_labels == 1:\n","                    self.config.problem_type = \"regression\"\n","                elif self.num_labels > 1 and (labels.dtype == torch.long or labels.dtype == torch.int):\n","                    self.config.problem_type = \"single_label_classification\"\n","                else:\n","                    self.config.problem_type = \"multi_label_classification\"\n","\n","            if self.config.problem_type == \"regression\":\n","                loss_fct = nn.MSELoss()\n","                if self.num_labels == 1:\n","                    loss = loss_fct(logits.squeeze(), labels.squeeze())\n","                else:\n","                    loss = loss_fct(logits, labels)\n","            elif self.config.problem_type == \"single_label_classification\":\n","                loss_fct = nn.CrossEntropyLoss()\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","            elif self.config.problem_type == \"multi_label_classification\":\n","                loss_fct = nn.BCEWithLogitsLoss()\n","                loss = loss_fct(logits, labels)\n","\n","        if not return_dict:\n","            output = (logits,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return SequenceClassifierOutput(\n","            loss=loss,\n","            logits=logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )\n","checkpoint_path = './checkpoints/RobertaCNN.pth'     \n","model = RobertaForSequenceClassification('vinai/phobert-base', config=config)\n","model.cuda()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":227},"id":"bzvbIKW6pcmW","executionInfo":{"status":"error","timestamp":1640252998857,"user_tz":-420,"elapsed":632,"user":{"displayName":"Duy Trần Quang","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gij1youI44C-pViNsGwdbcY5YEbgeVxxf70O_ZNIA=s64","userId":"02731206869052649620"}},"outputId":"1bb7025b-d949-4703-ad22-538afaa1bfc5"},"execution_count":18,"outputs":[{"output_type":"error","ename":"TypeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m<ipython-input-18-fd5587376af0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     97\u001b[0m         )\n\u001b[1;32m     98\u001b[0m \u001b[0mcheckpoint_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'./checkpoints/RobertaCNN.pth'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 99\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mRobertaForSequenceClassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'vinai/phobert-base'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    100\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: __init__() got multiple values for argument 'config'"]}]},{"cell_type":"markdown","source":["## Phobert + LSTM + CRF + Tiki dataset"],"metadata":{"id":"Y5i4fuQrHUne"}},{"cell_type":"markdown","source":["https://github.com/hemingkx/CLUENER2020/tree/main/BERT-Softmax"],"metadata":{"id":"-YkZxXqfF2vY"}},{"cell_type":"code","source":["def softmax(x):\n","    \"\"\"Compute softmax values for each sets of scores in x.\"\"\"\n","    return np.exp(x) / np.sum(np.exp(x), axis=0)"],"metadata":{"id":"hJyszNxPx5Q2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Defining the training function on the 80% of the dataset for tuning the bert model\n","def train_modified(epoch, verbose = False):\n","    tr_loss, tr_accuracy = 0, 0\n","    nb_tr_examples, nb_tr_steps = 0, 0\n","    tr_preds, tr_labels = [], []\n","    labels_sent_ar = []\n","    y_preds = None\n","    # put model in training mode\n","    model.train()\n","    \n","    for idx, batch in enumerate(train_loader):\n","        ids, labels, labels_sent = batch\n","        ids = ids.to(device)\n","        labels = labels.to(device)\n","        labels_sent = labels_sent.to(device)\n","        mask = ids!=1\n","        \n","        outputs = model(input_ids=ids, attention_mask=mask, labels=labels, labels_sent=labels_sent)\n","        loss = outputs[0]\n","        # predictions = outputs[1]\n","        # print('prediction:', predictions, predictions.shape)\n","\n","        y_pred = outputs[2].squeeze().detach().cpu().numpy()\n","        y_preds = np.atleast_1d(y_pred) if y_preds is None else np.concatenate([y_preds, np.atleast_1d(y_pred)])\n","                \n","        # tr_logits = outputs[1]\n","        tr_loss += loss.item()\n","\n","        nb_tr_steps += 1\n","        nb_tr_examples += labels.size(0)\n","        \n","        if idx % 100==0 and verbose:\n","            loss_step = tr_loss/nb_tr_steps\n","            print(f\"Training loss per 100 training steps: {loss_step}\")\n","           \n","        # compute training accuracy\n","        flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","        # active_logits = tr_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","        # flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","        flattened_predictions = outputs[1].view(-1)\n","        \n","        # only compute accuracy at active labels\n","        # print(labels)\n","        active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","        active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n","        \n","        labels = torch.masked_select(flattened_targets, active_accuracy)\n","        # print(flattened_predictions.shape, active_accuracy.shape)\n","        predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","        \n","        tr_labels.extend(labels)\n","        tr_preds.extend(predictions)\n","        labels_sent_ar.extend(labels_sent.cpu().detach().numpy())\n","\n","        # tmp_tr_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n","        # tr_accuracy += tmp_tr_accuracy\n","    \n","        # gradient clipping\n","        torch.nn.utils.clip_grad_norm_(\n","            parameters=model.parameters(), max_norm=MAX_GRAD_NORM\n","        )\n","        \n","        # backward pass\n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","\n","    epoch_loss = tr_loss / nb_tr_steps\n","    tr_accuracy = tr_accuracy / nb_tr_steps\n","\n","    labels = [ids_to_labels[id.item()] for id in tr_labels]\n","    # print(tr_preds, tr_preds.shape)\n","    predictions = [ids_to_labels[id.item()] for id in tr_preds]\n","    # labels_se = [ids_to_labels[id.item()] for id in labels_sent_ar]\n","    # print(val_preds.shape)\n","    # print(len(labels_sent_ar))\n","    # print(labels_sent_ar.shape)\n","    f1_token = seqeval.metrics.f1_score([labels], [predictions])\n","    val_preds = np.argmax(softmax(y_preds), axis=1)\n","    # for item in zip(labels_sent_ar, val_preds):\n","    #   print(item)\n","    f1_sent = f1_score(labels_sent_ar, val_preds, average='micro')\n","\n","    print(f\"Training loss epoch: {epoch_loss}\", f\"Training F1 epoch: {f1_token}\")\n","    print(f\"Training loss epoch: {epoch_loss}\", f\"Training F1 epoch: {f1_sent}\")"],"metadata":{"id":"uRgQ7asQ5XhH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def valid_modified(model, test_loader, verbose=False):\n","    # put model in evaluation mode\n","    model.eval()\n","    \n","    eval_loss, eval_accuracy = 0, 0\n","    nb_eval_examples, nb_eval_steps = 0, 0\n","    eval_preds, eval_labels = [], []\n","    labels_sent_ar = []\n","    val_preds = None\n","    \n","    with torch.no_grad():\n","        for idx, batch in enumerate(test_loader):\n","            \n","            ids, labels, labels_sent = batch\n","            ids = ids.to(device)\n","            labels = labels.to(device)\n","            labels_sent = labels_sent.to(device)\n","            mask = ids!=1 \n","\n","            outputs = model(input_ids=ids, attention_mask=mask, labels=labels, labels_sent=labels_sent)\n","            loss = outputs[0]\n","            # eval_logits = outputs[1]\n","\n","            y_pred = outputs[2].squeeze().detach().cpu().numpy()\n","            # print(y_pred.shape)\n","            val_preds = np.atleast_1d(y_pred) if val_preds is None else np.concatenate([val_preds, np.atleast_1d(y_pred)])\n","\n","            eval_loss += loss.item()\n","\n","            nb_eval_steps += 1\n","            nb_eval_examples += labels.size(0)\n","        \n","            if idx % 100==0 and verbose:\n","                loss_step = eval_loss/nb_eval_steps\n","                print(f\"Validation loss per 100 evaluation steps: {loss_step}\")\n","              \n","            # compute evaluation accuracy\n","            flattened_targets = labels.view(-1) # shape (batch_size * seq_len,)\n","            # active_logits = eval_logits.view(-1, model.num_labels) # shape (batch_size * seq_len, num_labels)\n","            # flattened_predictions = torch.argmax(active_logits, axis=1) # shape (batch_size * seq_len,)\n","            flattened_predictions = outputs[1].view(-1)\n","            \n","            # only compute accuracy at active labels\n","            active_accuracy = labels.view(-1) != -100 # shape (batch_size, seq_len)\n","            active_labels = torch.where(active_accuracy, labels.view(-1), torch.tensor(-100).type_as(labels))\n","\n","            labels = torch.masked_select(flattened_targets, active_accuracy)\n","            predictions = torch.masked_select(flattened_predictions, active_accuracy)\n","            \n","            eval_labels.extend(labels)\n","            eval_preds.extend(predictions)\n","            labels_sent_ar.extend(labels_sent.cpu().detach().numpy())\n","            \n","            # tmp_eval_accuracy = accuracy_score(labels.cpu().numpy(), predictions.cpu().numpy())\n","            # eval_accuracy += tmp_eval_accuracy\n","\n","    labels = [ids_to_labels[id.item()] for id in eval_labels]\n","    predictions = [ids_to_labels[id.item()] for id in eval_preds]\n","    # labels_se = [ids_to_labels[id.item()] for id in labels_sent_ar]\n","    \n","    eval_loss = eval_loss / nb_eval_steps\n","    # eval_accuracy = eval_accuracy / nb_eval_steps\n","    f1_token = seqeval.metrics.f1_score([labels], [predictions])\n","    val_preds = np.argmax(softmax(val_preds), axis=1)\n","    # print(val_preds.shape)\n","    # print(len(labels_sent_ar))\n","    # print(labels_sent_ar.shape)\n","    f1_sent = f1_score(labels_sent_ar, val_preds, average='micro')\n","\n","    print(f\"Validation Loss: {eval_loss}\", f\"Validation F1: {f1_token}\")\n","    print(f\"Validation Loss: {eval_loss}\", f\"Validation F1: {f1_sent}\")\n","\n","    return labels, predictions"],"metadata":{"id":"LaGsKxg3XVwE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn.functional as F\n","EMBEDDING_SIZE = 768\n","NUM_FILTERS = 10\n","\n","class RobertaLSTMCRF(RobertaPreTrainedModel):\n","\n","    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n","    _keys_to_ignore_on_load_missing = [r\"position_ids\"]\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.roberta = RobertaModel(config, add_pooling_layer=False)\n","        classifier_dropout = (\n","            config.classifier_dropout if config.classifier_dropout is not None else config.hidden_dropout_prob\n","        )\n","        self.dropout = nn.Dropout(classifier_dropout)\n","        self.bilstm = nn.LSTM(config.hidden_size, (config.hidden_size) // 2, dropout=0.1, batch_first=True, bidirectional=True)\n","        self.classifier = nn.Linear(config.hidden_size, self.num_labels)\n","\n","        self.crf = CRF(num_tags=self.num_labels, batch_first=True)\n","        self.post_init()\n","\n","        window_sizes=(1,2,3,5)\n","\n","        self.convs = nn.ModuleList([nn.Conv2d(1, NUM_FILTERS, [window_size, EMBEDDING_SIZE], padding=(window_size - 1, 0)) for window_size in window_sizes])\n","       \n","        self.fc = nn.Linear(NUM_FILTERS * len(window_sizes), 3)\n","\n","        # self.init_weights()\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        labels_sent=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","        r\"\"\"\n","        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n","            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n","            1]``.\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        outputs = self.roberta(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=True,\n","            return_dict=return_dict,\n","        )\n","        # print(type(outputs[0]))\n","        # print(outputs[0].shape)\n","        # print(len(outputs[1]))\n","\n","        sequence_output = outputs[0]\n","        sequence_output = self.dropout(sequence_output)\n","        lstm_output, hc = self.bilstm(sequence_output)\n","        logits_lstm = self.classifier(lstm_output)\n","\n","        cls_output = torch.cat((outputs[1][-1][:,0, ...],outputs[1][-2][:,0, ...], outputs[1][-3][:,0, ...], outputs[1][-4][:,0, ...]),-1)\n","\n","        xs = []\n","        for conv in self.convs:\n","            x2 = F.relu(conv(outputs[0].unsqueeze(1)))\n","            x2 = torch.squeeze(x2, -1)\n","            x2 = F.max_pool1d(x2, x2.size(2))\n","            xs.append(x2)\n","        x = torch.cat(xs, 2)       \n","        x = x.view(x.size(0), -1)\n","        logits = self.fc(x)\n","\n","        loss_cls = nn.CrossEntropyLoss()\n","        # print(logits.shape)\n","        # print(labels_sent.shape)\n","        loss_cls =  loss_cls(logits, labels_sent.squeeze(1))\n","\n","        loss = None\n","        if labels is not None:\n","            labels[labels==-100] = 0\n","            log_likelihood, tags = self.crf(logits_lstm, labels), self.crf.decode(logits_lstm)\n","            loss = 0 - log_likelihood\n","        else:\n","            tags = self.crf.decode(logits_lstm)\n","        tags = torch.Tensor(tags)\n","        tags = tags.to(device)\n","\n","        if not return_dict:\n","            output = (tags,) + outputs[2:]\n","            return (((0.05 * loss)+loss_cls,) + output) if loss is not None else output\n","\n","        return (0.05 * loss)+loss_cls, tags, logits"],"metadata":{"id":"U4AjvocHHY12"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = RobertaLSTMCRF.from_pretrained('vinai/phobert-base', num_labels=5)\n","model.cuda()"],"metadata":{"id":"moBnXhdFpb2M"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Creating optimizer and lr schedulers\n","param_optimizer = list(model.named_parameters())\n","no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n","]\n","num_train_optimization_steps = int(args.epochs*len(train_dataset)/args.batch_size/args.accumulation_steps)\n","optimizer = AdamW(optimizer_grouped_parameters, lr=5e-5, correct_bias=False)  # To reproduce BertAdam specific behavior set correct_bias=False\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=100, num_training_steps=num_train_optimization_steps)  # PyTorch scheduler\n","scheduler0 = get_constant_schedule(optimizer)  # PyTorch scheduler\n","\n","tsfm = model.roberta\n","for child in tsfm.children():\n","    for param in child.parameters():\n","        if not param.requires_grad:\n","            print(\"whoopsies\")\n","        param.requires_grad = False\n","frozen = True\n","\n","EPOCHS = 10\n","MAX_GRAD_NORM = 1\n","\n","import time\n","\n","for epoch in range(EPOCHS):\n","    if epoch > 0 and frozen:\n","        for child in tsfm.children():\n","            for param in child.parameters():\n","                param.requires_grad = True\n","        frozen = False\n","        del scheduler0\n","        torch.cuda.empty_cache()\n","    st = time.time()\n","    print(f\"Training epoch: {epoch + 1}\")\n","    train_modified(epoch)\n","    labels, predictions = valid_modified(model, valid_loader)\n","    \n","    print('Time: ',time.time() - st)"],"metadata":{"id":"Ygafo_w0WzoM","colab":{"base_uri":"https://localhost:8080/"},"outputId":"10965a3c-06dd-4ac8-d41d-c828976e887b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Training epoch: 1\n","Training loss epoch: 3.469600145206895 Training F1 epoch: 0.3770815692915609\n","Training loss epoch: 3.469600145206895 Training F1 epoch: 0.5923392612859097\n","Validation Loss: 1.5113120947798637 Validation F1: 0.6185177409195884\n","Validation Loss: 1.5113120947798637 Validation F1: 0.7109589041095891\n","Time:  302.3198871612549\n","Training epoch: 2\n","Training loss epoch: 1.5101695066437675 Training F1 epoch: 0.7605388516109205\n","Training loss epoch: 1.5101695066437675 Training F1 epoch: 0.7055403556771546\n","Validation Loss: 0.8301835503888457 Validation F1: 0.8645441961637335\n","Validation Loss: 0.8301835503888457 Validation F1: 0.7575342465753426\n","Time:  377.74051785469055\n","Training epoch: 3\n","Training loss epoch: 0.8452368989566446 Training F1 epoch: 0.8881943021110883\n","Training loss epoch: 0.8452368989566446 Training F1 epoch: 0.8033515731874145\n","Validation Loss: 0.7875342660417703 Validation F1: 0.897861283230062\n","Validation Loss: 0.7875342660417703 Validation F1: 0.7698630136986301\n","Time:  374.27558422088623\n","Training epoch: 4\n","Training loss epoch: 0.5873615878816343 Training F1 epoch: 0.9282164267049239\n","Training loss epoch: 0.5873615878816343 Training F1 epoch: 0.8543091655266758\n","Validation Loss: 0.7899781086622444 Validation F1: 0.9137137137137138\n","Validation Loss: 0.7899781086622444 Validation F1: 0.7808219178082192\n","Time:  369.7823257446289\n","Training epoch: 5\n","Training loss epoch: 0.42913257119165565 Training F1 epoch: 0.944128059104202\n","Training loss epoch: 0.42913257119165565 Training F1 epoch: 0.9008207934336526\n","Validation Loss: 0.8059895885495902 Validation F1: 0.9074889867841411\n","Validation Loss: 0.8059895885495902 Validation F1: 0.7657534246575344\n","Time:  375.42671751976013\n","Training epoch: 6\n","Training loss epoch: 0.4118133314646954 Training F1 epoch: 0.9433690280065898\n","Training loss epoch: 0.4118133314646954 Training F1 epoch: 0.9103967168262654\n","Validation Loss: 0.896840872957486 Validation F1: 0.9098490865766481\n","Validation Loss: 0.896840872957486 Validation F1: 0.7315068493150684\n","Time:  375.23667216300964\n","Training epoch: 7\n","Training loss epoch: 0.2975519739764089 Training F1 epoch: 0.9620721554116559\n","Training loss epoch: 0.2975519739764089 Training F1 epoch: 0.9257865937072504\n","Validation Loss: 0.9583356384187937 Validation F1: 0.9258436047686401\n","Validation Loss: 0.9583356384187937 Validation F1: 0.7301369863013699\n","Time:  372.0978651046753\n","Training epoch: 8\n","Training loss epoch: 0.2326524364010325 Training F1 epoch: 0.9646104062869177\n","Training loss epoch: 0.2326524364010325 Training F1 epoch: 0.948358413132695\n","Validation Loss: 1.030243815504627 Validation F1: 0.9146268656716419\n","Validation Loss: 1.030243815504627 Validation F1: 0.7643835616438355\n","Time:  372.6890277862549\n","Training epoch: 9\n","Training loss epoch: 0.20030326845771576 Training F1 epoch: 0.9703559339419427\n","Training loss epoch: 0.20030326845771576 Training F1 epoch: 0.9606703146374829\n","Validation Loss: 1.1552096063851647 Validation F1: 0.9102818578801112\n","Validation Loss: 1.1552096063851647 Validation F1: 0.758904109589041\n","Time:  371.17238330841064\n","Training epoch: 10\n","Training loss epoch: 0.17186744575593849 Training F1 epoch: 0.9742958288440821\n","Training loss epoch: 0.17186744575593849 Training F1 epoch: 0.9678522571819426\n","Validation Loss: 1.1322806003018704 Validation F1: 0.9210050251256281\n","Validation Loss: 1.1322806003018704 Validation F1: 0.7506849315068492\n","Time:  370.91458678245544\n"]}]},{"cell_type":"code","source":["a = torch.rand((2,3), dtype=torch.float32)\n","b = torch.tensor([[1],[2]], dtype=torch.long)\n","l = nn.CrossEntropyLoss()\n"],"metadata":{"id":"1tl_ObVfpWv4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["l(a,b.squeeze())"],"metadata":{"id":"t0kh-vYoxh6Z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["b.squeeze().shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RIG4Njwxxv67","outputId":"5ef8fdac-d6a9-44ee-9c2f-cd250fe0bc8a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([2])"]},"metadata":{},"execution_count":48}]},{"cell_type":"code","source":["torch.rand((1,1)).squeeze(1).shape"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IxYQjIRCydkO","outputId":"fecdf865-221b-4e65-b810-c2b5af002eed"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["torch.Size([1])"]},"metadata":{},"execution_count":51}]},{"cell_type":"markdown","source":["# Temp\n"],"metadata":{"id":"7k1WqTO59A5Z"}},{"cell_type":"code","source":["from vncorenlp import VnCoreNLP\n","\n","# To perform word segmentation, POS tagging, NER and then dependency parsing\n","annotator = VnCoreNLP(\"/content/VnCoreNLP/VnCoreNLP-1.1.1.jar\", annotators=\"wseg,pos,ner,parse\", max_heap_size='-Xmx2g') \n"," \n","    \n","# Input \n","text = \"Ông Nguyễn Khắc Chúc  đang làm việc tại Đại học Quốc gia Hà Nội. Bà Lan, vợ ông Chúc, cũng làm việc tại đây.\"\n","\n","# To perform word segmentation, POS tagging, NER and then dependency parsing\n","annotated_text = annotator.annotate(text)\n","\n","# To perform word segmentation only\n","word_segmented_text = annotator.tokenize(text) \n"],"metadata":{"id":"mYzxKxlX9CYp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Input \n","text = \"thô lỗ\"\n","\n","# To perform word segmentation, POS tagging, NER and then dependency parsing\n","annotated_text = annotator.annotate(text)\n","\n","# To perform word segmentation only\n","word_segmented_text = annotator.tokenize(text) \n","word_segmented_text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"32MdsTte-B-R","outputId":"56323918-7024-491d-b21d-a07ae9a78c5a"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[['thô_lỗ']]"]},"metadata":{},"execution_count":16}]},{"cell_type":"code","source":[""],"metadata":{"id":"eSobWje-MLGt"},"execution_count":null,"outputs":[]}]}