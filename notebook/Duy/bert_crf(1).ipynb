{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"bert_crf.ipynb","provenance":[],"collapsed_sections":["unOcWNMX2haV","qJozAjT4BIrv","V6VpSZb02j8h","t2K1K2UD9Gmh"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"55adb9b7dc1540c7b974e03882095ade":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0541bb0057a54acc9122987876c31318","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_acabe51d8d394d0b8b5a5ae555d701d5","IPY_MODEL_9d8ac6668f8843fdae798077e9c1f3c3","IPY_MODEL_676e0f1d4aeb49caa44bb6bc6cefbc27"]}},"0541bb0057a54acc9122987876c31318":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"acabe51d8d394d0b8b5a5ae555d701d5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_145ddc8f231c4da399328922dae88eab","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_36cc7a2074a148e58223be859dd88bf4"}},"9d8ac6668f8843fdae798077e9c1f3c3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_7261c4da6a154d0aab303fb5b057e1f6","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":2,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":2,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e5f00893934e43fe9668cbc5204fd6d1"}},"676e0f1d4aeb49caa44bb6bc6cefbc27":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_3ceba6d962154169b5b471d193e90cd8","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 2/2 [00:00&lt;00:00, 41.09it/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_21fada6ebe094405a3d2b4aa6276f5b6"}},"145ddc8f231c4da399328922dae88eab":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"36cc7a2074a148e58223be859dd88bf4":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7261c4da6a154d0aab303fb5b057e1f6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"e5f00893934e43fe9668cbc5204fd6d1":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"3ceba6d962154169b5b471d193e90cd8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"21fada6ebe094405a3d2b4aa6276f5b6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"8f4efd576ba8420c9dcab0753f20fcc5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_82db1d6029764377903e3f537b184f73","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f897b9de1708413eb07ddb805a530519","IPY_MODEL_db81a86f197b4b77be22ac80722578a3","IPY_MODEL_7a984c6be93248ccbb75825ad45c2003"]}},"82db1d6029764377903e3f537b184f73":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f897b9de1708413eb07ddb805a530519":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2cf02e8e54fe4347bfd2f927fa63b9bc","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_04792a3e4c68461ab70a6ade63e891e5"}},"db81a86f197b4b77be22ac80722578a3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_9ae34c49ce48488d98179b5823caf3f6","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":213450,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":213450,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_8b7334664d5b4847ad1364cce90213bd"}},"7a984c6be93248ccbb75825ad45c2003":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f98dfe2ff4ea473fa2ea162c5af14157","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 208k/208k [00:00&lt;00:00, 1.04MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_c558ce04bec947dcbde46bb42d841a15"}},"2cf02e8e54fe4347bfd2f927fa63b9bc":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"04792a3e4c68461ab70a6ade63e891e5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"9ae34c49ce48488d98179b5823caf3f6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"8b7334664d5b4847ad1364cce90213bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f98dfe2ff4ea473fa2ea162c5af14157":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"c558ce04bec947dcbde46bb42d841a15":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"a68c9e3ee80a49759800296bcab4c77c":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_0db3d8ee13c24b2c9f1d8669de1bbb2e","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_05228d81338e44e0a0ddc0ec412a0907","IPY_MODEL_58709aff78e9470390cb3e92abbba2b3","IPY_MODEL_e72fc5a90a0f493da8a5dbec559cb7e4"]}},"0db3d8ee13c24b2c9f1d8669de1bbb2e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"05228d81338e44e0a0ddc0ec412a0907":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_b27dd9de4b5b4721b34490f34f33b683","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_9e624ac1912f44b690eef482467d390c"}},"58709aff78e9470390cb3e92abbba2b3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5851052c4b6942829649634aa61dc8f5","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":435797,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":435797,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_33c61eb87b0542b6affd42718f5e51da"}},"e72fc5a90a0f493da8a5dbec559cb7e4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_da890f61fb274c9c8f9e23c3d1a9c475","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 426k/426k [00:00&lt;00:00, 1.02MB/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_7658a7a8ffd442aca6df4f0b32bf9153"}},"b27dd9de4b5b4721b34490f34f33b683":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"9e624ac1912f44b690eef482467d390c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5851052c4b6942829649634aa61dc8f5":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"33c61eb87b0542b6affd42718f5e51da":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"da890f61fb274c9c8f9e23c3d1a9c475":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"7658a7a8ffd442aca6df4f0b32bf9153":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f7ac2c89eedf47cdb800ad3936e989e5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_894fdb3ac7d047629ced0946da06b6c9","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_398b40608c0f4980b72b3fc4ef894c94","IPY_MODEL_72da733a43fb4e19975dd05b4b950218","IPY_MODEL_7675029694004a7e95b56552763ad100"]}},"894fdb3ac7d047629ced0946da06b6c9":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"398b40608c0f4980b72b3fc4ef894c94":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_333f9a97cc704f788d6a688472150472","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"Downloading: 100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_e7b38b48424949d89505f7697a5cb768"}},"72da733a43fb4e19975dd05b4b950218":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_12eb1c54ba484fbda5f3db62d474bf73","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":29,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":29,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_24f107c42a494a3ab349dbe22e585098"}},"7675029694004a7e95b56552763ad100":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_f00cc6ea88c340ab9e022a4c0caea067","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 29.0/29.0 [00:00&lt;00:00, 680B/s]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_19c95b3718b44bfca1d479f89f9ac78a"}},"333f9a97cc704f788d6a688472150472":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"e7b38b48424949d89505f7697a5cb768":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"12eb1c54ba484fbda5f3db62d474bf73":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"24f107c42a494a3ab349dbe22e585098":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f00cc6ea88c340ab9e022a4c0caea067":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"19c95b3718b44bfca1d479f89f9ac78a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"b9eb1241f85e46f9bc7dd800762cd3ee":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_41f2d12d145b45a2b1d5c2984111f147","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f5b222b5c2c14fce8912fb559ecfe5f5","IPY_MODEL_c962fa997fd8415a8fbb042db0420a20","IPY_MODEL_2bf4c3ab05bd4095ba306361791838a5"]}},"41f2d12d145b45a2b1d5c2984111f147":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f5b222b5c2c14fce8912fb559ecfe5f5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_2a618e34a00441b4a5b434fece7c331e","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_578ac25082b04a3f81ba9d27e8be98a0"}},"c962fa997fd8415a8fbb042db0420a20":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_c509bb293b354c54b5db57f44d7d714c","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_a743ceb67cbf4c73a0842cf8f31f7edf"}},"2bf4c3ab05bd4095ba306361791838a5":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_01b907b208d74f319aa17e978d44e65c","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:07&lt;00:00,  7.13s/ba]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_655b4a271d5b4f97beb21271052ad298"}},"2a618e34a00441b4a5b434fece7c331e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"578ac25082b04a3f81ba9d27e8be98a0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"c509bb293b354c54b5db57f44d7d714c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"a743ceb67cbf4c73a0842cf8f31f7edf":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"01b907b208d74f319aa17e978d44e65c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"655b4a271d5b4f97beb21271052ad298":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"471029247332469cb1859523a625c815":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_view_name":"HBoxView","_dom_classes":[],"_model_name":"HBoxModel","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.5.0","box_style":"","layout":"IPY_MODEL_ecfc28a044434f519081ca84c5e2398a","_model_module":"@jupyter-widgets/controls","children":["IPY_MODEL_f9a24f8f29f54fa883a068def27d8598","IPY_MODEL_381eae1f4ba847ec9ff5a02de5c021d3","IPY_MODEL_3966a6d313b1401ebc716c10a78c1282"]}},"ecfc28a044434f519081ca84c5e2398a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"f9a24f8f29f54fa883a068def27d8598":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_53b911d3c0c14fa7908d972c4f9aa47a","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":"100%","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_da49104d929643c6a17373e7560e525a"}},"381eae1f4ba847ec9ff5a02de5c021d3":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_view_name":"ProgressView","style":"IPY_MODEL_5ebd3b3c22694f9e870172b8706d9ea3","_dom_classes":[],"description":"","_model_name":"FloatProgressModel","bar_style":"success","max":1,"_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":1,"_view_count":null,"_view_module_version":"1.5.0","orientation":"horizontal","min":0,"description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_343dcb667f6e4323b3c73f04c7ebf84a"}},"3966a6d313b1401ebc716c10a78c1282":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_view_name":"HTMLView","style":"IPY_MODEL_7b3dd81768be4900afd7fb11cf189b29","_dom_classes":[],"description":"","_model_name":"HTMLModel","placeholder":"​","_view_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","value":" 1/1 [00:01&lt;00:00,  1.54s/ba]","_view_count":null,"_view_module_version":"1.5.0","description_tooltip":null,"_model_module":"@jupyter-widgets/controls","layout":"IPY_MODEL_ede237c111ff474ea90f0b965ae258c8"}},"53b911d3c0c14fa7908d972c4f9aa47a":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"da49104d929643c6a17373e7560e525a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"5ebd3b3c22694f9e870172b8706d9ea3":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"ProgressStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","bar_color":null,"_model_module":"@jupyter-widgets/controls"}},"343dcb667f6e4323b3c73f04c7ebf84a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}},"7b3dd81768be4900afd7fb11cf189b29":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_view_name":"StyleView","_model_name":"DescriptionStyleModel","description_width":"","_view_module":"@jupyter-widgets/base","_model_module_version":"1.5.0","_view_count":null,"_view_module_version":"1.2.0","_model_module":"@jupyter-widgets/controls"}},"ede237c111ff474ea90f0b965ae258c8":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_view_name":"LayoutView","grid_template_rows":null,"right":null,"justify_content":null,"_view_module":"@jupyter-widgets/base","overflow":null,"_model_module_version":"1.2.0","_view_count":null,"flex_flow":null,"width":null,"min_width":null,"border":null,"align_items":null,"bottom":null,"_model_module":"@jupyter-widgets/base","top":null,"grid_column":null,"overflow_y":null,"overflow_x":null,"grid_auto_flow":null,"grid_area":null,"grid_template_columns":null,"flex":null,"_model_name":"LayoutModel","justify_items":null,"grid_row":null,"max_height":null,"align_content":null,"visibility":null,"align_self":null,"height":null,"min_height":null,"padding":null,"grid_auto_rows":null,"grid_gap":null,"max_width":null,"order":null,"_view_module_version":"1.2.0","grid_template_areas":null,"object_position":null,"object_fit":null,"grid_auto_columns":null,"margin":null,"display":null,"left":null}}}}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"U37U25jRvEnl","executionInfo":{"status":"ok","timestamp":1639582582769,"user_tz":-420,"elapsed":4177,"user":{"displayName":"Quang Duy Trần","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvMaEX5IKcYvanQuv2HTQEdjP5mTCjGM4CkVJX=s64","userId":"16019943656023573816"}},"outputId":"0d7ae009-6e34-4a29-ec9b-1f33819bd75e"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pytorch-crf\n","  Downloading pytorch_crf-0.7.2-py3-none-any.whl (9.5 kB)\n","Installing collected packages: pytorch-crf\n","Successfully installed pytorch-crf-0.7.2\n"]}],"source":["!pip install pytorch-crf"]},{"cell_type":"markdown","source":["### CRF"],"metadata":{"id":"unOcWNMX2haV"}},{"cell_type":"code","source":["import torch \n","from torchcrf import CRF\n","num_tags = 5\n","model = CRF(num_tags)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SllXAMI2vJkg","executionInfo":{"status":"ok","timestamp":1639582616461,"user_tz":-420,"elapsed":8,"user":{"displayName":"Quang Duy Trần","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvMaEX5IKcYvanQuv2HTQEdjP5mTCjGM4CkVJX=s64","userId":"16019943656023573816"}},"outputId":"47c35cfb-4b97-4522-c8a6-31f7a19560f3"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["CRF(num_tags=5)"]},"metadata":{},"execution_count":2}]},{"cell_type":"code","source":["print(model)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bdhDbhmkvUQC","executionInfo":{"status":"ok","timestamp":1639582630583,"user_tz":-420,"elapsed":3,"user":{"displayName":"Quang Duy Trần","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvMaEX5IKcYvanQuv2HTQEdjP5mTCjGM4CkVJX=s64","userId":"16019943656023573816"}},"outputId":"c921d9a4-a2ba-4c5a-f76d-85cef0e02706"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["CRF(num_tags=5)\n"]}]},{"cell_type":"code","source":["seq_length = 3  # maximum sequence length in a batch\n","batch_size = 2  # number of samples in the batch\n","emissions = torch.randn(seq_length, batch_size, num_tags)\n","tags = torch.tensor([\n","    [0, 1], [2, 4], [3, 1]\n","    ], dtype=torch.long)  # (seq_length, batch_size)"],"metadata":{"id":"WeVxTJCfvY2b"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model(emissions, tags)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FWBSadaBvi4n","executionInfo":{"status":"ok","timestamp":1639582676956,"user_tz":-420,"elapsed":2,"user":{"displayName":"Quang Duy Trần","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvMaEX5IKcYvanQuv2HTQEdjP5mTCjGM4CkVJX=s64","userId":"16019943656023573816"}},"outputId":"79eb9505-cd28-4bfa-9093-c759aab33832"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torchcrf/__init__.py:249: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  ../aten/src/ATen/native/TensorCompare.cpp:328.)\n","  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"]},{"output_type":"execute_result","data":{"text/plain":["tensor(-9.3489, grad_fn=<SumBackward0>)"]},"metadata":{},"execution_count":6}]},{"cell_type":"code","source":["# mask size is (seq_length, batch_size)\n","# the last sample has length of 1\n","mask = torch.tensor([\n","   [1, 1], [1, 1], [1, 0] ], dtype=torch.uint8)\n","model(emissions, tags, mask=mask)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WRyOTZ_dvoww","executionInfo":{"status":"ok","timestamp":1639582725145,"user_tz":-420,"elapsed":559,"user":{"displayName":"Quang Duy Trần","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvMaEX5IKcYvanQuv2HTQEdjP5mTCjGM4CkVJX=s64","userId":"16019943656023573816"}},"outputId":"70b84dcf-7d36-4ab0-9432-8d00a5ca5075"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["tensor(-7.8509, grad_fn=<SumBackward0>)"]},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["model.decode(emissions)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HzPr-D8wv3bA","executionInfo":{"status":"ok","timestamp":1639582761854,"user_tz":-420,"elapsed":554,"user":{"displayName":"Quang Duy Trần","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvMaEX5IKcYvanQuv2HTQEdjP5mTCjGM4CkVJX=s64","userId":"16019943656023573816"}},"outputId":"d5f18aa5-9227-480a-d764-5192d4f976d7"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[[4, 2, 3], [1, 1, 3]]"]},"metadata":{},"execution_count":8}]},{"cell_type":"markdown","source":["### Simple BiLSTM + CRF\n","\n","https://github.com/jidasheng/bi-lstm-crf"],"metadata":{"id":"qJozAjT4BIrv"}},{"cell_type":"code","source":["!pip install bi-lstm-crf"],"metadata":{"id":"NEhKT5zaHE93"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn as nn\n","from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n","from bi_lstm_crf import CRF\n","\n","\n","class BiRnnCrf(nn.Module):\n","    def __init__(self, vocab_size, tagset_size, embedding_dim, hidden_dim, num_rnn_layers=1, rnn=\"lstm\"):\n","        super(BiRnnCrf, self).__init__()\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.vocab_size = vocab_size\n","        self.tagset_size = tagset_size\n","\n","        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n","        RNN = nn.LSTM if rnn == \"lstm\" else nn.GRU\n","        self.rnn = RNN(embedding_dim, hidden_dim // 2, num_layers=num_rnn_layers,\n","                       bidirectional=True, batch_first=True)\n","        self.crf = CRF(hidden_dim, self.tagset_size)\n","\n","    def __build_features(self, sentences):\n","        masks = sentences.gt(0)\n","        embeds = self.embedding(sentences.long())\n","        # print(masks)\n","        seq_length = masks.sum(1)\n","        sorted_seq_length, perm_idx = seq_length.sort(descending=True)\n","        embeds = embeds[perm_idx, :]\n","\n","        pack_sequence = pack_padded_sequence(embeds, lengths=sorted_seq_length, batch_first=True)\n","        packed_output, _ = self.rnn(pack_sequence)\n","        lstm_out, _ = pad_packed_sequence(packed_output, batch_first=True)\n","        _, unperm_idx = perm_idx.sort()\n","        lstm_out = lstm_out[unperm_idx, :]\n","\n","        return lstm_out, masks\n","\n","    def loss(self, xs, tags):\n","        features, masks = self.__build_features(xs)\n","        loss = self.crf.loss(features, tags, masks=masks)\n","        return loss\n","\n","    def forward(self, xs):\n","        # Get the emission scores from the BiLSTM\n","        features, masks = self.__build_features(xs)\n","        scores, tag_seq = self.crf(features, masks)\n","        return scores, tag_seq"],"metadata":{"id":"Ra2pflbZKQ9u"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\"\"\"\n","from bi_lstm_crf import CRF\n","\n","# a BERT-CRF model for sequence tagging\n","class BertCrf(nn.Module):\n","    def __init__(self, ...):\n","        ...\n","        self.bert = BERT(...)\n","        self.crf = CRF(in_features, num_tags)\n","\n","    def loss(self, xs, tags):\n","        features, = self.bert(xs)\n","        masks = xs.gt(0)\n","        loss = self.crf.loss(features, tags, masks)\n","        return loss\n","\n","    def forward(self, xs):\n","        features, = self.bert(xs)\n","        masks = xs.gt(0)\n","        scores, tag_seq = self.crf(features, masks)\n","        return scores, tag_seq\n","\"\"\""],"metadata":{"id":"rdFMw8VTKO_w"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def argmax(vec):\n","    # return the argmax as a python int\n","    _, idx = torch.max(vec, 1)\n","    return idx.item()\n","\n","\n","def prepare_sequence(seq, to_ix):\n","    idxs = [to_ix[w] for w in seq]\n","    return torch.tensor(idxs, dtype=torch.long)\n","\n","\n","# Compute log sum exp in a numerically stable way for the forward algorithm\n","def log_sum_exp(vec):\n","    max_score = vec[0, argmax(vec)]\n","    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n","    return max_score + \\\n","        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"],"metadata":{"id":"k6Wio6nqyjEL"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["START_TAG = \"<START>\"\n","STOP_TAG = \"<STOP>\"\n","EMBEDDING_DIM = 5\n","HIDDEN_DIM = 4"],"metadata":{"id":"L3spHTlcyl9y"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Make up some training data\n","training_data = [(\n","    \"the wall street journal reported today that apple corporation made money\".split(),\n","    \"B I I I O O O B I O O\".split()\n","), (\n","    \"georgia tech is a university in georgia\".split(),\n","    \"B I O O O O B\".split()\n",")]\n","\n","word_to_ix = {}\n","for sentence, tags in training_data:\n","    for word in sentence:\n","        if word not in word_to_ix:\n","            word_to_ix[word] = len(word_to_ix)\n","\n","tag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4}\n","\n","model = BiRnnCrf(\n","    vocab_size = len(word_to_ix), \n","    tagset_size = len(tag_to_ix), \n","    embedding_dim = EMBEDDING_DIM, \n","    hidden_dim = HIDDEN_DIM,\n","    num_rnn_layers=1, \n","    rnn=\"lstm\"\n","    )\n","\n","optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n","\n","# Check predictions before training\n","with torch.no_grad():\n","    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n","    print(precheck_sent)\n","    precheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long)\n","    print(precheck_tags)\n","    print(model(precheck_sent))\n","\n","# Make sure prepare_sequence from earlier in the LSTM section is loaded\n","for epoch in range(\n","        300):  # again, normally you would NOT do 300 epochs, it is toy data\n","    for sentence, tags in training_data:\n","        # Step 1. Remember that Pytorch accumulates gradients.\n","        # We need to clear them out before each instance\n","        model.zero_grad()\n","\n","        # Step 2. Get our inputs ready for the network, that is,\n","        # turn them into Tensors of word indices.\n","        sentence_in = prepare_sequence(sentence, word_to_ix)\n","        targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n","\n","        # Step 3. Run our forward pass.\n","        loss = model.loss(sentence_in, targets)\n","\n","        # Step 4. Compute the loss, gradients, and update the parameters by\n","        # calling optimizer.step()\n","        loss.backward()\n","        optimizer.step()\n","\n","# Check predictions after training\n","with torch.no_grad():\n","    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n","    print(model(precheck_sent))\n","# We got it!"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":432},"id":"B92KEed8yYNA","executionInfo":{"status":"error","timestamp":1639937418659,"user_tz":-420,"elapsed":297,"user":{"displayName":"Quang Duy Trần","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvMaEX5IKcYvanQuv2HTQEdjP5mTCjGM4CkVJX=s64","userId":"16019943656023573816"}},"outputId":"fbf15ad4-ba29-4520-a7fb-8c04575aedf2"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])\n","tensor([0, 1, 1, 1, 2, 2, 2, 0, 1, 2, 2])\n"]},{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-29-bbc703d19b55>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m     \u001b[0mprecheck_tags\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtag_to_ix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtraining_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecheck_tags\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprecheck_sent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;31m# Make sure prepare_sequence from earlier in the LSTM section is loaded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-e596fae156ac>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;31m# Get the emission scores from the BiLSTM\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m         \u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__build_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_seq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcrf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mscores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtag_seq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-e596fae156ac>\u001b[0m in \u001b[0;36m__build_features\u001b[0;34m(self, sentences)\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msentences\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlong\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0;31m# print(masks)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m         \u001b[0mseq_length\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmasks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m         \u001b[0msorted_seq_length\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mperm_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mseq_length\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdescending\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0membeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0membeds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: Dimension out of range (expected to be in range of [-1, 0], but got 1)"]}]},{"cell_type":"markdown","source":["### BiLSTM + CRF"],"metadata":{"id":"V6VpSZb02j8h"}},{"cell_type":"code","source":["import torch\n","import torch.autograd as autograd\n","import torch.nn as nn\n","import torch.optim as optim\n","\n","torch.manual_seed(42)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"zOvrvlue2lTF","executionInfo":{"status":"ok","timestamp":1639935665288,"user_tz":-420,"elapsed":8189,"user":{"displayName":"Quang Duy Trần","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvMaEX5IKcYvanQuv2HTQEdjP5mTCjGM4CkVJX=s64","userId":"16019943656023573816"}},"outputId":"60f71870-e5a0-4ae3-f98d-5e89da576d6e"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<torch._C.Generator at 0x7f9bbf93aab0>"]},"metadata":{},"execution_count":1}]},{"cell_type":"code","source":["def argmax(vec):\n","    # return the argmax as a python int\n","    _, idx = torch.max(vec, 1)\n","    return idx.item()\n","\n","\n","def prepare_sequence(seq, to_ix):\n","    idxs = [to_ix[w] for w in seq]\n","    return torch.tensor(idxs, dtype=torch.long)\n","\n","\n","# Compute log sum exp in a numerically stable way for the forward algorithm\n","def log_sum_exp(vec):\n","    max_score = vec[0, argmax(vec)]\n","    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n","    return max_score + \\\n","        torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))"],"metadata":{"id":"b3wBaMS82uxK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["START_TAG = \"<START>\"\n","STOP_TAG = \"<STOP>\"\n","EMBEDDING_DIM = 5\n","HIDDEN_DIM = 4"],"metadata":{"id":"F_C-JCHY27jE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class BiLSTM_CRF(nn.Module):\n","\n","    def __init__(self, vocab_size, tag_to_ix, embedding_dim, hidden_dim):\n","        super(BiLSTM_CRF, self).__init__()\n","        self.embedding_dim = embedding_dim\n","        self.hidden_dim = hidden_dim\n","        self.vocab_size = vocab_size\n","        self.tag_to_ix = tag_to_ix\n","        self.tagset_size = len(tag_to_ix)\n","\n","        self.word_embeds = nn.Embedding(vocab_size, embedding_dim)\n","        self.lstm = nn.LSTM(embedding_dim, hidden_dim // 2,\n","                            num_layers=1, bidirectional=True)\n","\n","        # Maps the output of the LSTM into tag space.\n","        self.hidden2tag = nn.Linear(hidden_dim, self.tagset_size)\n","\n","        # Matrix of transition parameters.  Entry i,j is the score of\n","        # transitioning *to* i *from* j.\n","        self.transitions = nn.Parameter(\n","            torch.randn(self.tagset_size, self.tagset_size))\n","\n","        # These two statements enforce the constraint that we never transfer\n","        # to the start tag and we never transfer from the stop tag\n","        self.transitions.data[tag_to_ix[START_TAG], :] = -10000\n","        self.transitions.data[:, tag_to_ix[STOP_TAG]] = -10000\n","\n","        self.hidden = self.init_hidden()\n","\n","    def init_hidden(self):\n","        return (torch.randn(2, 1, self.hidden_dim // 2),\n","                torch.randn(2, 1, self.hidden_dim // 2))\n","\n","    def _forward_alg(self, feats):\n","        # Do the forward algorithm to compute the partition function\n","        init_alphas = torch.full((1, self.tagset_size), -10000.)\n","        # START_TAG has all of the score.\n","        init_alphas[0][self.tag_to_ix[START_TAG]] = 0.\n","\n","        # Wrap in a variable so that we will get automatic backprop\n","        forward_var = init_alphas\n","\n","        # Iterate through the sentence\n","        for feat in feats:\n","            alphas_t = []  # The forward tensors at this timestep\n","            for next_tag in range(self.tagset_size):\n","                # broadcast the emission score: it is the same regardless of\n","                # the previous tag\n","                emit_score = feat[next_tag].view(\n","                    1, -1).expand(1, self.tagset_size)\n","                # the ith entry of trans_score is the score of transitioning to\n","                # next_tag from i\n","                trans_score = self.transitions[next_tag].view(1, -1)\n","                # The ith entry of next_tag_var is the value for the\n","                # edge (i -> next_tag) before we do log-sum-exp\n","                next_tag_var = forward_var + trans_score + emit_score\n","                # The forward variable for this tag is log-sum-exp of all the\n","                # scores.\n","                alphas_t.append(log_sum_exp(next_tag_var).view(1))\n","            forward_var = torch.cat(alphas_t).view(1, -1)\n","        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n","        alpha = log_sum_exp(terminal_var)\n","        return alpha\n","\n","    def _get_lstm_features(self, sentence):\n","        self.hidden = self.init_hidden()\n","        embeds = self.word_embeds(sentence).view(len(sentence), 1, -1)\n","        lstm_out, self.hidden = self.lstm(embeds, self.hidden)\n","        lstm_out = lstm_out.view(len(sentence), self.hidden_dim)\n","        lstm_feats = self.hidden2tag(lstm_out)\n","        return lstm_feats\n","\n","    def _score_sentence(self, feats, tags):\n","        # Gives the score of a provided tag sequence\n","        score = torch.zeros(1)\n","        tags = torch.cat([torch.tensor([self.tag_to_ix[START_TAG]], dtype=torch.long), tags])\n","        for i, feat in enumerate(feats):\n","            score = score + \\\n","                self.transitions[tags[i + 1], tags[i]] + feat[tags[i + 1]]\n","        score = score + self.transitions[self.tag_to_ix[STOP_TAG], tags[-1]]\n","        return score\n","\n","    def _viterbi_decode(self, feats):\n","        backpointers = []\n","\n","        # Initialize the viterbi variables in log space\n","        init_vvars = torch.full((1, self.tagset_size), -10000.)\n","        init_vvars[0][self.tag_to_ix[START_TAG]] = 0\n","\n","        # forward_var at step i holds the viterbi variables for step i-1\n","        forward_var = init_vvars\n","        for feat in feats:\n","            bptrs_t = []  # holds the backpointers for this step\n","            viterbivars_t = []  # holds the viterbi variables for this step\n","\n","            for next_tag in range(self.tagset_size):\n","                # next_tag_var[i] holds the viterbi variable for tag i at the\n","                # previous step, plus the score of transitioning\n","                # from tag i to next_tag.\n","                # We don't include the emission scores here because the max\n","                # does not depend on them (we add them in below)\n","                next_tag_var = forward_var + self.transitions[next_tag]\n","                best_tag_id = argmax(next_tag_var)\n","                bptrs_t.append(best_tag_id)\n","                viterbivars_t.append(next_tag_var[0][best_tag_id].view(1))\n","            # Now add in the emission scores, and assign forward_var to the set\n","            # of viterbi variables we just computed\n","            forward_var = (torch.cat(viterbivars_t) + feat).view(1, -1)\n","            backpointers.append(bptrs_t)\n","\n","        # Transition to STOP_TAG\n","        terminal_var = forward_var + self.transitions[self.tag_to_ix[STOP_TAG]]\n","        best_tag_id = argmax(terminal_var)\n","        path_score = terminal_var[0][best_tag_id]\n","\n","        # Follow the back pointers to decode the best path.\n","        best_path = [best_tag_id]\n","        for bptrs_t in reversed(backpointers):\n","            best_tag_id = bptrs_t[best_tag_id]\n","            best_path.append(best_tag_id)\n","        # Pop off the start tag (we dont want to return that to the caller)\n","        start = best_path.pop()\n","        assert start == self.tag_to_ix[START_TAG]  # Sanity check\n","        best_path.reverse()\n","        return path_score, best_path\n","\n","    def neg_log_likelihood(self, sentence, tags):\n","        feats = self._get_lstm_features(sentence)\n","        forward_score = self._forward_alg(feats)\n","        gold_score = self._score_sentence(feats, tags)\n","        return forward_score - gold_score\n","\n","    def forward(self, sentence):  # dont confuse this with _forward_alg above.\n","        # Get the emission scores from the BiLSTM\n","        lstm_feats = self._get_lstm_features(sentence)\n","\n","        # Find the best path, given the features.\n","        score, tag_seq = self._viterbi_decode(lstm_feats)\n","        return score, tag_seq"],"metadata":{"id":"qS3pQHuQ2xm6"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Make up some training data\n","training_data = [(\n","    \"the wall street journal reported today that apple corporation made money\".split(),\n","    \"B I I I O O O B I O O\".split()\n","), (\n","    \"georgia tech is a university in georgia\".split(),\n","    \"B I O O O O B\".split()\n",")]\n","\n","word_to_ix = {}\n","for sentence, tags in training_data:\n","    for word in sentence:\n","        if word not in word_to_ix:\n","            word_to_ix[word] = len(word_to_ix)\n","\n","tag_to_ix = {\"B\": 0, \"I\": 1, \"O\": 2, START_TAG: 3, STOP_TAG: 4}\n","\n","model = BiLSTM_CRF(len(word_to_ix), tag_to_ix, EMBEDDING_DIM, HIDDEN_DIM)\n","optimizer = optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4)\n","\n","# Check predictions before training\n","with torch.no_grad():\n","    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n","    precheck_tags = torch.tensor([tag_to_ix[t] for t in training_data[0][1]], dtype=torch.long)\n","    print(model(precheck_sent))\n","\n","# Make sure prepare_sequence from earlier in the LSTM section is loaded\n","for epoch in range(\n","        300):  # again, normally you would NOT do 300 epochs, it is toy data\n","    for sentence, tags in training_data:\n","        # Step 1. Remember that Pytorch accumulates gradients.\n","        # We need to clear them out before each instance\n","        model.zero_grad()\n","\n","        # Step 2. Get our inputs ready for the network, that is,\n","        # turn them into Tensors of word indices.\n","        sentence_in = prepare_sequence(sentence, word_to_ix)\n","        targets = torch.tensor([tag_to_ix[t] for t in tags], dtype=torch.long)\n","\n","        # Step 3. Run our forward pass.\n","        loss = model.neg_log_likelihood(sentence_in, targets)\n","\n","        # Step 4. Compute the loss, gradients, and update the parameters by\n","        # calling optimizer.step()\n","        loss.backward()\n","        optimizer.step()\n","\n","# Check predictions after training\n","with torch.no_grad():\n","    precheck_sent = prepare_sequence(training_data[0][0], word_to_ix)\n","    print(model(precheck_sent))\n","# We got it!"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fJkiMRni3pUJ","executionInfo":{"status":"ok","timestamp":1639584834410,"user_tz":-420,"elapsed":7469,"user":{"displayName":"Quang Duy Trần","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvMaEX5IKcYvanQuv2HTQEdjP5mTCjGM4CkVJX=s64","userId":"16019943656023573816"}},"outputId":"ae7d1f53-aed7-48d5-8699-356c401bda4a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["(tensor(9.3807), [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0])\n","(tensor(25.3154), [0, 1, 1, 1, 2, 2, 2, 0, 1, 2, 2])\n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"zU14rmNQ316s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### BERT + CRF"],"metadata":{"id":"t2K1K2UD9Gmh"}},{"cell_type":"code","source":["!pip install pytorch-pretrained-bert"],"metadata":{"id":"n5pw5f1V9ovo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import sys\n","import os\n","import time\n","import importlib\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import torch\n","import torch.nn.functional as F\n","import torch.nn as nn\n","import torch.autograd as autograd\n","import torch.optim as optim\n","\n","from torch.utils.data.distributed import DistributedSampler\n","from torch.utils import data\n","\n","from tqdm import tqdm, trange\n","import collections\n","\n","from pytorch_pretrained_bert.modeling import BertModel, BertForTokenClassification, BertLayerNorm\n","import pickle\n","from pytorch_pretrained_bert.optimization import BertAdam, WarmupLinearSchedule\n","from pytorch_pretrained_bert.tokenization import BertTokenizer\n","\n","def set_work_dir(local_path=\"\", server_path=\"ner_bert_crf\"):\n","    if (os.path.exists(os.getenv(\"content\")+'/'+local_path)):\n","        os.chdir(os.getenv(\"content\")+'/'+local_path)\n","    elif (os.path.exists(os.getenv(\"content\")+'/'+server_path)):\n","        os.chdir(os.getenv(\"content\")+'/'+server_path)\n","    else:\n","        raise Exception('Set work path error!')\n","\n","\n","def get_data_dir(local_path=\"\", server_path=\"ner_bert_crf\"):\n","    if (os.path.exists(os.getenv(\"content\")+'/'+local_path)):\n","        return os.getenv(\"content\")+'/'+local_path\n","    elif (os.path.exists(os.getenv(\"content\")+'/'+server_path)):\n","        return os.getenv(\"content\")+'/'+server_path\n","    else:\n","        raise Exception('get data path error!')"],"metadata":{"id":"TW0Qe9b09fCx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('Python version ', sys.version)\n","print('PyTorch version ', torch.__version__)\n","\n","# set_work_dir()\n","print('Current dir:', os.getcwd())\n","\n","cuda_yes = torch.cuda.is_available()\n","# cuda_yes = False\n","print('Cuda is available?', cuda_yes)\n","device = torch.device(\"cuda:0\" if cuda_yes else \"cpu\")\n","print('Device:', device)\n","\n","data_dir = './data'\n","# \"Whether to run training.\"\n","do_train = True\n","# \"Whether to run eval on the dev set.\"\n","do_eval = True\n","# \"Whether to run the model in inference mode on the test set.\"\n","do_predict = True\n","# Whether load checkpoint file before train model\n","load_checkpoint = True\n","# \"The vocabulary file that the BERT model was trained on.\"\n","max_seq_length = 180 #256\n","batch_size = 32 #32\n","# \"The initial learning rate for Adam.\"\n","learning_rate0 = 5e-5\n","lr0_crf_fc = 8e-5\n","weight_decay_finetune = 1e-5 #0.01\n","weight_decay_crf_fc = 5e-6 #0.005\n","total_train_epochs = 15\n","gradient_accumulation_steps = 1\n","warmup_proportion = 0.1\n","output_dir = './output/'\n","bert_model_scale = 'bert-base-cased'\n","do_lower_case = False"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7CK0RKuJ9dGu","executionInfo":{"status":"ok","timestamp":1639586764668,"user_tz":-420,"elapsed":361,"user":{"displayName":"Quang Duy Trần","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvMaEX5IKcYvanQuv2HTQEdjP5mTCjGM4CkVJX=s64","userId":"16019943656023573816"}},"outputId":"627382bd-3283-4be7-d1c8-906d0c665f7c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Python version  3.7.12 (default, Sep 10 2021, 00:21:48) \n","[GCC 7.5.0]\n","PyTorch version  1.10.0+cu111\n","Current dir: /content\n","Cuda is available? False\n","Device: cpu\n"]}]},{"cell_type":"code","source":["class InputExample(object):\n","    \"\"\"A single training/test example for NER.\"\"\"\n","\n","    def __init__(self, guid, words, labels):\n","        \"\"\"Constructs a InputExample.\n","        Args:\n","          guid: Unique id for the example(a sentence or a pair of sentences).\n","          words: list of words of sentence\n","          labels_a/labels_b: (Optional) string. The label seqence of the text_a/text_b. This should be\n","            specified for train and dev examples, but not for test examples.\n","        \"\"\"\n","        self.guid = guid\n","        # list of words of the sentence,example: [EU, rejects, German, call, to, boycott, British, lamb .]\n","        self.words = words\n","        # list of label sequence of the sentence,like: [B-ORG, O, B-MISC, O, O, O, B-MISC, O, O]\n","        self.labels = labels\n","\n","\n","class InputFeatures(object):\n","    \"\"\"A single set of features of data.\n","    result of convert_examples_to_features(InputExample)\n","    \"\"\"\n","\n","    def __init__(self, input_ids, input_mask, segment_ids,  predict_mask, label_ids):\n","        self.input_ids = input_ids\n","        self.input_mask = input_mask\n","        self.segment_ids = segment_ids\n","        self.predict_mask = predict_mask\n","        self.label_ids = label_ids\n","\n","\n","\n","class DataProcessor(object):\n","    \"\"\"Base class for data converters for sequence classification data sets.\"\"\"\n","\n","    def get_train_examples(self, data_dir):\n","        \"\"\"Gets a collection of `InputExample`s for the train set.\"\"\"\n","        raise NotImplementedError()\n","\n","    def get_dev_examples(self, data_dir):\n","        \"\"\"Gets a collection of `InputExample`s for the dev set.\"\"\"\n","        raise NotImplementedError()\n","\n","    def get_labels(self):\n","        \"\"\"Gets the list of labels for this data set.\"\"\"\n","        raise NotImplementedError()\n","\n","    @classmethod\n","    def _read_data(cls, input_file):\n","        \"\"\"\n","        Reads a BIO data.\n","        \"\"\"\n","        with open(input_file) as f:\n","            # out_lines = []\n","            out_lists = []\n","            entries = f.read().strip().split(\"\\n\\n\")\n","            for entry in entries:\n","                words = []\n","                ner_labels = []\n","                pos_tags = []\n","                bio_pos_tags = []\n","                for line in entry.splitlines():\n","                    pieces = line.strip().split()\n","                    if len(pieces) < 1:\n","                        continue\n","                    word = pieces[0]\n","                    # if word == \"-DOCSTART-\" or word == '':\n","                    #     continue\n","                    words.append(word)\n","                    pos_tags.append(pieces[1])\n","                    bio_pos_tags.append(pieces[2])\n","                    ner_labels.append(pieces[-1])\n","                # sentence = ' '.join(words)\n","                # ner_seq = ' '.join(ner_labels)\n","                # pos_tag_seq = ' '.join(pos_tags)\n","                # bio_pos_tag_seq = ' '.join(bio_pos_tags)\n","                # out_lines.append([sentence, pos_tag_seq, bio_pos_tag_seq, ner_seq])\n","                # out_lines.append([sentence, ner_seq])\n","                out_lists.append([words,pos_tags,bio_pos_tags,ner_labels])\n","        return out_lists\n","\n","class CoNLLDataProcessor(DataProcessor):\n","    '''\n","    CoNLL-2003\n","    '''\n","\n","    def __init__(self):\n","        self._label_types = [ 'X', '[CLS]', '[SEP]', 'O', 'I-LOC', 'B-PER', 'I-PER', 'I-ORG', 'I-MISC', 'B-MISC', 'B-LOC', 'B-ORG']\n","        self._num_labels = len(self._label_types)\n","        self._label_map = {label: i for i,\n","                           label in enumerate(self._label_types)}\n","\n","    def get_train_examples(self, data_dir):\n","        return self._create_examples(\n","            self._read_data(os.path.join(data_dir, \"train.txt\")))\n","\n","    def get_dev_examples(self, data_dir):\n","        return self._create_examples(\n","            self._read_data(os.path.join(data_dir, \"valid.txt\")))\n","\n","    def get_test_examples(self, data_dir):\n","        return self._create_examples(\n","            self._read_data(os.path.join(data_dir, \"test.txt\")))\n","\n","    def get_labels(self):\n","        return self._label_types\n","\n","    def get_num_labels(self):\n","        return self.get_num_labels\n","\n","    def get_label_map(self):\n","        return self._label_map\n","\n","    def get_start_label_id(self):\n","        return self._label_map['[CLS]']\n","\n","    def get_stop_label_id(self):\n","        return self._label_map['[SEP]']\n","\n","    def _create_examples(self, all_lists):\n","        examples = []\n","        for (i, one_lists) in enumerate(all_lists):\n","            guid = i\n","            words = one_lists[0]\n","            labels = one_lists[-1]\n","            examples.append(InputExample(\n","                guid=guid, words=words, labels=labels))\n","        return examples\n","\n","    def _create_examples2(self, lines):\n","        examples = []\n","        for (i, line) in enumerate(lines):\n","            guid = i\n","            text = line[0]\n","            ner_label = line[-1]\n","            examples.append(InputExample(\n","                guid=guid, text_a=text, labels_a=ner_label))\n","        return examples"],"metadata":{"id":"T7X9VTLG_Vcg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def example2feature(example, tokenizer, label_map, max_seq_length):\n","\n","    add_label = 'X'\n","    # tokenize_count = []\n","    tokens = ['[CLS]']\n","    predict_mask = [0]\n","    label_ids = [label_map['[CLS]']]\n","    for i, w in enumerate(example.words):\n","        # use bertTokenizer to split words\n","        # 1996-08-22 => 1996 - 08 - 22\n","        # sheepmeat => sheep ##me ##at\n","        sub_words = tokenizer.tokenize(w)\n","        if not sub_words:\n","            sub_words = ['[UNK]']\n","        # tokenize_count.append(len(sub_words))\n","        tokens.extend(sub_words)\n","        for j in range(len(sub_words)):\n","            if j == 0:\n","                predict_mask.append(1)\n","                label_ids.append(label_map[example.labels[i]])\n","            else:\n","                # '##xxx' -> 'X' (see bert paper)\n","                predict_mask.append(0)\n","                label_ids.append(label_map[add_label])\n","\n","    # truncate\n","    if len(tokens) > max_seq_length - 1:\n","        print('Example No.{} is too long, length is {}, truncated to {}!'.format(example.guid, len(tokens), max_seq_length))\n","        tokens = tokens[0:(max_seq_length - 1)]\n","        predict_mask = predict_mask[0:(max_seq_length - 1)]\n","        label_ids = label_ids[0:(max_seq_length - 1)]\n","    tokens.append('[SEP]')\n","    predict_mask.append(0)\n","    label_ids.append(label_map['[SEP]'])\n","\n","    input_ids = tokenizer.convert_tokens_to_ids(tokens)\n","    segment_ids = [0] * len(input_ids)\n","    input_mask = [1] * len(input_ids)\n","\n","    feat=InputFeatures(\n","                # guid=example.guid,\n","                # tokens=tokens,\n","                input_ids=input_ids,\n","                input_mask=input_mask,\n","                segment_ids=segment_ids,\n","                predict_mask=predict_mask,\n","                label_ids=label_ids)\n","\n","    return feat\n","\n","    \n","class NerDataset(data.Dataset):\n","    def __init__(self, examples, tokenizer, label_map, max_seq_length):\n","        self.examples=examples\n","        self.tokenizer=tokenizer\n","        self.label_map=label_map\n","        self.max_seq_length=max_seq_length\n","\n","    def __len__(self):\n","        return len(self.examples)\n","\n","    def __getitem__(self, idx):\n","        feat=example2feature(self.examples[idx], self.tokenizer, self.label_map, max_seq_length)\n","        return feat.input_ids, feat.input_mask, feat.segment_ids, feat.predict_mask, feat.label_ids\n","\n","    @classmethod\n","    def pad(cls, batch):\n","\n","        seqlen_list = [len(sample[0]) for sample in batch]\n","        maxlen = np.array(seqlen_list).max()\n","\n","        f = lambda x, seqlen: [sample[x] + [0] * (seqlen - len(sample[x])) for sample in batch] # 0: X for padding\n","        input_ids_list = torch.LongTensor(f(0, maxlen))\n","        input_mask_list = torch.LongTensor(f(1, maxlen))\n","        segment_ids_list = torch.LongTensor(f(2, maxlen))\n","        predict_mask_list = torch.ByteTensor(f(3, maxlen))\n","        label_ids_list = torch.LongTensor(f(4, maxlen))\n","\n","        return input_ids_list, input_mask_list, segment_ids_list, predict_mask_list, label_ids_list\n","\n","def f1_score(y_true, y_pred):\n","    '''\n","    0,1,2,3 are [CLS],[SEP],[X],O\n","    '''\n","    ignore_id=3\n","\n","    num_proposed = len(y_pred[y_pred>ignore_id])\n","    num_correct = (np.logical_and(y_true==y_pred, y_true>ignore_id)).sum()\n","    num_gold = len(y_true[y_true>ignore_id])\n","\n","    try:\n","        precision = num_correct / num_proposed\n","    except ZeroDivisionError:\n","        precision = 1.0\n","\n","    try:\n","        recall = num_correct / num_gold\n","    except ZeroDivisionError:\n","        recall = 1.0\n","\n","    try:\n","        f1 = 2*precision*recall / (precision + recall)\n","    except ZeroDivisionError:\n","        if precision*recall==0:\n","            f1=1.0\n","        else:\n","            f1=0\n","\n","    return precision, recall, f1\n"],"metadata":{"id":"JloBkyep_0vA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["'''\n","Prepare data set\n","'''\n","# random.seed(44)\n","np.random.seed(44)\n","torch.manual_seed(44)\n","if cuda_yes:\n","    torch.cuda.manual_seed_all(44)\n","\n","# Load pre-trained model tokenizer (vocabulary)\n","conllProcessor = CoNLLDataProcessor()\n","label_list = conllProcessor.get_labels()\n","label_map = conllProcessor.get_label_map()\n","train_examples = conllProcessor.get_train_examples(data_dir)\n","dev_examples = conllProcessor.get_dev_examples(data_dir)\n","test_examples = conllProcessor.get_test_examples(data_dir)\n","\n","total_train_steps = int(len(train_examples) / batch_size / gradient_accumulation_steps * total_train_epochs)\n","\n","print(\"***** Running training *****\")\n","print(\"  Num examples = %d\"% len(train_examples))\n","print(\"  Batch size = %d\"% batch_size)\n","print(\"  Num steps = %d\"% total_train_steps)\n","\n","tokenizer = BertTokenizer.from_pretrained(bert_model_scale, do_lower_case=do_lower_case)\n","\n","train_dataset = NerDataset(train_examples,tokenizer,label_map,max_seq_length)\n","dev_dataset = NerDataset(dev_examples,tokenizer,label_map,max_seq_length)\n","test_dataset = NerDataset(test_examples,tokenizer,label_map,max_seq_length)\n","\n","train_dataloader = data.DataLoader(dataset=train_dataset,\n","                                batch_size=batch_size,\n","                                shuffle=True,\n","                                num_workers=4,\n","                                collate_fn=NerDataset.pad)\n","\n","dev_dataloader = data.DataLoader(dataset=dev_dataset,\n","                                batch_size=batch_size,\n","                                shuffle=False,\n","                                num_workers=4,\n","                                collate_fn=NerDataset.pad)\n","\n","test_dataloader = data.DataLoader(dataset=test_dataset,\n","                                batch_size=batch_size,\n","                                shuffle=False,\n","                                num_workers=4,\n","                                collate_fn=NerDataset.pad)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fxGhWVl3_oyS","executionInfo":{"status":"ok","timestamp":1639587030280,"user_tz":-420,"elapsed":2835,"user":{"displayName":"Quang Duy Trần","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvMaEX5IKcYvanQuv2HTQEdjP5mTCjGM4CkVJX=s64","userId":"16019943656023573816"}},"outputId":"94c86cb0-9c7c-4599-e056-39e1fc18c94f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["***** Running training *****\n","  Num examples = 14987\n","  Batch size = 32\n","  Num steps = 7025\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n"]}]},{"cell_type":"code","source":["print('*** Use BertModel + CRF ***')\n","\n","def log_sum_exp_1vec(vec):  # shape(1,m)\n","    max_score = vec[0, np.argmax(vec)]\n","    max_score_broadcast = max_score.view(1, -1).expand(1, vec.size()[1])\n","    return max_score + torch.log(torch.sum(torch.exp(vec - max_score_broadcast)))\n","\n","def log_sum_exp_mat(log_M, axis=-1):  # shape(n,m)\n","    return torch.max(log_M, axis)[0]+torch.log(torch.exp(log_M-torch.max(log_M, axis)[0][:, None]).sum(axis))\n","\n","def log_sum_exp_batch(log_Tensor, axis=-1): # shape (batch_size,n,m)\n","    return torch.max(log_Tensor, axis)[0]+torch.log(torch.exp(log_Tensor-torch.max(log_Tensor, axis)[0].view(log_Tensor.shape[0],-1,1)).sum(axis))\n","\n","\n","class BERT_CRF_NER(nn.Module):\n","\n","    def __init__(self, bert_model, start_label_id, stop_label_id, num_labels, max_seq_length, batch_size, device):\n","        super(BERT_CRF_NER, self).__init__()\n","        self.hidden_size = 768\n","        self.start_label_id = start_label_id\n","        self.stop_label_id = stop_label_id\n","        self.num_labels = num_labels\n","        # self.max_seq_length = max_seq_length\n","        self.batch_size = batch_size\n","        self.device=device\n","\n","        # use pretrainded BertModel\n","        self.bert = bert_model\n","        self.dropout = torch.nn.Dropout(0.2)\n","        # Maps the output of the bert into label space.\n","        self.hidden2label = nn.Linear(self.hidden_size, self.num_labels)\n","\n","        # Matrix of transition parameters.  Entry i,j is the score of transitioning *to* i *from* j.\n","        self.transitions = nn.Parameter(\n","            torch.randn(self.num_labels, self.num_labels))\n","\n","        # These two statements enforce the constraint that we never transfer *to* the start tag(or label),\n","        # and we never transfer *from* the stop label (the model would probably learn this anyway,\n","        # so this enforcement is likely unimportant)\n","        self.transitions.data[start_label_id, :] = -10000\n","        self.transitions.data[:, stop_label_id] = -10000\n","\n","        nn.init.xavier_uniform_(self.hidden2label.weight)\n","        nn.init.constant_(self.hidden2label.bias, 0.0)\n","        # self.apply(self.init_bert_weights)\n","\n","    def init_bert_weights(self, module):\n","        \"\"\" Initialize the weights.\n","        \"\"\"\n","        if isinstance(module, (nn.Linear, nn.Embedding)):\n","            # Slightly different from the TF version which uses truncated_normal for initialization\n","            # cf https://github.com/pytorch/pytorch/pull/5617\n","            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n","        elif isinstance(module, BertLayerNorm):\n","            module.bias.data.zero_()\n","            module.weight.data.fill_(1.0)\n","        if isinstance(module, nn.Linear) and module.bias is not None:\n","            module.bias.data.zero_()\n","\n","    def _forward_alg(self, feats):\n","        '''\n","        this also called alpha-recursion or forward recursion, to calculate log_prob of all barX\n","        '''\n","\n","        # T = self.max_seq_length\n","        T = feats.shape[1]\n","        batch_size = feats.shape[0]\n","\n","        # alpha_recursion,forward, alpha(zt)=p(zt,bar_x_1:t)\n","        log_alpha = torch.Tensor(batch_size, 1, self.num_labels).fill_(-10000.).to(self.device)\n","        # normal_alpha_0 : alpha[0]=Ot[0]*self.PIs\n","        # self.start_label has all of the score. it is log,0 is p=1\n","        log_alpha[:, 0, self.start_label_id] = 0\n","\n","        # feats: sentances -> word embedding -> lstm -> MLP -> feats\n","        # feats is the probability of emission, feat.shape=(1,tag_size)\n","        for t in range(1, T):\n","            log_alpha = (log_sum_exp_batch(self.transitions + log_alpha, axis=-1) + feats[:, t]).unsqueeze(1)\n","\n","        # log_prob of all barX\n","        log_prob_all_barX = log_sum_exp_batch(log_alpha)\n","        return log_prob_all_barX\n","\n","    def _get_bert_features(self, input_ids, segment_ids, input_mask):\n","        '''\n","        sentances -> word embedding -> lstm -> MLP -> feats\n","        '''\n","        bert_seq_out, _ = self.bert(input_ids, token_type_ids=segment_ids, attention_mask=input_mask, output_all_encoded_layers=False)\n","        bert_seq_out = self.dropout(bert_seq_out)\n","        bert_feats = self.hidden2label(bert_seq_out)\n","        return bert_feats\n","\n","    def _score_sentence(self, feats, label_ids):\n","        '''\n","        Gives the score of a provided label sequence\n","        p(X=w1:t,Zt=tag1:t)=...p(Zt=tag_t|Zt-1=tag_t-1)p(xt|Zt=tag_t)...\n","        '''\n","\n","        # T = self.max_seq_length\n","        T = feats.shape[1]\n","        batch_size = feats.shape[0]\n","\n","        batch_transitions = self.transitions.expand(batch_size,self.num_labels,self.num_labels)\n","        batch_transitions = batch_transitions.flatten(1)\n","\n","        score = torch.zeros((feats.shape[0],1)).to(device)\n","        # the 0th node is start_label->start_word,\bthe probability of them=1. so t begin with 1.\n","        for t in range(1, T):\n","            score = score + \\\n","                batch_transitions.gather(-1, (label_ids[:, t]*self.num_labels+label_ids[:, t-1]).view(-1,1)) \\\n","                    + feats[:, t].gather(-1, label_ids[:, t].view(-1,1)).view(-1,1)\n","        return score\n","\n","    def _viterbi_decode(self, feats):\n","        '''\n","        Max-Product Algorithm or viterbi algorithm, argmax(p(z_0:t|x_0:t))\n","        '''\n","\n","        # T = self.max_seq_length\n","        T = feats.shape[1]\n","        batch_size = feats.shape[0]\n","\n","        # batch_transitions=self.transitions.expand(batch_size,self.num_labels,self.num_labels)\n","\n","        log_delta = torch.Tensor(batch_size, 1, self.num_labels).fill_(-10000.).to(self.device)\n","        log_delta[:, 0, self.start_label_id] = 0\n","\n","        # psi is for the vaule of the last latent that make P(this_latent) maximum.\n","        psi = torch.zeros((batch_size, T, self.num_labels), dtype=torch.long).to(self.device)  # psi[0]=0000 useless\n","        for t in range(1, T):\n","            # delta[t][k]=max_z1:t-1( p(x1,x2,...,xt,z1,z2,...,zt-1,zt=k|theta) )\n","            # delta[t] is the max prob of the path from  z_t-1 to z_t[k]\n","            log_delta, psi[:, t] = torch.max(self.transitions + log_delta, -1)\n","            # psi[t][k]=argmax_z1:t-1( p(x1,x2,...,xt,z1,z2,...,zt-1,zt=k|theta) )\n","            # psi[t][k] is the path choosed from z_t-1 to z_t[k],the value is the z_state(is k) index of z_t-1\n","            log_delta = (log_delta + feats[:, t]).unsqueeze(1)\n","\n","        # trace back\n","        path = torch.zeros((batch_size, T), dtype=torch.long).to(self.device)\n","\n","        # max p(z1:t,all_x|theta)\n","        max_logLL_allz_allx, path[:, -1] = torch.max(log_delta.squeeze(), -1)\n","\n","        for t in range(T-2, -1, -1):\n","            # choose the state of z_t according the state choosed of z_t+1.\n","            path[:, t] = psi[:, t+1].gather(-1,path[:, t+1].view(-1,1)).squeeze()\n","\n","        return max_logLL_allz_allx, path\n","\n","    def neg_log_likelihood(self, input_ids, segment_ids, input_mask, label_ids):\n","        bert_feats = self._get_bert_features(input_ids, segment_ids, input_mask)\n","        forward_score = self._forward_alg(bert_feats)\n","        # p(X=w1:t,Zt=tag1:t)=...p(Zt=tag_t|Zt-1=tag_t-1)p(xt|Zt=tag_t)...\n","        gold_score = self._score_sentence(bert_feats, label_ids)\n","        # - log[ p(X=w1:t,Zt=tag1:t)/p(X=w1:t) ] = - log[ p(Zt=tag1:t|X=w1:t) ]\n","        return torch.mean(forward_score - gold_score)\n","\n","    # this forward is just for predict, not for train\n","    # dont confuse this with _forward_alg above.\n","    def forward(self, input_ids, segment_ids, input_mask):\n","        # Get the emission scores from the BiLSTM\n","        bert_feats = self._get_bert_features(input_ids, segment_ids, input_mask)\n","\n","        # Find the best path, given the features.\n","        score, label_seq_ids = self._viterbi_decode(bert_feats)\n","        return score, label_seq_ids\n","\n","\n","start_label_id = conllProcessor.get_start_label_id()\n","stop_label_id = conllProcessor.get_stop_label_id()\n","\n","bert_model = BertModel.from_pretrained(bert_model_scale)\n","model = BERT_CRF_NER(bert_model, start_label_id, stop_label_id, len(label_list), max_seq_length, batch_size, device)\n","\n","#%%\n","if load_checkpoint and os.path.exists(output_dir+'/ner_bert_crf_checkpoint.pt'):\n","    checkpoint = torch.load(output_dir+'/ner_bert_crf_checkpoint.pt', map_location='cpu')\n","    start_epoch = checkpoint['epoch']+1\n","    valid_acc_prev = checkpoint['valid_acc']\n","    valid_f1_prev = checkpoint['valid_f1']\n","    pretrained_dict=checkpoint['model_state']\n","    net_state_dict = model.state_dict()\n","    pretrained_dict_selected = {k: v for k, v in pretrained_dict.items() if k in net_state_dict}\n","    net_state_dict.update(pretrained_dict_selected)\n","    model.load_state_dict(net_state_dict)\n","    print('Loaded the pretrain NER_BERT_CRF model, epoch:',checkpoint['epoch'],'valid acc:',\n","            checkpoint['valid_acc'], 'valid f1:', checkpoint['valid_f1'])\n","else:\n","    start_epoch = 0\n","    valid_acc_prev = 0\n","    valid_f1_prev = 0\n","\n","model.to(device)\n","\n","# Prepare optimizer\n","param_optimizer = list(model.named_parameters())\n","\n","no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n","new_param = ['transitions', 'hidden2label.weight', 'hidden2label.bias']\n","optimizer_grouped_parameters = [\n","    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay) \\\n","        and not any(nd in n for nd in new_param)], 'weight_decay': weight_decay_finetune},\n","    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay) \\\n","        and not any(nd in n for nd in new_param)], 'weight_decay': 0.0},\n","    {'params': [p for n, p in param_optimizer if n in ('transitions','hidden2label.weight')] \\\n","        , 'lr':lr0_crf_fc, 'weight_decay': weight_decay_crf_fc},\n","    {'params': [p for n, p in param_optimizer if n == 'hidden2label.bias'] \\\n","        , 'lr':lr0_crf_fc, 'weight_decay': 0.0}\n","]\n","optimizer = BertAdam(optimizer_grouped_parameters, lr=learning_rate0, warmup=warmup_proportion, t_total=total_train_steps)\n","# optimizer = optim.Adam(model.parameters(), lr=learning_rate0)\n","\n","def warmup_linear(x, warmup=0.002):\n","    if x < warmup:\n","        return x/warmup\n","    return 1.0 - x\n","\n","def evaluate(model, predict_dataloader, batch_size, epoch_th, dataset_name):\n","    # print(\"***** Running prediction *****\")\n","    model.eval()\n","    all_preds = []\n","    all_labels = []\n","    total=0\n","    correct=0\n","    start = time.time()\n","    with torch.no_grad():\n","        for batch in predict_dataloader:\n","            batch = tuple(t.to(device) for t in batch)\n","            input_ids, input_mask, segment_ids, predict_mask, label_ids = batch\n","            _, predicted_label_seq_ids = model(input_ids, segment_ids, input_mask)\n","            # _, predicted = torch.max(out_scores, -1)\n","            valid_predicted = torch.masked_select(predicted_label_seq_ids, predict_mask)\n","            valid_label_ids = torch.masked_select(label_ids, predict_mask)\n","            all_preds.extend(valid_predicted.tolist())\n","            all_labels.extend(valid_label_ids.tolist())\n","            # print(len(valid_label_ids),len(valid_predicted),len(valid_label_ids)==len(valid_predicted))\n","            total += len(valid_label_ids)\n","            correct += valid_predicted.eq(valid_label_ids).sum().item()\n","\n","    test_acc = correct/total\n","    precision, recall, f1 = f1_score(np.array(all_labels), np.array(all_preds))\n","    end = time.time()\n","    print('Epoch:%d, Acc:%.2f, Precision: %.2f, Recall: %.2f, F1: %.2f on %s, Spend:%.3f minutes for evaluation' \\\n","        % (epoch_th, 100.*test_acc, 100.*precision, 100.*recall, 100.*f1, dataset_name,(end-start)/60.0))\n","    print('--------------------------------------------------------------')\n","    return test_acc, f1\n","\n","#%%\n","# train procedure\n","global_step_th = int(len(train_examples) / batch_size / gradient_accumulation_steps * start_epoch)\n","\n","# train_start=time.time()\n","# for epoch in trange(start_epoch, total_train_epochs, desc=\"Epoch\"):\n","for epoch in range(start_epoch, total_train_epochs):\n","    tr_loss = 0\n","    train_start = time.time()\n","    model.train()\n","    optimizer.zero_grad()\n","    # for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n","    for step, batch in enumerate(train_dataloader):\n","        batch = tuple(t.to(device) for t in batch)\n","        input_ids, input_mask, segment_ids, predict_mask, label_ids = batch\n","\n","        neg_log_likelihood = model.neg_log_likelihood(input_ids, segment_ids, input_mask, label_ids)\n","\n","        if gradient_accumulation_steps > 1:\n","            neg_log_likelihood = neg_log_likelihood / gradient_accumulation_steps\n","\n","        neg_log_likelihood.backward()\n","\n","        tr_loss += neg_log_likelihood.item()\n","\n","        if (step + 1) % gradient_accumulation_steps == 0:\n","            # modify learning rate with special warm up BERT uses\n","            lr_this_step = learning_rate0 * warmup_linear(global_step_th/total_train_steps, warmup_proportion)\n","            for param_group in optimizer.param_groups:\n","                param_group['lr'] = lr_this_step\n","            optimizer.step()\n","            optimizer.zero_grad()\n","            global_step_th += 1\n","\n","        print(\"Epoch:{}-{}/{}, Negative loglikelihood: {} \".format(epoch, step, len(train_dataloader), neg_log_likelihood.item()))\n","\n","    print('--------------------------------------------------------------')\n","    print(\"Epoch:{} completed, Total training's Loss: {}, Spend: {}m\".format(epoch, tr_loss, (time.time() - train_start)/60.0))\n","    valid_acc, valid_f1 = evaluate(model, dev_dataloader, batch_size, epoch, 'Valid_set')\n","\n","    # Save a checkpoint\n","    if valid_f1 > valid_f1_prev:\n","        # model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n","        torch.save({'epoch': epoch, 'model_state': model.state_dict(), 'valid_acc': valid_acc,\n","            'valid_f1': valid_f1, 'max_seq_length': max_seq_length, 'lower_case': do_lower_case},\n","                    os.path.join(output_dir, 'ner_bert_crf_checkpoint.pt'))\n","        valid_f1_prev = valid_f1\n","\n","evaluate(model, test_dataloader, batch_size, total_train_epochs-1, 'Test_set')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"D3Wtsklo9IFg","executionInfo":{"status":"error","timestamp":1639599509262,"user_tz":-420,"elapsed":12478984,"user":{"displayName":"Quang Duy Trần","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgvMaEX5IKcYvanQuv2HTQEdjP5mTCjGM4CkVJX=s64","userId":"16019943656023573816"}},"outputId":"d04e03fd-c706-454c-c71d-2571ad5ac49c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["*** Use BertModel + CRF ***\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n","  cpuset_checked))\n","/usr/local/lib/python3.7/dist-packages/pytorch_pretrained_bert/optimization.py:275: UserWarning: This overload of add_ is deprecated:\n","\tadd_(Number alpha, Tensor other)\n","Consider using one of the following signatures instead:\n","\tadd_(Tensor other, *, Number alpha) (Triggered internally at  ../torch/csrc/utils/python_arg_parser.cpp:1050.)\n","  next_m.mul_(beta1).add_(1 - beta1, grad)\n"]},{"output_type":"stream","name":"stdout","text":["Epoch:0-0/469, Negative loglikelihood: 9825.7421875 \n","Epoch:0-1/469, Negative loglikelihood: 9811.798828125 \n","Epoch:0-2/469, Negative loglikelihood: 9814.4033203125 \n","Epoch:0-3/469, Negative loglikelihood: 9813.134765625 \n","Epoch:0-4/469, Negative loglikelihood: 9802.5244140625 \n","Epoch:0-5/469, Negative loglikelihood: 9806.798828125 \n","Epoch:0-6/469, Negative loglikelihood: 9811.4345703125 \n","Epoch:0-7/469, Negative loglikelihood: 9790.734375 \n","Epoch:0-8/469, Negative loglikelihood: 9804.0732421875 \n","Epoch:0-9/469, Negative loglikelihood: 9829.3828125 \n","Epoch:0-10/469, Negative loglikelihood: 9806.873046875 \n","Epoch:0-11/469, Negative loglikelihood: 9794.3115234375 \n","Epoch:0-12/469, Negative loglikelihood: 9803.1064453125 \n","Epoch:0-13/469, Negative loglikelihood: 9775.8583984375 \n","Epoch:0-14/469, Negative loglikelihood: 9817.615234375 \n","Epoch:0-15/469, Negative loglikelihood: 9852.787109375 \n","Epoch:0-16/469, Negative loglikelihood: 9840.0888671875 \n","Epoch:0-17/469, Negative loglikelihood: 9799.294921875 \n","Epoch:0-18/469, Negative loglikelihood: 9785.974609375 \n","Epoch:0-19/469, Negative loglikelihood: 9801.36328125 \n","Epoch:0-20/469, Negative loglikelihood: 9820.498046875 \n","Epoch:0-21/469, Negative loglikelihood: 9804.794921875 \n","Epoch:0-22/469, Negative loglikelihood: 9791.759765625 \n","Epoch:0-23/469, Negative loglikelihood: 9805.8447265625 \n","Epoch:0-24/469, Negative loglikelihood: 9794.064453125 \n","Epoch:0-25/469, Negative loglikelihood: 9782.2421875 \n","Epoch:0-26/469, Negative loglikelihood: 9806.8134765625 \n","Epoch:0-27/469, Negative loglikelihood: 9796.095703125 \n","Epoch:0-28/469, Negative loglikelihood: 9773.13671875 \n","Epoch:0-29/469, Negative loglikelihood: 9841.875 \n","Epoch:0-30/469, Negative loglikelihood: 9813.77734375 \n","Epoch:0-31/469, Negative loglikelihood: 9801.19921875 \n","Epoch:0-32/469, Negative loglikelihood: 9767.4462890625 \n","Epoch:0-33/469, Negative loglikelihood: 9839.5751953125 \n","Epoch:0-34/469, Negative loglikelihood: 9477.09375 \n","Epoch:0-35/469, Negative loglikelihood: 9495.78125 \n","Epoch:0-36/469, Negative loglikelihood: 9819.447265625 \n","Epoch:0-37/469, Negative loglikelihood: 9769.171875 \n","Epoch:0-38/469, Negative loglikelihood: 9770.767578125 \n","Epoch:0-39/469, Negative loglikelihood: 9801.8076171875 \n","Epoch:0-40/469, Negative loglikelihood: 9790.6171875 \n","Epoch:0-41/469, Negative loglikelihood: 9784.857421875 \n","Epoch:0-42/469, Negative loglikelihood: 9788.3876953125 \n","Epoch:0-43/469, Negative loglikelihood: 9775.931640625 \n","Epoch:0-44/469, Negative loglikelihood: 9784.33203125 \n","Epoch:0-45/469, Negative loglikelihood: 9793.6640625 \n","Epoch:0-46/469, Negative loglikelihood: 9782.041015625 \n","Epoch:0-47/469, Negative loglikelihood: 9788.83203125 \n","Epoch:0-48/469, Negative loglikelihood: 9785.7314453125 \n","Epoch:0-49/469, Negative loglikelihood: 9781.3349609375 \n","Epoch:0-50/469, Negative loglikelihood: 9793.1728515625 \n","Epoch:0-51/469, Negative loglikelihood: 9778.82421875 \n","Epoch:0-52/469, Negative loglikelihood: 9778.1171875 \n","Epoch:0-53/469, Negative loglikelihood: 9769.5224609375 \n","Epoch:0-54/469, Negative loglikelihood: 9787.6357421875 \n","Epoch:0-55/469, Negative loglikelihood: 9763.34765625 \n","Epoch:0-56/469, Negative loglikelihood: 9793.306640625 \n","Epoch:0-57/469, Negative loglikelihood: 9773.1796875 \n","Epoch:0-58/469, Negative loglikelihood: 9465.09765625 \n","Epoch:0-59/469, Negative loglikelihood: 9773.8017578125 \n","Epoch:0-60/469, Negative loglikelihood: 9767.7333984375 \n","Epoch:0-61/469, Negative loglikelihood: 9755.9228515625 \n","Epoch:0-62/469, Negative loglikelihood: 9782.9912109375 \n","Epoch:0-63/469, Negative loglikelihood: 9776.259765625 \n","Epoch:0-64/469, Negative loglikelihood: 9795.341796875 \n","Epoch:0-65/469, Negative loglikelihood: 9761.732421875 \n","Epoch:0-66/469, Negative loglikelihood: 9767.73828125 \n","Epoch:0-67/469, Negative loglikelihood: 9758.2314453125 \n","Epoch:0-68/469, Negative loglikelihood: 9764.1181640625 \n","Epoch:0-69/469, Negative loglikelihood: 9751.4775390625 \n","Epoch:0-70/469, Negative loglikelihood: 9759.662109375 \n","Epoch:0-71/469, Negative loglikelihood: 9767.984375 \n","Epoch:0-72/469, Negative loglikelihood: 9761.27734375 \n","Epoch:0-73/469, Negative loglikelihood: 9755.9951171875 \n","Epoch:0-74/469, Negative loglikelihood: 9794.6083984375 \n","Epoch:0-75/469, Negative loglikelihood: 9766.78125 \n","Epoch:0-76/469, Negative loglikelihood: 9758.6533203125 \n","Epoch:0-77/469, Negative loglikelihood: 9755.17578125 \n","Epoch:0-78/469, Negative loglikelihood: 9753.533203125 \n","Epoch:0-79/469, Negative loglikelihood: 9756.0234375 \n","Epoch:0-80/469, Negative loglikelihood: 9756.943359375 \n","Epoch:0-81/469, Negative loglikelihood: 9741.9169921875 \n","Epoch:0-82/469, Negative loglikelihood: 9752.0166015625 \n","Epoch:0-83/469, Negative loglikelihood: 9753.8037109375 \n","Epoch:0-84/469, Negative loglikelihood: 9740.333984375 \n","Epoch:0-85/469, Negative loglikelihood: 9747.9521484375 \n","Epoch:0-86/469, Negative loglikelihood: 9744.4521484375 \n","Epoch:0-87/469, Negative loglikelihood: 9745.1337890625 \n","Epoch:0-88/469, Negative loglikelihood: 9740.693359375 \n","Epoch:0-89/469, Negative loglikelihood: 9744.2763671875 \n","Epoch:0-90/469, Negative loglikelihood: 9730.689453125 \n","Epoch:0-91/469, Negative loglikelihood: 9737.322265625 \n","Epoch:0-92/469, Negative loglikelihood: 9426.5478515625 \n","Epoch:0-93/469, Negative loglikelihood: 9726.62890625 \n","Epoch:0-94/469, Negative loglikelihood: 9728.123046875 \n","Epoch:0-95/469, Negative loglikelihood: 9730.740234375 \n","Epoch:0-96/469, Negative loglikelihood: 9730.44921875 \n","Epoch:0-97/469, Negative loglikelihood: 9732.791015625 \n","Epoch:0-98/469, Negative loglikelihood: 9723.0341796875 \n","Epoch:0-99/469, Negative loglikelihood: 9724.1806640625 \n","Epoch:0-100/469, Negative loglikelihood: 9727.974609375 \n","Epoch:0-101/469, Negative loglikelihood: 9417.1845703125 \n","Epoch:0-102/469, Negative loglikelihood: 9720.6591796875 \n","Epoch:0-103/469, Negative loglikelihood: 9710.3115234375 \n","Epoch:0-104/469, Negative loglikelihood: 9727.107421875 \n","Epoch:0-105/469, Negative loglikelihood: 9712.5556640625 \n","Epoch:0-106/469, Negative loglikelihood: 9719.01953125 \n","Epoch:0-107/469, Negative loglikelihood: 9716.380859375 \n","Epoch:0-108/469, Negative loglikelihood: 9720.138671875 \n","Epoch:0-109/469, Negative loglikelihood: 9714.958984375 \n","Epoch:0-110/469, Negative loglikelihood: 9715.8359375 \n","Epoch:0-111/469, Negative loglikelihood: 9709.9169921875 \n","Epoch:0-112/469, Negative loglikelihood: 9711.916015625 \n","Epoch:0-113/469, Negative loglikelihood: 9703.837890625 \n","Epoch:0-114/469, Negative loglikelihood: 9715.3837890625 \n","Epoch:0-115/469, Negative loglikelihood: 9704.8603515625 \n","Epoch:0-116/469, Negative loglikelihood: 9706.98046875 \n","Epoch:0-117/469, Negative loglikelihood: 9390.11328125 \n","Epoch:0-118/469, Negative loglikelihood: 9703.6357421875 \n","Epoch:0-119/469, Negative loglikelihood: 9704.359375 \n","Epoch:0-120/469, Negative loglikelihood: 9706.75390625 \n","Epoch:0-121/469, Negative loglikelihood: 9709.74609375 \n","Epoch:0-122/469, Negative loglikelihood: 9704.5576171875 \n","Epoch:0-123/469, Negative loglikelihood: 9703.685546875 \n","Epoch:0-124/469, Negative loglikelihood: 9389.951171875 \n","Epoch:0-125/469, Negative loglikelihood: 9703.34765625 \n","Epoch:0-126/469, Negative loglikelihood: 9701.255859375 \n","Epoch:0-127/469, Negative loglikelihood: 9696.5859375 \n","Epoch:0-128/469, Negative loglikelihood: 9698.25 \n","Epoch:0-129/469, Negative loglikelihood: 9700.970703125 \n","Epoch:0-130/469, Negative loglikelihood: 9699.978515625 \n","Epoch:0-131/469, Negative loglikelihood: 9698.78515625 \n","Epoch:0-132/469, Negative loglikelihood: 9697.9609375 \n","Epoch:0-133/469, Negative loglikelihood: 9696.1123046875 \n","Epoch:0-134/469, Negative loglikelihood: 9693.0146484375 \n","Epoch:0-135/469, Negative loglikelihood: 9693.318359375 \n","Epoch:0-136/469, Negative loglikelihood: 9693.0390625 \n","Epoch:0-137/469, Negative loglikelihood: 9694.125 \n","Epoch:0-138/469, Negative loglikelihood: 9694.4033203125 \n","Epoch:0-139/469, Negative loglikelihood: 9691.943359375 \n","Epoch:0-140/469, Negative loglikelihood: 9697.064453125 \n","Epoch:0-141/469, Negative loglikelihood: 9688.7421875 \n","Epoch:0-142/469, Negative loglikelihood: 9691.2509765625 \n","Epoch:0-143/469, Negative loglikelihood: 9693.2216796875 \n","Epoch:0-144/469, Negative loglikelihood: 9695.73828125 \n","Epoch:0-145/469, Negative loglikelihood: 9689.5634765625 \n","Epoch:0-146/469, Negative loglikelihood: 9693.341796875 \n","Epoch:0-147/469, Negative loglikelihood: 9689.0830078125 \n","Epoch:0-148/469, Negative loglikelihood: 9686.693359375 \n","Epoch:0-149/469, Negative loglikelihood: 9686.08203125 \n","Epoch:0-150/469, Negative loglikelihood: 9375.5 \n","Epoch:0-151/469, Negative loglikelihood: 9684.501953125 \n","Epoch:0-152/469, Negative loglikelihood: 9687.5498046875 \n","Epoch:0-153/469, Negative loglikelihood: 9683.1953125 \n","Epoch:0-154/469, Negative loglikelihood: 9684.884765625 \n","Epoch:0-155/469, Negative loglikelihood: 9682.6611328125 \n","Epoch:0-156/469, Negative loglikelihood: 9684.126953125 \n","Epoch:0-157/469, Negative loglikelihood: 9680.666015625 \n","Epoch:0-158/469, Negative loglikelihood: 9684.720703125 \n","Epoch:0-159/469, Negative loglikelihood: 9681.7099609375 \n","Epoch:0-160/469, Negative loglikelihood: 9683.328125 \n","Epoch:0-161/469, Negative loglikelihood: 9680.150390625 \n","Epoch:0-162/469, Negative loglikelihood: 9680.8046875 \n","Epoch:0-163/469, Negative loglikelihood: 9680.2431640625 \n","Epoch:0-164/469, Negative loglikelihood: 9678.16796875 \n","Epoch:0-165/469, Negative loglikelihood: 9680.14453125 \n","Epoch:0-166/469, Negative loglikelihood: 9675.0078125 \n","Epoch:0-167/469, Negative loglikelihood: 9678.0390625 \n","Epoch:0-168/469, Negative loglikelihood: 9676.97265625 \n","Epoch:0-169/469, Negative loglikelihood: 9678.3662109375 \n","Epoch:0-170/469, Negative loglikelihood: 9676.2333984375 \n","Epoch:0-171/469, Negative loglikelihood: 9674.1025390625 \n","Epoch:0-172/469, Negative loglikelihood: 9677.41015625 \n","Epoch:0-173/469, Negative loglikelihood: 9677.4736328125 \n","Epoch:0-174/469, Negative loglikelihood: 9675.40625 \n","Epoch:0-175/469, Negative loglikelihood: 9675.64453125 \n","Epoch:0-176/469, Negative loglikelihood: 9673.767578125 \n","Epoch:0-177/469, Negative loglikelihood: 9671.9296875 \n","Epoch:0-178/469, Negative loglikelihood: 9673.48828125 \n","Epoch:0-179/469, Negative loglikelihood: 9673.0029296875 \n","Epoch:0-180/469, Negative loglikelihood: 9674.3447265625 \n","Epoch:0-181/469, Negative loglikelihood: 9673.37890625 \n","Epoch:0-182/469, Negative loglikelihood: 9674.4853515625 \n","Epoch:0-183/469, Negative loglikelihood: 9672.734375 \n","Epoch:0-184/469, Negative loglikelihood: 9670.837890625 \n","Epoch:0-185/469, Negative loglikelihood: 9674.3837890625 \n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-24-42ed5e5487cb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 261\u001b[0;31m         \u001b[0mneg_log_likelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mneg_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    262\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mgradient_accumulation_steps\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-42ed5e5487cb>\u001b[0m in \u001b[0;36mneg_log_likelihood\u001b[0;34m(self, input_ids, segment_ids, input_mask, label_ids)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mneg_log_likelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         \u001b[0mbert_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_bert_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m         \u001b[0mforward_score\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_alg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_feats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m         \u001b[0;31m# p(X=w1:t,Zt=tag1:t)=...p(Zt=tag_t|Zt-1=tag_t-1)p(xt|Zt=tag_t)...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-24-42ed5e5487cb>\u001b[0m in \u001b[0;36m_get_bert_features\u001b[0;34m(self, input_ids, segment_ids, input_mask)\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0msentances\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mword\u001b[0m \u001b[0membedding\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mlstm\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mMLP\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mfeats\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         '''\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mbert_seq_out\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msegment_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m         \u001b[0mbert_seq_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_seq_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mbert_feats\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhidden2label\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_seq_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    731\u001b[0m         encoded_layers = self.encoder(embedding_output,\n\u001b[1;32m    732\u001b[0m                                       \u001b[0mextended_attention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 733\u001b[0;31m                                       output_all_encoded_layers=output_all_encoded_layers)\n\u001b[0m\u001b[1;32m    734\u001b[0m         \u001b[0msequence_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mencoded_layers\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    735\u001b[0m         \u001b[0mpooled_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpooler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msequence_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask, output_all_encoded_layers)\u001b[0m\n\u001b[1;32m    404\u001b[0m         \u001b[0mall_encoder_layers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    405\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mlayer_module\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 406\u001b[0;31m             \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlayer_module\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    407\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0moutput_all_encoded_layers\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m                 \u001b[0mall_encoder_layers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, attention_mask)\u001b[0m\n\u001b[1;32m    390\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    391\u001b[0m         \u001b[0mattention_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mattention\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 392\u001b[0;31m         \u001b[0mintermediate_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    393\u001b[0m         \u001b[0mlayer_output\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mintermediate_output\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    394\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlayer_output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pytorch_pretrained_bert/modeling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states)\u001b[0m\n\u001b[1;32m    362\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    363\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 364\u001b[0;31m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    365\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintermediate_act_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1846\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"cell_type":"code","source":["'''\n","Test_set prediction using the best epoch of NER_BERT_CRF model\n","'''\n","checkpoint = torch.load(output_dir+'/ner_bert_crf_checkpoint.pt', map_location='cpu')\n","epoch = checkpoint['epoch']\n","valid_acc_prev = checkpoint['valid_acc']\n","valid_f1_prev = checkpoint['valid_f1']\n","pretrained_dict=checkpoint['model_state']\n","net_state_dict = model.state_dict()\n","pretrained_dict_selected = {k: v for k, v in pretrained_dict.items() if k in net_state_dict}\n","net_state_dict.update(pretrained_dict_selected)\n","model.load_state_dict(net_state_dict)\n","print('Loaded the pretrain  NER_BERT_CRF  model, epoch:',checkpoint['epoch'],'valid acc:',\n","      checkpoint['valid_acc'], 'valid f1:', checkpoint['valid_f1'])\n","\n","model.to(device)\n","#evaluate(model, train_dataloader, batch_size, total_train_epochs-1, 'Train_set')\n","evaluate(model, test_dataloader, batch_size, epoch, 'Test_set')\n","# print('Total spend:',(time.time()-train_start)/60.0)\n","\n","\n","#%%\n","model.eval()\n","with torch.no_grad():\n","    demon_dataloader = data.DataLoader(dataset=test_dataset,\n","                                batch_size=10,\n","                                shuffle=False,\n","                                num_workers=4,\n","                                collate_fn=pad)\n","    for batch in demon_dataloader:\n","        batch = tuple(t.to(device) for t in batch)\n","        input_ids, input_mask, segment_ids, predict_mask, label_ids = batch\n","        _, predicted_label_seq_ids = model(input_ids, segment_ids, input_mask)\n","        # _, predicted = torch.max(out_scores, -1)\n","        valid_predicted = torch.masked_select(predicted_label_seq_ids, predict_mask)\n","        # valid_label_ids = torch.masked_select(label_ids, predict_mask)\n","        for i in range(10):\n","            print(predicted_label_seq_ids[i])\n","            print(label_ids[i])\n","            new_ids=predicted_label_seq_ids[i].cpu().numpy()[predict_mask[i].cpu().numpy()==1]\n","            print(list(map(lambda i: label_list[i], new_ids)))\n","            print(test_examples[i].labels)\n","        break\n","#%%\n","print(conllProcessor.get_label_map())"],"metadata":{"id":"X_7z2Zjf9Yli"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[""],"metadata":{"id":"ERX-ZkrEAJb3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### Bert CRF \n","https://github.com/shushanxingzhe/transformers_ner"],"metadata":{"id":"LzVq4FjHMBaM"}},{"cell_type":"code","source":["!pip install pytorch-crf transformers -q\n","!pip install datasets -q"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ULQQLWHzMG9W","executionInfo":{"status":"ok","timestamp":1639976226412,"user_tz":-420,"elapsed":30376,"user":{"displayName":"Trinh Truong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibwjDAT5JHasAFlEGN-kjgr2JnHi7XIqtdSxt4=s64","userId":"05537025703764185240"}},"outputId":"91ae3564-c00f-48cf-c10b-d922de713a7b"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[K     |████████████████████████████████| 3.4 MB 5.5 MB/s \n","\u001b[K     |████████████████████████████████| 61 kB 316 kB/s \n","\u001b[K     |████████████████████████████████| 596 kB 38.6 MB/s \n","\u001b[K     |████████████████████████████████| 895 kB 19.6 MB/s \n","\u001b[K     |████████████████████████████████| 3.3 MB 32.2 MB/s \n","\u001b[K     |████████████████████████████████| 298 kB 5.2 MB/s \n","\u001b[K     |████████████████████████████████| 1.1 MB 35.6 MB/s \n","\u001b[K     |████████████████████████████████| 243 kB 42.9 MB/s \n","\u001b[K     |████████████████████████████████| 132 kB 50.7 MB/s \n","\u001b[K     |████████████████████████████████| 192 kB 48.3 MB/s \n","\u001b[K     |████████████████████████████████| 160 kB 34.7 MB/s \n","\u001b[K     |████████████████████████████████| 271 kB 50.5 MB/s \n","\u001b[?25h"]}]},{"cell_type":"code","source":["from transformers import BertPreTrainedModel, BertModel\n","from transformers.modeling_outputs import  TokenClassifierOutput\n","from torch import nn\n","from torch.nn import CrossEntropyLoss\n","import torch\n","from torchcrf import CRF\n","\n","\n","class BertForTokenClassification(BertPreTrainedModel):\n","\n","    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.bert = BertModel(config, add_pooling_layer=False)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","\n","        self.init_weights()\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","        r\"\"\"\n","        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n","            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n","            1]``.\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        sequence_output = outputs[0]\n","\n","        sequence_output = self.dropout(sequence_output)\n","        logits = self.classifier(sequence_output)\n","\n","        loss = None\n","        if labels is not None:\n","            loss_fct = CrossEntropyLoss()\n","            # Only keep active parts of the loss\n","            if attention_mask is not None:\n","                active_loss = attention_mask.view(-1) == 1\n","                active_logits = logits.view(-1, self.num_labels)\n","                active_labels = torch.where(\n","                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n","                )\n","                loss = loss_fct(active_logits, active_labels)\n","            else:\n","                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n","\n","        if not return_dict:\n","            output = (logits,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return TokenClassifierOutput(\n","            loss=loss,\n","            logits=logits,\n","            hidden_states=outputs.hidden_states,\n","            attentions=outputs.attentions,\n","        )\n","\n","\n","class BertLstmCRF(BertPreTrainedModel):\n","\n","    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.bert = BertModel(config, add_pooling_layer=False)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.bilstm = nn.LSTM(config.hidden_size, (config.hidden_size) // 2, dropout=0.5, batch_first=True,\n","                              bidirectional=True)\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","        self.crf = CRF(num_tags=config.num_labels, batch_first=True)\n","        self.init_weights()\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","        r\"\"\"\n","        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n","            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n","            1]``.\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        sequence_output = outputs[0]\n","        sequence_output = self.dropout(sequence_output)\n","        lstm_output, hc = self.bilstm(sequence_output)\n","        logits = self.classifier(lstm_output)\n","\n","        loss = None\n","        if labels is not None:\n","            log_likelihood, tags = self.crf(logits, labels), self.crf.decode(logits)\n","            loss = 0 - log_likelihood\n","        else:\n","            tags = self.crf.decode(logits)\n","        tags = torch.Tensor(tags)\n","\n","        if not return_dict:\n","            output = (tags,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return loss, tags\n","\n","class BertCRF(BertPreTrainedModel):\n","\n","    _keys_to_ignore_on_load_unexpected = [r\"pooler\"]\n","\n","    def __init__(self, config):\n","        super().__init__(config)\n","        self.num_labels = config.num_labels\n","\n","        self.bert = BertModel(config, add_pooling_layer=False)\n","        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n","        self.classifier = nn.Linear(config.hidden_size, config.num_labels)\n","        self.crf = CRF(num_tags=config.num_labels, batch_first=True)\n","        self.init_weights()\n","\n","    def forward(\n","        self,\n","        input_ids=None,\n","        attention_mask=None,\n","        token_type_ids=None,\n","        position_ids=None,\n","        head_mask=None,\n","        inputs_embeds=None,\n","        labels=None,\n","        output_attentions=None,\n","        output_hidden_states=None,\n","        return_dict=None,\n","    ):\n","        r\"\"\"\n","        labels (:obj:`torch.LongTensor` of shape :obj:`(batch_size, sequence_length)`, `optional`):\n","            Labels for computing the token classification loss. Indices should be in ``[0, ..., config.num_labels -\n","            1]``.\n","        \"\"\"\n","        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n","\n","        outputs = self.bert(\n","            input_ids,\n","            attention_mask=attention_mask,\n","            token_type_ids=token_type_ids,\n","            position_ids=position_ids,\n","            head_mask=head_mask,\n","            inputs_embeds=inputs_embeds,\n","            output_attentions=output_attentions,\n","            output_hidden_states=output_hidden_states,\n","            return_dict=return_dict,\n","        )\n","\n","        sequence_output = outputs[0]\n","        sequence_output = self.dropout(sequence_output)\n","        logits = self.classifier(sequence_output)\n","\n","        loss = None\n","        if labels is not None:\n","            log_likelihood, tags = self.crf(logits, labels), self.crf.decode(logits)\n","            loss = 0 - log_likelihood\n","        else:\n","            tags = self.crf.decode(logits)\n","        tags = torch.Tensor(tags)\n","\n","        if not return_dict:\n","            output = (tags,) + outputs[2:]\n","            return ((loss,) + output) if loss is not None else output\n","\n","        return loss, tags"],"metadata":{"id":"XhO6iJ7zMMU9","executionInfo":{"status":"ok","timestamp":1639976357391,"user_tz":-420,"elapsed":435,"user":{"displayName":"Trinh Truong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibwjDAT5JHasAFlEGN-kjgr2JnHi7XIqtdSxt4=s64","userId":"05537025703764185240"}}},"execution_count":4,"outputs":[]},{"cell_type":"code","source":["from datasets import load_dataset\n","from sklearn.metrics import classification_report, f1_score\n","from transformers import BertTokenizerFast, Trainer, TrainingArguments\n","from transformers.trainer_utils import IntervalStrategy\n","\n","# from models import BertCRF\n","\n","train_dataset, test_dataset = load_dataset('conll2003', split=['train', 'test'])\n","print(train_dataset, test_dataset)\n","\n","model = BertLstmCRF.from_pretrained('bert-base-cased', num_labels=9)\n","tokenizer = BertTokenizerFast.from_pretrained('bert-base-cased')\n","\n","\n","def tokenize(batch):\n","    result = {\n","        'label_ids': [],\n","        'input_ids': [],\n","        'token_type_ids': [],\n","    }\n","    max_length = tokenizer.max_model_input_sizes['bert-base-cased']\n","\n","    for tokens, label in zip(batch['tokens'], batch['label_ids']):\n","        tokenids = tokenizer(tokens, add_special_tokens=False)\n","\n","        token_ids = []\n","        label_ids = []\n","        for ids, lab in zip(tokenids['input_ids'], label):\n","            if len(ids) > 1 and lab % 2 == 1:\n","                token_ids.extend(ids)\n","                chunk = [lab + 1] * len(ids)\n","                chunk[0] = lab\n","                label_ids.extend(chunk)\n","            else:\n","                token_ids.extend(ids)\n","                chunk = [lab] * len(ids)\n","                label_ids.extend(chunk)\n","\n","        token_type_ids = tokenizer.create_token_type_ids_from_sequences(token_ids)\n","        token_ids = tokenizer.build_inputs_with_special_tokens(token_ids)\n","        label_ids.insert(0, 0)\n","        label_ids.append(0)\n","        result['input_ids'].append(token_ids)\n","        result['label_ids'].append(label_ids)\n","        result['token_type_ids'].append(token_type_ids)\n","\n","    result = tokenizer.pad(result, padding='longest', max_length=max_length, return_attention_mask=True)\n","    for i in range(len(result['input_ids'])):\n","        diff = len(result['input_ids'][i]) - len(result['label_ids'][i])\n","        result['label_ids'][i] += [0] * diff\n","    return result\n","\n","\n","train_dataset = train_dataset.remove_columns(['id', 'pos_tags', 'chunk_tags'])\n","train_dataset = train_dataset.rename_column('ner_tags', 'label_ids')\n","test_dataset = test_dataset.remove_columns(['id', 'pos_tags', 'chunk_tags'])\n","test_dataset = test_dataset.rename_column('ner_tags', 'label_ids')\n","\n","train_dataset = train_dataset.map(tokenize, batched=True, batch_size=len(train_dataset))\n","test_dataset = test_dataset.map(tokenize, batched=True, batch_size=len(test_dataset))\n","train_dataset.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label_ids'])\n","test_dataset.set_format('torch', columns=['input_ids', 'token_type_ids', 'attention_mask', 'label_ids'])\n","\n","\n","def compute_metrics(pred):\n","    labels = pred.label_ids.flatten()\n","    preds = pred.predictions.flatten()\n","    f1 = f1_score(labels, preds, average='macro')\n","    print(classification_report(labels, preds))\n","    return {\n","        'f1': f1\n","    }\n","\n","\n","training_args = TrainingArguments(\n","    output_dir='./results',\n","    num_train_epochs=3,\n","    per_device_train_batch_size=32,\n","    per_device_eval_batch_size=32,\n","    warmup_steps=200,\n","    weight_decay=0.01,\n","    save_strategy=IntervalStrategy.EPOCH,\n","    logging_dir='./logs',\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    compute_metrics=compute_metrics,\n","    train_dataset=train_dataset,\n","    eval_dataset=test_dataset\n",")\n","\n","trainer.train()\n","\n","print(trainer.evaluate())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["55adb9b7dc1540c7b974e03882095ade","0541bb0057a54acc9122987876c31318","acabe51d8d394d0b8b5a5ae555d701d5","9d8ac6668f8843fdae798077e9c1f3c3","676e0f1d4aeb49caa44bb6bc6cefbc27","145ddc8f231c4da399328922dae88eab","36cc7a2074a148e58223be859dd88bf4","7261c4da6a154d0aab303fb5b057e1f6","e5f00893934e43fe9668cbc5204fd6d1","3ceba6d962154169b5b471d193e90cd8","21fada6ebe094405a3d2b4aa6276f5b6","8f4efd576ba8420c9dcab0753f20fcc5","82db1d6029764377903e3f537b184f73","f897b9de1708413eb07ddb805a530519","db81a86f197b4b77be22ac80722578a3","7a984c6be93248ccbb75825ad45c2003","2cf02e8e54fe4347bfd2f927fa63b9bc","04792a3e4c68461ab70a6ade63e891e5","9ae34c49ce48488d98179b5823caf3f6","8b7334664d5b4847ad1364cce90213bd","f98dfe2ff4ea473fa2ea162c5af14157","c558ce04bec947dcbde46bb42d841a15","a68c9e3ee80a49759800296bcab4c77c","0db3d8ee13c24b2c9f1d8669de1bbb2e","05228d81338e44e0a0ddc0ec412a0907","58709aff78e9470390cb3e92abbba2b3","e72fc5a90a0f493da8a5dbec559cb7e4","b27dd9de4b5b4721b34490f34f33b683","9e624ac1912f44b690eef482467d390c","5851052c4b6942829649634aa61dc8f5","33c61eb87b0542b6affd42718f5e51da","da890f61fb274c9c8f9e23c3d1a9c475","7658a7a8ffd442aca6df4f0b32bf9153","f7ac2c89eedf47cdb800ad3936e989e5","894fdb3ac7d047629ced0946da06b6c9","398b40608c0f4980b72b3fc4ef894c94","72da733a43fb4e19975dd05b4b950218","7675029694004a7e95b56552763ad100","333f9a97cc704f788d6a688472150472","e7b38b48424949d89505f7697a5cb768","12eb1c54ba484fbda5f3db62d474bf73","24f107c42a494a3ab349dbe22e585098","f00cc6ea88c340ab9e022a4c0caea067","19c95b3718b44bfca1d479f89f9ac78a","b9eb1241f85e46f9bc7dd800762cd3ee","41f2d12d145b45a2b1d5c2984111f147","f5b222b5c2c14fce8912fb559ecfe5f5","c962fa997fd8415a8fbb042db0420a20","2bf4c3ab05bd4095ba306361791838a5","2a618e34a00441b4a5b434fece7c331e","578ac25082b04a3f81ba9d27e8be98a0","c509bb293b354c54b5db57f44d7d714c","a743ceb67cbf4c73a0842cf8f31f7edf","01b907b208d74f319aa17e978d44e65c","655b4a271d5b4f97beb21271052ad298","471029247332469cb1859523a625c815","ecfc28a044434f519081ca84c5e2398a","f9a24f8f29f54fa883a068def27d8598","381eae1f4ba847ec9ff5a02de5c021d3","3966a6d313b1401ebc716c10a78c1282","53b911d3c0c14fa7908d972c4f9aa47a","da49104d929643c6a17373e7560e525a","5ebd3b3c22694f9e870172b8706d9ea3","343dcb667f6e4323b3c73f04c7ebf84a","7b3dd81768be4900afd7fb11cf189b29","ede237c111ff474ea90f0b965ae258c8"]},"id":"mlTaq1AIMlc7","executionInfo":{"status":"ok","timestamp":1639980095129,"user_tz":-420,"elapsed":103174,"user":{"displayName":"Trinh Truong","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GibwjDAT5JHasAFlEGN-kjgr2JnHi7XIqtdSxt4=s64","userId":"05537025703764185240"}},"outputId":"1611582c-8b36-4089-a34a-473d575b1277"},"execution_count":5,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["Reusing dataset conll2003 (/root/.cache/huggingface/datasets/conll2003/conll2003/1.0.0/40e7cb6bcc374f7c349c83acd1e9352a4f09474eb691f64f364ee62eb65d0ca6)\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"55adb9b7dc1540c7b974e03882095ade","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/2 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["Dataset({\n","    features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n","    num_rows: 14041\n","}) Dataset({\n","    features: ['id', 'tokens', 'pos_tags', 'chunk_tags', 'ner_tags'],\n","    num_rows: 3453\n","})\n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/nn/modules/rnn.py:65: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.5 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","Some weights of the model checkpoint at bert-base-cased were not used when initializing BertLstmCRF: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight']\n","- This IS expected if you are initializing BertLstmCRF from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertLstmCRF from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertLstmCRF were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['bilstm.bias_hh_l0', 'bilstm.weight_hh_l0', 'crf.start_transitions', 'crf.transitions', 'crf.end_transitions', 'bilstm.bias_ih_l0', 'bilstm.bias_hh_l0_reverse', 'classifier.bias', 'bilstm.weight_ih_l0_reverse', 'bilstm.weight_hh_l0_reverse', 'bilstm.bias_ih_l0_reverse', 'classifier.weight', 'bilstm.weight_ih_l0']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8f4efd576ba8420c9dcab0753f20fcc5","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/208k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a68c9e3ee80a49759800296bcab4c77c","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/426k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f7ac2c89eedf47cdb800ad3936e989e5","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/29.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b9eb1241f85e46f9bc7dd800762cd3ee","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"471029247332469cb1859523a625c815","version_major":2,"version_minor":0},"text/plain":["  0%|          | 0/1 [00:00<?, ?ba/s]"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["The following columns in the training set  don't have a corresponding argument in `BertLstmCRF.forward` and have been ignored: tokens.\n","***** Running training *****\n","  Num examples = 14041\n","  Num Epochs = 3\n","  Instantaneous batch size per device = 32\n","  Total train batch size (w. parallel, distributed & accumulation) = 32\n","  Gradient Accumulation steps = 1\n","  Total optimization steps = 1317\n","/usr/local/lib/python3.7/dist-packages/torchcrf/__init__.py:249: UserWarning: where received a uint8 condition tensor. This behavior is deprecated and will be removed in a future version of PyTorch. Use a boolean condition instead. (Triggered internally at  ../aten/src/ATen/native/TensorCompare.cpp:328.)\n","  score = torch.where(mask[i].unsqueeze(1), next_score, score)\n"]},{"data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1318' max='1317' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1317/1317 59:58, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>1083.940700</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>32.374700</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"output_type":"stream","name":"stderr","text":["Saving model checkpoint to ./results/checkpoint-439\n","Configuration saved in ./results/checkpoint-439/config.json\n","Model weights saved in ./results/checkpoint-439/pytorch_model.bin\n","Saving model checkpoint to ./results/checkpoint-878\n","Configuration saved in ./results/checkpoint-878/config.json\n","Model weights saved in ./results/checkpoint-878/pytorch_model.bin\n","Saving model checkpoint to ./results/checkpoint-1317\n","Configuration saved in ./results/checkpoint-1317/config.json\n","Model weights saved in ./results/checkpoint-1317/pytorch_model.bin\n","\n","\n","Training completed. Do not forget to share your model on huggingface.co/models =)\n","\n","\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='1317' max='1317' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [1317/1317 1:00:03, Epoch 3/3]\n","    </div>\n","    <table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: left;\">\n","      <th>Step</th>\n","      <th>Training Loss</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <td>500</td>\n","      <td>1083.940700</td>\n","    </tr>\n","    <tr>\n","      <td>1000</td>\n","      <td>32.374700</td>\n","    </tr>\n","  </tbody>\n","</table><p>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stderr","text":["The following columns in the evaluation set  don't have a corresponding argument in `BertLstmCRF.forward` and have been ignored: tokens.\n","***** Running Evaluation *****\n","  Num examples = 3453\n","  Batch size = 32\n"]},{"output_type":"display_data","data":{"text/html":["\n","    <div>\n","      \n","      <progress value='108' max='108' style='width:300px; height:20px; vertical-align: middle;'></progress>\n","      [108/108 01:41]\n","    </div>\n","    "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{}},{"output_type":"stream","name":"stdout","text":["              precision    recall  f1-score   support\n","\n","           0       1.00      1.00      1.00    495508\n","           1       0.96      0.95      0.95      1617\n","           2       0.97      0.96      0.97      4082\n","           3       0.88      0.91      0.90      1661\n","           4       0.88      0.93      0.91      3172\n","           5       0.93      0.94      0.93      1668\n","           6       0.90      0.91      0.90      1748\n","           7       0.81      0.85      0.83       702\n","           8       0.61      0.68      0.64       886\n","\n","    accuracy                           1.00    511044\n","   macro avg       0.88      0.90      0.89    511044\n","weighted avg       1.00      1.00      1.00    511044\n","\n","{'eval_loss': 87.60346984863281, 'eval_f1': 0.8930595445960056, 'eval_runtime': 103.5779, 'eval_samples_per_second': 33.337, 'eval_steps_per_second': 1.043, 'epoch': 3.0}\n"]}]},{"cell_type":"code","source":["train_dataset"],"metadata":{"id":"CrUsoHtUM7oQ"},"execution_count":null,"outputs":[]}]}